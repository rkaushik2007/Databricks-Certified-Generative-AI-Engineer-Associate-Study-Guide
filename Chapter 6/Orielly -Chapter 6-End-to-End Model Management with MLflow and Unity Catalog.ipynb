{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3b94d92-d790-484e-8938-d08fe32fbf6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hands-On Lab: End-to-End Model Management with MLflow and Unity Catalog\n",
    "\n",
    "## Scenario\n",
    "You are a data scientist at a financial services company developing a **customer support assistant** that answers questions about account policies and churn drivers. The team has a working prototype that uses a **Retrieval-Augmented Generation (RAG) pipeline** to ground responses in internal documentation stored in the lakehouse.\n",
    "\n",
    "Leadership is satisfied with early demonstrations but is concerned about three production risks:\n",
    "1. **Reproducibility of experiments** - Can we recreate results and compare different RAG configurations?\n",
    "2. **Governance and audit readiness** - Can we prove to regulators which model version was served at any point in time?\n",
    "3. **Runaway costs** - How do we prevent uncontrolled LLM usage from creating budget overruns?\n",
    "\n",
    "To address these risks, you will operationalize the RAG solution using **MLflow** for experiment tracking, evaluation evidence, and artifact management, and **Unity Catalog** for model registration, version governance, and access control. Regulators and internal audit partners require a complete record of how the model was developed, which version was served at any point in time, and who had permission to promote or invoke the model.\n",
    "\n",
    "### Your Workflow Will Cover:\n",
    "1. **Track an MLflow experiment** that captures the RAG configuration, including key parameters such as retrieval settings and prompt template identifiers, so that results can be reproduced and compared across runs.\n",
    "2. **Log model artifacts and supporting evidence**, including representative prompt–response examples and retrieved context samples, so that reviewers can inspect what the system generated and what information the system used.\n",
    "3. **Evaluate RAG performance** using MLflow by logging summary metrics and qualitative artifacts that reflect relevance and grounding, so that promotion decisions are supported by evidence rather than intuition.\n",
    "4. **Register the candidate model** in the Unity Catalog–backed Model Registry, then manage versions using aliases (for example, Champion, Challenger) and tags (for example, lifecycle=archived) to prevent experimental versions from being used in production by mistake.\n",
    "5. **Implement Unity Catalog governance**, including role-based access control (RBAC), audit logging, and lineage, to ensure that access and changes are transparent and traceable.\n",
    "6. **Implement operational cost controls** by applying disciplined experimental hygiene, restricting access to production endpoints, and retiring unused versions, so that LLM usage remains predictable as adoption grows.\n",
    "\n",
    "This lab mirrors a real enterprise pattern: a team must demonstrate not only that a model works, but also that the team can explain, govern, and sustain the model over time.\n",
    "\n",
    "## Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "1. **Implement MLflow experiment tracking** for a governed RAG workflow by logging parameters, tags, metrics, and artifacts in a consistent structure that supports reproducibility and audit review.\n",
    "2. **Log and organize model outputs as MLflow artifacts**, including prompt templates, retrieved context samples, and representative responses, so that reviewers can validate model behavior without rerunning the full pipeline.\n",
    "3. **Evaluate RAG behavior** using MLflow by capturing both quantitative metrics and qualitative evidence that supports the assessment of relevance and grounding.\n",
    "4. **Register a model to the Unity Catalog–backed Model Registry** using a fully qualified model name, then manage versions using aliases (for example, Champion, Challenger) and tags (for example, lifecycle=archived) to separate experimentation from production usage.\n",
    "5. **Apply Unity Catalog governance controls** by enforcing RBAC, reviewing audit-relevant activity, and using lineage to trace a model version back to the training or evaluation context.\n",
    "6. **Apply operational best practices** by documenting assumptions, maintaining model metadata and signatures, archiving older versions, and cleaning up unused runs and models to keep the registry usable.\n",
    "7. **Apply cost-aware operational practices** by limiting uncontrolled experimentation, reusing proven configurations, and restricting production access so that LLM inference does not scale without governance.\n",
    "\n",
    "## Prerequisites\n",
    "- Databricks workspace with Unity Catalog enabled\n",
    "- Access to create catalogs, schemas, and tables\n",
    "- MLflow 2.8+ installed (pre-installed in Databricks Runtime ML)\n",
    "- Access to Foundation Model APIs (for LLM integration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4baf3b1-dd2e-462e-9034-d14ce1053153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Environment Setup and Prerequisites\n",
    "\n",
    "### What You'll Learn in This Section\n",
    "In this section, we will:\n",
    "1. Import necessary libraries for RAG pipeline development and MLflow tracking\n",
    "2. Configure Unity Catalog settings for governed model storage\n",
    "3. Create sample internal documentation data (knowledge base for RAG)\n",
    "4. Generate synthetic customer questions for testing\n",
    "\n",
    "### Why This Matters\n",
    "**Reproducibility from Day One:** Proper environment setup ensures that every experiment can be recreated. By establishing a consistent Unity Catalog namespace and logging all dependencies, you create an audit trail that satisfies regulatory requirements.\n",
    "\n",
    "**Governance Foundation:** Unity Catalog provides enterprise-grade data governance, while MLflow handles the complete model lifecycle. This combination ensures that every artifact, from training data to model versions, is tracked, secured, and auditable.\n",
    "\n",
    "**RAG-Specific Considerations:** Unlike traditional ML models, RAG systems depend on external knowledge sources. Tracking the version and lineage of your knowledge base is just as important as tracking model parameters. This section establishes that foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "711c50c0-389c-4dc9-9bec-d25538680c08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries for RAG pipeline and MLflow tracking\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# For text processing and embeddings\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import hashlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LIBRARY IMPORT STATUS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"✓ MLflow and tracking libraries imported\")\n",
    "print(\"✓ Data processing libraries imported (pandas, numpy, pyspark)\")\n",
    "print(\"✓ Text processing utilities imported\")\n",
    "print(f\"\\n\uD83D\uDCE6 MLflow version: {mlflow.__version__}\")\n",
    "print(f\"\uD83D\uDCE6 Python version: {pd.__version__}\")\n",
    "print(\"\\n\uD83D\uDCA1 Note: This lab uses MLflow 2.8+ features for RAG evaluation\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cc86168-67f8-4537-9c4b-75d00d930b75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configure Unity Catalog Settings\n",
    "\n",
    "#### Understanding the Three-Level Namespace\n",
    "Unity Catalog uses a three-level namespace: **`catalog.schema.object`**. This hierarchical structure enables:\n",
    "- **Catalog-level governance**: Broad access control and compliance boundaries\n",
    "- **Schema-level organization**: Logical grouping of related assets (data, models, functions)\n",
    "- **Object-level precision**: Fine-grained permissions on individual tables, models, or volumes\n",
    "\n",
    "#### Our Namespace Structure\n",
    "We'll set up the following Unity Catalog namespace for this RAG project:\n",
    "- **Catalog**: `financial_services` - Top-level container representing our business domain\n",
    "- **Schema**: `rag_support_assistant` - Logical grouping for RAG-related assets (knowledge base, models, evaluation data)\n",
    "- **Tables**:\n",
    "  - `knowledge_base` - Internal documentation for retrieval\n",
    "  - `evaluation_questions` - Test questions for RAG evaluation\n",
    "- **Model**: `customer_support_rag_model` - Registered RAG model with full lineage\n",
    "\n",
    "#### Governance Benefits\n",
    "**Centralized Access Control:** Unity Catalog provides RBAC at every level. You can grant different teams different permissions (e.g., data scientists can create models, analysts can only read data).\n",
    "\n",
    "**Automatic Audit Logging:** Every operation (read, write, delete, grant) is automatically logged. This creates a complete audit trail for compliance.\n",
    "\n",
    "**Data Lineage:** Unity Catalog automatically tracks relationships between tables, models, and downstream consumers. You can trace a model back to the exact data version it was trained on.\n",
    "\n",
    "**Cross-Workspace Sharing:** Models registered in Unity Catalog can be accessed from any workspace attached to the same metastore, enabling true enterprise-wide governance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "434d7f04-ba09-4a95-a43d-79c357fef4b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Unity Catalog namespace for RAG project\n",
    "CATALOG_NAME = \"financial_services\"\n",
    "SCHEMA_NAME = \"rag_support_assistant\"\n",
    "KNOWLEDGE_BASE_TABLE = \"knowledge_base\"\n",
    "EVAL_QUESTIONS_TABLE = \"evaluation_questions\"\n",
    "MODEL_NAME = f\"{CATALOG_NAME}.{SCHEMA_NAME}.customer_support_rag_model\"\n",
    "\n",
    "# Create catalog and schema if they don't exist\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA_NAME}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"UNITY CATALOG CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"✓ Catalog: {CATALOG_NAME}\")\n",
    "print(f\"✓ Schema: {SCHEMA_NAME}\")\n",
    "print(f\"\\n\uD83D\uDCCA Data Assets:\")\n",
    "print(f\"  • Knowledge Base Table: {CATALOG_NAME}.{SCHEMA_NAME}.{KNOWLEDGE_BASE_TABLE}\")\n",
    "print(f\"  • Evaluation Questions: {CATALOG_NAME}.{SCHEMA_NAME}.{EVAL_QUESTIONS_TABLE}\")\n",
    "print(f\"\\n\uD83E\uDD16 Model Registry:\")\n",
    "print(f\"  • RAG Model: {MODEL_NAME}\")\n",
    "print(f\"\\n\uD83D\uDCA1 All assets are now governed by Unity Catalog with:\")\n",
    "print(f\"  • Automatic audit logging\")\n",
    "print(f\"  • Fine-grained access control (RBAC)\")\n",
    "print(f\"  • Complete data lineage tracking\")\n",
    "print(f\"  • Cross-workspace accessibility\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ef09385-3946-45b1-9e42-3f4188cd6b3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate Sample Knowledge Base for RAG\n",
    "\n",
    "#### What is a Knowledge Base in RAG?\n",
    "A **knowledge base** is the collection of documents that the RAG system retrieves from to answer questions. In production, this would be:\n",
    "- Internal policy documents\n",
    "- Product documentation\n",
    "- FAQ databases\n",
    "- Historical support tickets\n",
    "- Regulatory compliance documents\n",
    "\n",
    "#### Our Simulated Knowledge Base\n",
    "We'll create a realistic knowledge base containing internal documentation about:\n",
    "- **Account Policies**: Overdraft protection, minimum balance requirements, account closure procedures\n",
    "- **Churn Drivers**: Common reasons customers leave and retention strategies\n",
    "- **Product Information**: Account types, features, and benefits\n",
    "- **Compliance Information**: Regulatory requirements and customer rights\n",
    "\n",
    "#### Why This Matters for Governance\n",
    "**Data Lineage:** By storing the knowledge base in Unity Catalog, we can track which model version used which version of the documentation. If a policy changes, we can identify all models that need retraining.\n",
    "\n",
    "**Access Control:** Sensitive internal documents can be protected with Unity Catalog RBAC. Only authorized users can access or modify the knowledge base.\n",
    "\n",
    "**Audit Trail:** Every query against the knowledge base is logged, creating a complete record of what information was used to generate customer-facing responses.\n",
    "\n",
    "**Versioning:** Unity Catalog's Delta Lake integration provides time travel, allowing you to audit what documentation was available at any point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc977d3d-9c97-42be-8ddb-16d28001e1bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create comprehensive knowledge base documents\n",
    "knowledge_base_documents = [\n",
    "    # Account Policies\n",
    "    {\n",
    "        'doc_id': 'POL-001',\n",
    "        'category': 'Account Policies',\n",
    "        'title': 'Overdraft Protection Policy',\n",
    "        'content': 'Overdraft protection is available for Premium and Gold account holders. The service covers overdrafts up to $500 with a $35 fee per occurrence. Basic account holders must maintain a minimum balance of $100 to avoid monthly fees. Overdraft protection can be linked to a savings account or line of credit.',\n",
    "        'last_updated': '2024-01-15',\n",
    "        'version': '2.1'\n",
    "    },\n",
    "    {\n",
    "        'doc_id': 'POL-002',\n",
    "        'category': 'Account Policies',\n",
    "        'title': 'Account Closure Procedures',\n",
    "        'content': 'Customers may close their accounts at any time without penalty if the account balance is zero. For accounts with remaining balances, customers must transfer or withdraw all funds before closure. Account closure requests can be submitted online, by phone, or in person. Processing takes 3-5 business days. Any recurring payments must be cancelled separately.',\n",
    "        'last_updated': '2024-02-01',\n",
    "        'version': '1.5'\n",
    "    },\n",
    "    {\n",
    "        'doc_id': 'POL-003',\n",
    "        'category': 'Account Policies',\n",
    "        'title': 'Minimum Balance Requirements',\n",
    "        'content': 'Basic accounts require a $100 minimum daily balance to avoid a $12 monthly maintenance fee. Premium accounts require $2,500 minimum balance to waive the $25 monthly fee. Gold accounts require $10,000 minimum balance to waive the $35 monthly fee. Students and seniors over 65 are exempt from minimum balance requirements on Basic accounts.',\n",
    "        'last_updated': '2024-01-20',\n",
    "        'version': '3.0'\n",
    "    },\n",
    "    # Churn Drivers and Retention\n",
    "    {\n",
    "        'doc_id': 'CHR-001',\n",
    "        'category': 'Churn Analysis',\n",
    "        'title': 'Top Reasons for Customer Churn',\n",
    "        'content': 'Analysis of customer exit surveys reveals the top churn drivers: 1) High fees and charges (42%), 2) Poor customer service experience (28%), 3) Better offers from competitors (18%), 4) Difficulty using online/mobile banking (8%), 5) Relocation or life changes (4%). Customers who file complaints are 3x more likely to churn within 90 days.',\n",
    "        'last_updated': '2024-03-10',\n",
    "        'version': '1.2'\n",
    "    },\n",
    "    {\n",
    "        'doc_id': 'CHR-002',\n",
    "        'category': 'Churn Analysis',\n",
    "        'title': 'Retention Strategies and Best Practices',\n",
    "        'content': 'Effective retention strategies include: proactive outreach to at-risk customers, fee waivers for long-term customers, personalized product recommendations, and priority customer service. Customers with multiple products have 60% lower churn rates. Mobile app engagement reduces churn by 35%. Regular communication and financial education programs improve retention by 25%.',\n",
    "        'last_updated': '2024-03-15',\n",
    "        'version': '2.0'\n",
    "    },\n",
    "    {\n",
    "        'doc_id': 'CHR-003',\n",
    "        'category': 'Churn Analysis',\n",
    "        'title': 'Early Warning Indicators',\n",
    "        'content': 'Key indicators of potential churn include: decreased transaction frequency (50% reduction over 60 days), multiple customer service calls within 30 days, complaint filing, balance below minimum for 3+ consecutive months, and no mobile app usage for 90+ days. Customers showing 3 or more indicators have an 80% churn probability within 6 months.',\n",
    "        'last_updated': '2024-03-01',\n",
    "        'version': '1.0'\n",
    "    },\n",
    "    # Product Information\n",
    "    {\n",
    "        'doc_id': 'PRD-001',\n",
    "        'category': 'Products',\n",
    "        'title': 'Account Types and Features',\n",
    "        'content': 'We offer three account types: Basic (no monthly fee with $100 minimum balance, standard ATM access, online banking), Premium ($25/month or waived with $2,500 balance, unlimited ATM reimbursement, overdraft protection, priority support), and Gold ($35/month or waived with $10,000 balance, all Premium features plus dedicated relationship manager, premium credit card, investment advisory services).',\n",
    "        'last_updated': '2024-02-15',\n",
    "        'version': '4.1'\n",
    "    },\n",
    "    {\n",
    "        'doc_id': 'PRD-002',\n",
    "        'category': 'Products',\n",
    "        'title': 'Digital Banking Features',\n",
    "        'content': 'Our mobile app and online banking platform offer: real-time balance and transaction alerts, mobile check deposit, bill pay and recurring payments, peer-to-peer transfers, budgeting tools, spending analytics, and biometric authentication. Premium and Gold members get early access to new features and enhanced security options including transaction verification and travel notifications.',\n",
    "        'last_updated': '2024-03-20',\n",
    "        'version': '3.2'\n",
    "    },\n",
    "    # Compliance and Regulations\n",
    "    {\n",
    "        'doc_id': 'CMP-001',\n",
    "        'category': 'Compliance',\n",
    "        'title': 'Customer Rights and Protections',\n",
    "        'content': 'Under federal regulations, customers have the right to: dispute unauthorized transactions within 60 days, receive clear fee disclosures, access account information within 30 days of request, and opt-out of data sharing with third parties. We comply with FDIC insurance requirements, providing coverage up to $250,000 per depositor. Customer data is protected under GLBA and state privacy laws.',\n",
    "        'last_updated': '2024-01-10',\n",
    "        'version': '5.0'\n",
    "    },\n",
    "    {\n",
    "        'doc_id': 'CMP-002',\n",
    "        'category': 'Compliance',\n",
    "        'title': 'Fee Disclosure Requirements',\n",
    "        'content': 'All account fees must be disclosed in writing before account opening. Monthly maintenance fees, overdraft fees, ATM fees, wire transfer fees, and other service charges are detailed in the fee schedule provided to customers. Fee changes require 30-day advance notice. Customers can request fee waivers in cases of financial hardship, which are reviewed on a case-by-case basis.',\n",
    "        'last_updated': '2024-02-05',\n",
    "        'version': '2.3'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_knowledge_base = pd.DataFrame(knowledge_base_documents)\n",
    "\n",
    "# Add metadata for tracking\n",
    "df_knowledge_base['data_created_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_knowledge_base['char_count'] = df_knowledge_base['content'].str.len()\n",
    "df_knowledge_base['word_count'] = df_knowledge_base['content'].str.split().str.len()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KNOWLEDGE BASE GENERATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"✓ Generated {len(df_knowledge_base)} knowledge base documents\")\n",
    "print(f\"\\n\uD83D\uDCCA Document Statistics:\")\n",
    "print(f\"  • Categories: {df_knowledge_base['category'].nunique()}\")\n",
    "print(f\"  • Average document length: {df_knowledge_base['char_count'].mean():.0f} characters\")\n",
    "print(f\"  • Average word count: {df_knowledge_base['word_count'].mean():.0f} words\")\n",
    "print(f\"\\n\uD83D\uDCC1 Document Breakdown by Category:\")\n",
    "print(df_knowledge_base['category'].value_counts().to_string())\n",
    "print(\"\\n\uD83D\uDCA1 This knowledge base will be used for:\")\n",
    "print(\"  • Retrieval-Augmented Generation (RAG)\")\n",
    "print(\"  • Grounding LLM responses in factual information\")\n",
    "print(\"  • Ensuring compliance and accuracy in customer support\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0ba9e7c-9d90-4e3c-b7df-4fa9a38eed0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Save Knowledge Base to Unity Catalog\n",
    "\n",
    "#### Why Store Knowledge Base in Unity Catalog?\n",
    "In production RAG systems, the knowledge base is a **critical production asset** that requires the same governance as any other data:\n",
    "\n",
    "**ACID Transactions:** Delta Lake ensures that updates to the knowledge base are atomic and consistent. If a policy document is updated, either the entire update succeeds or none of it does—no partial updates.\n",
    "\n",
    "**Time Travel for Compliance:** Unity Catalog's Delta Lake integration allows you to query the knowledge base as it existed at any point in time. This is critical for compliance: \"What information was available to the model on March 15th when it answered customer X's question?\"\n",
    "\n",
    "**Automatic Lineage Tracking:** Unity Catalog automatically tracks which models were trained or evaluated using which version of the knowledge base. This creates an auditable chain from source documents to model predictions.\n",
    "\n",
    "**Fine-Grained Access Control:** Different documents may have different sensitivity levels. Unity Catalog RBAC allows you to control who can read, write, or modify specific documents or categories.\n",
    "\n",
    "**Audit Logging:** Every access to the knowledge base is logged with user identity, timestamp, and operation type. This creates a complete audit trail for regulatory review.\n",
    "\n",
    "#### What We're Storing\n",
    "We'll save the knowledge base as a Delta table with:\n",
    "- Document content and metadata\n",
    "- Version information for each document\n",
    "- Category tags for organization\n",
    "- Timestamps for audit purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9d16ff7-694d-457d-bc0d-83161f50e30c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame and save to Unity Catalog\n",
    "df_kb_spark = spark.createDataFrame(df_knowledge_base)\n",
    "\n",
    "# Write to Delta table in Unity Catalog\n",
    "kb_table_path = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{KNOWLEDGE_BASE_TABLE}\"\n",
    "df_kb_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(kb_table_path)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KNOWLEDGE BASE SAVED TO UNITY CATALOG\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"✓ Table: {kb_table_path}\")\n",
    "print(f\"✓ Format: Delta Lake (ACID compliant)\")\n",
    "print(f\"✓ Records: {df_kb_spark.count():,} documents\")\n",
    "\n",
    "# Verify table creation and show sample\n",
    "df_kb_loaded = spark.table(kb_table_path)\n",
    "print(f\"\\n\uD83D\uDCCA Table Schema:\")\n",
    "df_kb_loaded.printSchema()\n",
    "\n",
    "print(f\"\\n\uD83D\uDCC4 Sample Documents:\")\n",
    "display(df_kb_loaded.select('doc_id', 'category', 'title', 'word_count', 'version').limit(5))\n",
    "\n",
    "print(\"\\n\uD83D\uDCA1 Governance Features Now Active:\")\n",
    "print(\"  ✓ Time travel enabled - query historical versions\")\n",
    "print(\"  ✓ Audit logging active - all access tracked\")\n",
    "print(\"  ✓ Lineage tracking - models will link to this data\")\n",
    "print(\"  ✓ RBAC ready - permissions can be granted per document category\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4bf4dc0-b697-4c5c-8da6-61a0d8c06d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate Evaluation Questions\n",
    "\n",
    "#### Why Do We Need Evaluation Questions?\n",
    "Unlike traditional ML models where you have labeled test data, RAG systems require **question-answer pairs** to evaluate performance. These questions should:\n",
    "- Cover the breadth of topics in your knowledge base\n",
    "- Include both simple factual questions and complex reasoning questions\n",
    "- Test edge cases (questions with no answer, ambiguous questions)\n",
    "- Represent real user queries\n",
    "\n",
    "#### Evaluation Dimensions for RAG\n",
    "We'll create questions that test:\n",
    "1. **Retrieval Quality**: Does the system find the right documents?\n",
    "2. **Answer Relevance**: Is the generated answer relevant to the question?\n",
    "3. **Groundedness/Faithfulness**: Is the answer supported by the retrieved context?\n",
    "4. **Completeness**: Does the answer fully address the question?\n",
    "5. **Conciseness**: Is the answer appropriately detailed without unnecessary information?\n",
    "\n",
    "#### Our Evaluation Dataset\n",
    "We'll generate questions across different categories and difficulty levels to comprehensively test the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7384027c-1a73-440b-97f5-fded803b61f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate evaluation questions with expected answers\n",
    "evaluation_questions = [\n",
    "    {\n",
    "        'question_id': 'Q001',\n",
    "        'question': 'What is the overdraft protection limit for Premium account holders?',\n",
    "        'category': 'Account Policies',\n",
    "        'difficulty': 'easy',\n",
    "        'expected_answer': 'Overdraft protection covers up to $500 for Premium account holders with a $35 fee per occurrence.',\n",
    "        'relevant_doc_ids': ['POL-001']\n",
    "    },\n",
    "    {\n",
    "        'question_id': 'Q002',\n",
    "        'question': 'How long does it take to process an account closure request?',\n",
    "        'category': 'Account Policies',\n",
    "        'difficulty': 'easy',\n",
    "        'expected_answer': 'Account closure requests take 3-5 business days to process.',\n",
    "        'relevant_doc_ids': ['POL-002']\n",
    "    },\n",
    "    {\n",
    "        'question_id': 'Q003',\n",
    "        'question': 'What are the minimum balance requirements to avoid monthly fees for each account type?',\n",
    "        'category': 'Account Policies',\n",
    "        'difficulty': 'medium',\n",
    "        'expected_answer': 'Basic accounts require $100 minimum balance to avoid $12 monthly fee. Premium accounts require $2,500 to waive $25 fee. Gold accounts require $10,000 to waive $35 fee. Students and seniors over 65 are exempt from minimum balance requirements on Basic accounts.',\n",
    "        'relevant_doc_ids': ['POL-003']\n",
    "    },\n",
    "    {\n",
    "        'question_id': 'Q004',\n",
    "        'question': 'What are the top three reasons customers leave our bank?',\n",
    "        'category': 'Churn Analysis',\n",
    "        'difficulty': 'medium',\n",
    "        'expected_answer': 'The top three reasons for customer churn are: 1) High fees and charges (42%), 2) Poor customer service experience (28%), and 3) Better offers from competitors (18%).',\n",
    "        'relevant_doc_ids': ['CHR-001']\n",
    "    },\n",
    "    {\n",
    "        'question_id': 'Q005',\n",
    "        'question': 'What retention strategies are most effective according to our analysis?',\n",
    "        'category': 'Churn Analysis',\n",
    "        'difficulty': 'medium',\n",
    "        'expected_answer': 'Effective retention strategies include proactive outreach to at-risk customers, fee waivers for long-term customers, personalized product recommendations, and priority customer service. Customers with multiple products have 60% lower churn rates, mobile app engagement reduces churn by 35%, and financial education programs improve retention by 25%.',\n",
    "        'relevant_doc_ids': ['CHR-002']\n",
    "    },\n",
    "    {\n",
    "        'question_id': 'Q006',\n",
    "        'question': 'What are the early warning signs that a customer might churn?',\n",
    "        'category': 'Churn Analysis',\n",
    "        'difficulty': 'hard',\n",
    "        'expected_answer': 'Key early warning indicators include: decreased transaction frequency (50% reduction over 60 days), multiple customer service calls within 30 days, complaint filing, balance below minimum for 3+ consecutive months, and no mobile app usage for 90+ days. Customers showing 3 or more indicators have an 80% churn probability within 6 months.',\n",
    "        'relevant_doc_ids': ['CHR-003']\n",
    "    },\n",
    "    {\n",
    "        'question_id': 'Q007',\n",
    "        'question': 'What features are included in the Gold account?',\n",
    "        'category': 'Products',\n",
    "        'difficulty': 'easy',\n",
    "        'expected_answer': 'Gold accounts include all Premium features plus dedicated relationship manager, premium credit card, and investment advisory services. The monthly fee is $35 or waived with $10,000 minimum balance.',\n",
    "        'relevant_doc_ids': ['PRD-001']\n",
    "    },\n",
    "    {\n",
    "        'question_id': 'Q008',\n",
    "        'question': 'What digital banking features do we offer?',\n",
    "        'category': 'Products',\n",
    "        'difficulty': 'medium',\n",
    "        'expected_answer': 'Digital banking features include real-time balance and transaction alerts, mobile check deposit, bill pay and recurring payments, peer-to-peer transfers, budgeting tools, spending analytics, and biometric authentication. Premium and Gold members get early access to new features and enhanced security options.',\n",
    "        'relevant_doc_ids': ['PRD-002']\n",
    "    },\n",
    "    {\n",
    "        'question_id': 'Q009',\n",
    "        'question': 'What are customer rights regarding unauthorized transactions?',\n",
    "        'category': 'Compliance',\n",
    "        'difficulty': 'medium',\n",
    "        'expected_answer': 'Customers have the right to dispute unauthorized transactions within 60 days. Accounts are FDIC insured up to $250,000 per depositor, and customer data is protected under GLBA and state privacy laws.',\n",
    "        'relevant_doc_ids': ['CMP-001']\n",
    "    },\n",
    "    {\n",
    "        'question_id': 'Q010',\n",
    "        'question': 'How much advance notice is required for fee changes?',\n",
    "        'category': 'Compliance',\n",
    "        'difficulty': 'easy',\n",
    "        'expected_answer': 'Fee changes require 30-day advance notice to customers.',\n",
    "        'relevant_doc_ids': ['CMP-002']\n",
    "    },\n",
    "    {\n",
    "        'question_id': 'Q011',\n",
    "        'question': 'If a customer has a Premium account and wants to avoid fees, what should they do?',\n",
    "        'category': 'Account Policies',\n",
    "        'difficulty': 'hard',\n",
    "        'expected_answer': 'To avoid the $25 monthly fee on a Premium account, customers should maintain a minimum daily balance of $2,500.',\n",
    "        'relevant_doc_ids': ['POL-003']\n",
    "    },\n",
    "    {\n",
    "        'question_id': 'Q012',\n",
    "        'question': 'What is the relationship between mobile app usage and customer retention?',\n",
    "        'category': 'Churn Analysis',\n",
    "        'difficulty': 'hard',\n",
    "        'expected_answer': 'Mobile app engagement reduces churn by 35%. Additionally, no mobile app usage for 90+ days is an early warning indicator of potential churn.',\n",
    "        'relevant_doc_ids': ['CHR-002', 'CHR-003']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_eval_questions = pd.DataFrame(evaluation_questions)\n",
    "df_eval_questions['created_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION QUESTIONS GENERATED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"✓ Generated {len(df_eval_questions)} evaluation questions\")\n",
    "print(f\"\\n\uD83D\uDCCA Question Statistics:\")\n",
    "print(f\"  • Categories: {df_eval_questions['category'].nunique()}\")\n",
    "print(f\"  • Difficulty levels: {df_eval_questions['difficulty'].nunique()}\")\n",
    "print(f\"\\n\uD83D\uDCC1 Questions by Category:\")\n",
    "print(df_eval_questions['category'].value_counts().to_string())\n",
    "print(f\"\\n\uD83D\uDCC8 Questions by Difficulty:\")\n",
    "print(df_eval_questions['difficulty'].value_counts().to_string())\n",
    "print(\"\\n\uD83D\uDCA1 These questions will be used to:\")\n",
    "print(\"  • Evaluate RAG retrieval quality\")\n",
    "print(\"  • Measure answer relevance and groundedness\")\n",
    "print(\"  • Compare different RAG configurations\")\n",
    "print(\"  • Track model performance over time\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "456aa4f2-edc9-4dda-816a-9cf63a276704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save evaluation questions to Unity Catalog\n",
    "df_eval_spark = spark.createDataFrame(df_eval_questions)\n",
    "eval_table_path = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{EVAL_QUESTIONS_TABLE}\"\n",
    "df_eval_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(eval_table_path)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION QUESTIONS SAVED TO UNITY CATALOG\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"✓ Table: {eval_table_path}\")\n",
    "print(f\"✓ Records: {df_eval_spark.count():,} questions\")\n",
    "print(\"\\n\uD83D\uDCA1 Governance benefits:\")\n",
    "print(\"  • Evaluation data versioned alongside knowledge base\")\n",
    "print(\"  • Complete lineage from questions → model → results\")\n",
    "print(\"  • Audit trail of all evaluation runs\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7646602e-db9a-42c4-9489-1b6fc16413f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 2: Build a Simple RAG Pipeline\n",
    "\n",
    "### What You'll Learn in This Section\n",
    "In this section, we will:\n",
    "1. Implement a simple retrieval mechanism (keyword-based search)\n",
    "2. Create a mock LLM response generator (simulated for cost control)\n",
    "3. Build a RAG pipeline that combines retrieval + generation\n",
    "4. Package the RAG pipeline as an MLflow model\n",
    "\n",
    "### Understanding RAG Architecture\n",
    "A **Retrieval-Augmented Generation (RAG)** system has two main components:\n",
    "\n",
    "**1. Retriever:** Searches the knowledge base to find relevant documents\n",
    "   - Input: User question\n",
    "   - Process: Search/similarity matching against knowledge base\n",
    "   - Output: Top-k most relevant documents\n",
    "\n",
    "**2. Generator:** Uses retrieved documents to generate an answer\n",
    "   - Input: User question + retrieved documents (context)\n",
    "   - Process: LLM generates answer grounded in the context\n",
    "   - Output: Natural language answer\n",
    "\n",
    "### Why This Matters for Governance\n",
    "**Reproducibility:** By packaging the RAG pipeline as an MLflow model, we can:\n",
    "- Version the entire pipeline (retrieval logic + generation logic)\n",
    "- Track which knowledge base version was used\n",
    "- Reproduce exact results from any experiment\n",
    "\n",
    "**Cost Control:** In this lab, we'll use a **simulated LLM** to avoid costs during development. In production, you would:\n",
    "- Track token usage per request\n",
    "- Implement caching to reduce redundant LLM calls\n",
    "- Set budget limits and alerts\n",
    "\n",
    "**Auditability:** Every component of the RAG pipeline is logged:\n",
    "- Retrieval parameters (top-k, similarity threshold)\n",
    "- Prompt templates used\n",
    "- Retrieved context for each question\n",
    "- Generated responses\n",
    "\n",
    "This creates a complete audit trail: \"For question X, the model retrieved documents Y and Z, used prompt template V, and generated response W.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2dd87a0-d287-4092-ab02-8fe5374379e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Implement Simple Retrieval Function\n",
    "\n",
    "#### How Retrieval Works\n",
    "The retriever searches the knowledge base to find documents relevant to the user's question. In production, this would use:\n",
    "- **Vector embeddings** (e.g., sentence transformers, OpenAI embeddings)\n",
    "- **Vector databases** (e.g., FAISS, Pinecone, Databricks Vector Search)\n",
    "- **Semantic similarity** (cosine similarity between question and document embeddings)\n",
    "\n",
    "For this lab, we'll use a **simple keyword-based retrieval** to demonstrate the concept without incurring embedding costs.\n",
    "\n",
    "#### Retrieval Parameters\n",
    "- **top_k**: Number of documents to retrieve (typically 3-5)\n",
    "- **min_score**: Minimum relevance score threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a618506-994f-4710-9e7a-4420c4f80525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def simple_keyword_retrieval(question: str, knowledge_base: pd.DataFrame, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Simple keyword-based retrieval (for demonstration purposes).\n",
    "    In production, use vector embeddings and semantic search.\n",
    "\n",
    "    Args:\n",
    "        question: User's question\n",
    "        knowledge_base: DataFrame with knowledge base documents\n",
    "        top_k: Number of documents to retrieve\n",
    "\n",
    "    Returns:\n",
    "        List of retrieved documents with metadata\n",
    "    \"\"\"\n",
    "    # Convert question to lowercase for matching\n",
    "    question_lower = question.lower()\n",
    "\n",
    "    # Simple scoring: count keyword matches\n",
    "    def score_document(doc_content: str, doc_title: str) -> float:\n",
    "        content_lower = doc_content.lower()\n",
    "        title_lower = doc_title.lower()\n",
    "\n",
    "        # Extract keywords from question (simple approach)\n",
    "        question_words = set(re.findall(r'\\w+', question_lower))\n",
    "        # Remove common stop words\n",
    "        stop_words = {'what', 'is', 'are', 'the', 'a', 'an', 'how', 'do', 'does', 'can', 'for', 'to', 'of', 'in', 'on', 'at'}\n",
    "        question_words = question_words - stop_words\n",
    "\n",
    "        # Count matches in content and title\n",
    "        content_matches = sum(1 for word in question_words if word in content_lower)\n",
    "        title_matches = sum(1 for word in question_words if word in title_lower)\n",
    "\n",
    "        # Weight title matches higher\n",
    "        score = content_matches + (title_matches * 2)\n",
    "        return score\n",
    "\n",
    "    # Score all documents\n",
    "    knowledge_base = knowledge_base.copy()\n",
    "    knowledge_base['relevance_score'] = knowledge_base.apply(\n",
    "        lambda row: score_document(row['content'], row['title']), axis=1\n",
    "    )\n",
    "\n",
    "    # Get top-k documents\n",
    "    top_docs = knowledge_base.nlargest(top_k, 'relevance_score')\n",
    "\n",
    "    # Format results\n",
    "    retrieved_docs = []\n",
    "    for _, doc in top_docs.iterrows():\n",
    "        retrieved_docs.append({\n",
    "            'doc_id': doc['doc_id'],\n",
    "            'title': doc['title'],\n",
    "            'content': doc['content'],\n",
    "            'category': doc['category'],\n",
    "            'relevance_score': float(doc['relevance_score']),\n",
    "            'version': doc['version']\n",
    "        })\n",
    "\n",
    "    return retrieved_docs\n",
    "\n",
    "# Test the retrieval function\n",
    "test_question = \"What is the overdraft protection limit?\"\n",
    "retrieved = simple_keyword_retrieval(test_question, df_knowledge_base, top_k=3)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RETRIEVAL FUNCTION TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"\\n✓ Retrieved {len(retrieved)} documents:\")\n",
    "for i, doc in enumerate(retrieved, 1):\n",
    "    print(f\"\\n{i}. {doc['title']} (Score: {doc['relevance_score']})\")\n",
    "    print(f\"   Doc ID: {doc['doc_id']}\")\n",
    "    print(f\"   Category: {doc['category']}\")\n",
    "    print(f\"   Content preview: {doc['content'][:100]}...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cab3ec2d-419a-42a3-a4f9-d074e099b4d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Prompt Template and Mock LLM Generator\n",
    "\n",
    "#### Understanding Prompt Engineering for RAG\n",
    "The prompt template is **critical** for RAG quality. It should:\n",
    "1. Clearly instruct the LLM to use only the provided context\n",
    "2. Specify the desired output format\n",
    "3. Handle cases where the context doesn't contain the answer\n",
    "4. Maintain a professional, helpful tone\n",
    "\n",
    "#### Cost Control Strategy\n",
    "In this lab, we use a **mock LLM** that generates rule-based responses. This allows us to:\n",
    "- Develop and test the RAG pipeline without LLM costs\n",
    "- Validate retrieval quality independently\n",
    "- Establish the MLflow tracking infrastructure\n",
    "\n",
    "In production, you would replace this with:\n",
    "- Databricks Foundation Model APIs (e.g., DBRX, Llama)\n",
    "- OpenAI API (GPT-4, GPT-3.5)\n",
    "- Azure OpenAI Service\n",
    "- Other LLM providers\n",
    "\n",
    "**Cost Tracking:** Always log token usage, model name, and cost per request in MLflow for budget monitoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4bbfe82-ad9c-4bf7-bdca-653de83f1f94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"You are a helpful customer support assistant for a financial services company.\n",
    "\n",
    "Use the following context to answer the customer's question. Only use information from the context provided.\n",
    "If the context doesn't contain enough information to answer the question, say so clearly.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "def mock_llm_generate(prompt: str, max_tokens: int = 200) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Mock LLM generator for demonstration (avoids API costs).\n",
    "    In production, replace with actual LLM API call.\n",
    "\n",
    "    Args:\n",
    "        prompt: Full prompt including context and question\n",
    "        max_tokens: Maximum tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with generated text and metadata\n",
    "    \"\"\"\n",
    "    # Extract question and context from prompt (simple parsing)\n",
    "    if \"Question:\" in prompt and \"Context:\" in prompt:\n",
    "        question_part = prompt.split(\"Question:\")[1].split(\"Answer:\")[0].strip()\n",
    "        context_part = prompt.split(\"Context:\")[1].split(\"Question:\")[0].strip()\n",
    "\n",
    "        # Simple rule-based response generation (mock)\n",
    "        # In production, this would be: response = openai.ChatCompletion.create(...)\n",
    "\n",
    "        # For demonstration, extract first sentence from context as answer\n",
    "        sentences = context_part.split('.')\n",
    "        answer = sentences[0].strip() + '.' if sentences else \"I don't have enough information to answer that question.\"\n",
    "\n",
    "        # Simulate token usage\n",
    "        prompt_tokens = len(prompt.split())\n",
    "        completion_tokens = len(answer.split())\n",
    "        total_tokens = prompt_tokens + completion_tokens\n",
    "\n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'model': 'mock-llm-v1',\n",
    "            'prompt_tokens': prompt_tokens,\n",
    "            'completion_tokens': completion_tokens,\n",
    "            'total_tokens': total_tokens,\n",
    "            'estimated_cost_usd': total_tokens * 0.00002  # Mock cost calculation\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'answer': \"Error: Invalid prompt format\",\n",
    "            'model': 'mock-llm-v1',\n",
    "            'prompt_tokens': 0,\n",
    "            'completion_tokens': 0,\n",
    "            'total_tokens': 0,\n",
    "            'estimated_cost_usd': 0.0\n",
    "        }\n",
    "\n",
    "# Test the mock LLM\n",
    "test_context = \"Overdraft protection is available for Premium and Gold account holders. The service covers overdrafts up to $500 with a $35 fee per occurrence.\"\n",
    "test_prompt = PROMPT_TEMPLATE.format(context=test_context, question=\"What is the overdraft limit?\")\n",
    "test_response = mock_llm_generate(test_prompt)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MOCK LLM GENERATOR TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Prompt:\\n{test_prompt}\\n\")\n",
    "print(f\"Generated Answer: {test_response['answer']}\")\n",
    "print(f\"\\n\uD83D\uDCCA Token Usage:\")\n",
    "print(f\"  • Prompt tokens: {test_response['prompt_tokens']}\")\n",
    "print(f\"  • Completion tokens: {test_response['completion_tokens']}\")\n",
    "print(f\"  • Total tokens: {test_response['total_tokens']}\")\n",
    "print(f\"  • Estimated cost: ${test_response['estimated_cost_usd']:.6f}\")\n",
    "print(\"\\n\uD83D\uDCA1 In production, replace mock_llm_generate() with actual LLM API\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fd6d8c3-d6a1-4800-bca4-f1de99a53f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Build Complete RAG Pipeline as MLflow PyFunc Model\n",
    "\n",
    "#### Why Package as MLflow PyFunc?\n",
    "**MLflow PyFunc** is a generic Python function flavor that allows you to package any Python code as an MLflow model. Benefits:\n",
    "- **Standardized interface**: All models have predict() method\n",
    "- **Dependency management**: MLflow tracks all required libraries\n",
    "- **Reproducibility**: Exact environment can be recreated\n",
    "- **Deployment flexibility**: Can deploy to various serving platforms\n",
    "\n",
    "#### Our RAG Pipeline Class\n",
    "We'll create a custom PyFunc model that:\n",
    "1. Loads the knowledge base from Unity Catalog\n",
    "2. Implements the retrieval logic\n",
    "3. Generates answers using the LLM\n",
    "4. Tracks all intermediate steps for auditability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70819423-455d-4460-94dd-bcdbb57392ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SimpleRAGModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    Simple RAG model that combines retrieval and generation.\n",
    "    Packaged as MLflow PyFunc for standardized deployment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, knowledge_base: pd.DataFrame, top_k: int = 3, prompt_template: str = PROMPT_TEMPLATE):\n",
    "        \"\"\"\n",
    "        Initialize RAG model with knowledge base and configuration.\n",
    "\n",
    "        Args:\n",
    "            knowledge_base: DataFrame containing documents\n",
    "            top_k: Number of documents to retrieve\n",
    "            prompt_template: Template for LLM prompts\n",
    "        \"\"\"\n",
    "        self.knowledge_base = knowledge_base\n",
    "        self.top_k = top_k\n",
    "        self.prompt_template = prompt_template\n",
    "        self.retrieval_stats = []  # Track retrieval for audit\n",
    "        self.generation_stats = []  # Track generation for cost monitoring\n",
    "\n",
    "    def retrieve(self, question: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant documents for a question.\"\"\"\n",
    "        return simple_keyword_retrieval(question, self.knowledge_base, self.top_k)\n",
    "\n",
    "    def generate(self, question: str, retrieved_docs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate answer using retrieved context.\"\"\"\n",
    "        # Combine retrieved documents into context\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Document {i+1} ({doc['doc_id']} - {doc['title']}):\\n{doc['content']}\"\n",
    "            for i, doc in enumerate(retrieved_docs)\n",
    "        ])\n",
    "\n",
    "        # Create prompt\n",
    "        prompt = self.prompt_template.format(context=context, question=question)\n",
    "\n",
    "        # Generate answer (using mock LLM)\n",
    "        response = mock_llm_generate(prompt)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def predict(self, context: Any, model_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main prediction method called by MLflow.\n",
    "\n",
    "        Args:\n",
    "            context: MLflow context (not used in this simple implementation)\n",
    "            model_input: DataFrame with 'question' column\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with answers and metadata\n",
    "        \"\"\"\n",
    "        # Handle different input formats\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            questions = model_input['question'].tolist()\n",
    "        elif isinstance(model_input, dict):\n",
    "            questions = [model_input['question']] if 'question' in model_input else []\n",
    "        else:\n",
    "            questions = [str(model_input)]\n",
    "\n",
    "        results = []\n",
    "        for question in questions:\n",
    "            # Retrieve relevant documents\n",
    "            retrieved_docs = self.retrieve(question)\n",
    "\n",
    "            # Generate answer\n",
    "            response = self.generate(question, retrieved_docs)\n",
    "\n",
    "            # Track stats for audit and cost monitoring\n",
    "            self.retrieval_stats.append({\n",
    "                'question': question,\n",
    "                'num_docs_retrieved': len(retrieved_docs),\n",
    "                'doc_ids': [doc['doc_id'] for doc in retrieved_docs],\n",
    "                'relevance_scores': [doc['relevance_score'] for doc in retrieved_docs]\n",
    "            })\n",
    "\n",
    "            self.generation_stats.append({\n",
    "                'question': question,\n",
    "                'tokens_used': response['total_tokens'],\n",
    "                'estimated_cost': response['estimated_cost_usd']\n",
    "            })\n",
    "\n",
    "            # Compile result\n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'answer': response['answer'],\n",
    "                'retrieved_doc_ids': [doc['doc_id'] for doc in retrieved_docs],\n",
    "                'retrieved_doc_titles': [doc['title'] for doc in retrieved_docs],\n",
    "                'num_docs_retrieved': len(retrieved_docs),\n",
    "                'total_tokens': response['total_tokens'],\n",
    "                'estimated_cost_usd': response['estimated_cost_usd']\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Create and test the RAG model\n",
    "rag_model = SimpleRAGModel(\n",
    "    knowledge_base=df_knowledge_base,\n",
    "    top_k=3,\n",
    "    prompt_template=PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "# Test with a sample question\n",
    "test_input = pd.DataFrame({'question': ['What is the overdraft protection limit for Premium accounts?']})\n",
    "test_output = rag_model.predict(context=None, model_input=test_input)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RAG PIPELINE TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Question: {test_output['question'].iloc[0]}\")\n",
    "print(f\"\\n✓ Answer: {test_output['answer'].iloc[0]}\")\n",
    "print(f\"\\n\uD83D\uDCC4 Retrieved Documents:\")\n",
    "for i, (doc_id, title) in enumerate(zip(test_output['retrieved_doc_ids'].iloc[0],\n",
    "                                         test_output['retrieved_doc_titles'].iloc[0]), 1):\n",
    "    print(f\"  {i}. {doc_id}: {title}\")\n",
    "print(f\"\\n\uD83D\uDCCA Resource Usage:\")\n",
    "print(f\"  • Documents retrieved: {test_output['num_docs_retrieved'].iloc[0]}\")\n",
    "print(f\"  • Tokens used: {test_output['total_tokens'].iloc[0]}\")\n",
    "print(f\"  • Estimated cost: ${test_output['estimated_cost_usd'].iloc[0]:.6f}\")\n",
    "print(\"\\n✓ RAG pipeline working correctly!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2caaa6b4-5afe-4eca-b317-d3b8c4069bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: MLflow Experiment Tracking for RAG\n",
    "\n",
    "### What You'll Learn in This Section\n",
    "In this section, we will:\n",
    "1. Set up an MLflow experiment for RAG development\n",
    "2. Log RAG-specific parameters (retrieval settings, prompt templates)\n",
    "3. Log artifacts (prompt templates, retrieved context samples, example responses)\n",
    "4. Track cost metrics (token usage, estimated costs)\n",
    "5. Compare different RAG configurations\n",
    "\n",
    "### Why Experiment Tracking Matters for RAG\n",
    "**Reproducibility Challenge:** RAG systems have many moving parts:\n",
    "- Knowledge base version\n",
    "- Retrieval algorithm and parameters\n",
    "- Prompt template\n",
    "- LLM model and parameters\n",
    "- Post-processing logic\n",
    "\n",
    "Without proper tracking, it's impossible to reproduce results or understand why one configuration outperforms another.\n",
    "\n",
    "**Cost Transparency:** LLM inference costs can escalate quickly. MLflow tracking provides:\n",
    "- Token usage per experiment\n",
    "- Cost per query\n",
    "- Total experiment cost\n",
    "- Comparison of cost vs. quality trade-offs\n",
    "\n",
    "**Audit Requirements:** Regulators need to know:\n",
    "- What prompt was used for a specific customer interaction?\n",
    "- Which documents were retrieved?\n",
    "- What was the model's reasoning?\n",
    "- Who approved this configuration for production?\n",
    "\n",
    "MLflow provides the complete audit trail.\n",
    "\n",
    "### What We'll Track\n",
    "- **Parameters**: top_k, prompt_template_version, model_name, retrieval_method\n",
    "- **Metrics**: average_tokens_per_query, total_cost, retrieval_coverage\n",
    "- **Artifacts**:\n",
    "  - Prompt template file\n",
    "  - Sample retrieved contexts (JSON)\n",
    "  - Example question-answer pairs\n",
    "  - Cost breakdown report\n",
    "- **Tags**: experiment_type, developer, purpose, compliance_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf4c46f7-5593-475a-8a10-65b0a35ed2fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up MLflow experiment for RAG development\n",
    "current_user = spark.sql('SELECT current_user()').collect()[0][0]\n",
    "experiment_name = f\"/Users/{current_user}/rag_support_assistant_experiments\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Configure MLflow to use Unity Catalog for model registry (modern API)\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MLFLOW EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"✓ Experiment: {experiment_name}\")\n",
    "print(f\"✓ Model Registry: Unity Catalog (databricks-uc)\")\n",
    "print(f\"✓ Registry URI: {mlflow.get_registry_uri()}\")\n",
    "print(f\"✓ Current User: {current_user}\")\n",
    "print(\"\\n\uD83D\uDCA1 All experiments will be tracked with:\")\n",
    "print(\"  • Complete parameter logging\")\n",
    "print(\"  • Artifact versioning\")\n",
    "print(\"  • Cost tracking\")\n",
    "print(\"  • Audit trail\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66b3460c-4046-406f-8229-aa61e64bae6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Helper Function: RAG Experiment Tracking\n",
    "\n",
    "We'll create a reusable function that:\n",
    "- Creates a RAG model with specific configuration\n",
    "- Evaluates it on test questions\n",
    "- Logs all parameters, metrics, and artifacts to MLflow\n",
    "- Tracks costs and resource usage\n",
    "- Returns the run ID for model registration\n",
    "\n",
    "This function demonstrates **production-grade experiment tracking** for RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9f73231-cea2-43c8-8f8b-af536db280ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_rag_experiment(\n",
    "    run_name: str,\n",
    "    knowledge_base: pd.DataFrame,\n",
    "    eval_questions: pd.DataFrame,\n",
    "    top_k: int = 3,\n",
    "    prompt_template: str = PROMPT_TEMPLATE,\n",
    "    tags: Dict[str, str] = None\n",
    ") -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run a complete RAG experiment with MLflow tracking.\n",
    "\n",
    "    Args:\n",
    "        run_name: Name for this experiment run\n",
    "        knowledge_base: DataFrame with knowledge base documents\n",
    "        eval_questions: DataFrame with evaluation questions\n",
    "        top_k: Number of documents to retrieve\n",
    "        prompt_template: Template for LLM prompts\n",
    "        tags: Additional tags for the run\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (run_id, metrics_dict)\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"RUNNING EXPERIMENT: {run_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        # 1. Log Parameters\n",
    "        params = {\n",
    "            'top_k': top_k,\n",
    "            'retrieval_method': 'keyword_based',\n",
    "            'prompt_template_version': 'v1.0',\n",
    "            'llm_model': 'mock-llm-v1',  # In production: 'gpt-4', 'dbrx-instruct', etc.\n",
    "            'knowledge_base_version': df_knowledge_base['version'].iloc[0],\n",
    "            'num_kb_documents': len(knowledge_base),\n",
    "            'num_eval_questions': len(eval_questions)\n",
    "        }\n",
    "        mlflow.log_params(params)\n",
    "        print(f\"✓ Logged {len(params)} parameters\")\n",
    "\n",
    "        # 2. Set Tags for Organization and Audit\n",
    "        default_tags = {\n",
    "            'model_type': 'RAG',\n",
    "            'experiment_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'data_source': f'{CATALOG_NAME}.{SCHEMA_NAME}',\n",
    "            'purpose': 'customer_support_assistant',\n",
    "            'developer': current_user,\n",
    "            'compliance_status': 'under_review'\n",
    "        }\n",
    "        if tags:\n",
    "            default_tags.update(tags)\n",
    "\n",
    "        for key, value in default_tags.items():\n",
    "            mlflow.set_tag(key, value)\n",
    "        print(f\"✓ Set {len(default_tags)} tags\")\n",
    "\n",
    "        # 3. Create and Evaluate RAG Model\n",
    "        rag_model = SimpleRAGModel(\n",
    "            knowledge_base=knowledge_base,\n",
    "            top_k=top_k,\n",
    "            prompt_template=prompt_template\n",
    "        )\n",
    "\n",
    "        # Run predictions on evaluation set\n",
    "        eval_input = eval_questions[['question']].copy()\n",
    "        predictions = rag_model.predict(context=None, model_input=eval_input)\n",
    "\n",
    "        # 4. Calculate Metrics\n",
    "        total_tokens = predictions['total_tokens'].sum()\n",
    "        total_cost = predictions['estimated_cost_usd'].sum()\n",
    "        avg_tokens_per_query = predictions['total_tokens'].mean()\n",
    "        avg_docs_retrieved = predictions['num_docs_retrieved'].mean()\n",
    "\n",
    "        metrics = {\n",
    "            'total_questions_evaluated': len(predictions),\n",
    "            'total_tokens_used': int(total_tokens),\n",
    "            'total_estimated_cost_usd': float(total_cost),\n",
    "            'avg_tokens_per_query': float(avg_tokens_per_query),\n",
    "            'avg_docs_retrieved_per_query': float(avg_docs_retrieved),\n",
    "            'max_tokens_single_query': int(predictions['total_tokens'].max()),\n",
    "            'min_tokens_single_query': int(predictions['total_tokens'].min())\n",
    "        }\n",
    "        mlflow.log_metrics(metrics)\n",
    "        print(f\"✓ Logged {len(metrics)} metrics\")\n",
    "\n",
    "        # 5. Log Artifacts\n",
    "\n",
    "        # 5a. Save prompt template\n",
    "        with open('/tmp/prompt_template.txt', 'w') as f:\n",
    "            f.write(prompt_template)\n",
    "        mlflow.log_artifact('/tmp/prompt_template.txt', 'config')\n",
    "\n",
    "        # 5b. Save sample predictions\n",
    "        sample_predictions = predictions.head(5).to_dict('records')\n",
    "        with open('/tmp/sample_predictions.json', 'w') as f:\n",
    "            json.dump(sample_predictions, f, indent=2)\n",
    "        mlflow.log_artifact('/tmp/sample_predictions.json', 'examples')\n",
    "\n",
    "        # 5c. Save retrieval statistics\n",
    "        retrieval_stats = {\n",
    "            'total_retrievals': len(rag_model.retrieval_stats),\n",
    "            'sample_retrievals': rag_model.retrieval_stats[:5]\n",
    "        }\n",
    "        with open('/tmp/retrieval_stats.json', 'w') as f:\n",
    "            json.dump(retrieval_stats, f, indent=2)\n",
    "        mlflow.log_artifact('/tmp/retrieval_stats.json', 'analysis')\n",
    "\n",
    "        # 5d. Save cost breakdown\n",
    "        cost_breakdown = {\n",
    "            'total_cost_usd': float(total_cost),\n",
    "            'cost_per_query_usd': float(total_cost / len(predictions)),\n",
    "            'total_tokens': int(total_tokens),\n",
    "            'tokens_per_query': float(avg_tokens_per_query),\n",
    "            'estimated_monthly_cost_1000_queries': float((total_cost / len(predictions)) * 1000)\n",
    "        }\n",
    "        with open('/tmp/cost_breakdown.json', 'w') as f:\n",
    "            json.dump(cost_breakdown, f, indent=2)\n",
    "        mlflow.log_artifact('/tmp/cost_breakdown.json', 'cost_analysis')\n",
    "\n",
    "        print(f\"✓ Logged 4 artifact files\")\n",
    "\n",
    "        # 6. Log Model with Signature\n",
    "        # Define input/output schema\n",
    "        input_schema = Schema([ColSpec(\"string\", \"question\")])\n",
    "        output_schema = Schema([\n",
    "            ColSpec(\"string\", \"question\"),\n",
    "            ColSpec(\"string\", \"answer\"),\n",
    "            ColSpec(\"long\", \"total_tokens\")\n",
    "        ])\n",
    "        signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "        # Log the model\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"model\",\n",
    "            python_model=rag_model,\n",
    "            signature=signature,\n",
    "            input_example={\"question\": \"What is the overdraft protection limit?\"}\n",
    "        )\n",
    "        print(f\"✓ Logged RAG model with signature\")\n",
    "\n",
    "        # 7. Print Summary\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EXPERIMENT COMPLETE: {run_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Run ID: {run.info.run_id}\")\n",
    "        print(f\"\\n\uD83D\uDCCA Key Metrics:\")\n",
    "        print(f\"  • Questions evaluated: {metrics['total_questions_evaluated']}\")\n",
    "        print(f\"  • Total tokens: {metrics['total_tokens_used']:,}\")\n",
    "        print(f\"  • Total cost: ${metrics['total_estimated_cost_usd']:.4f}\")\n",
    "        print(f\"  • Avg tokens/query: {metrics['avg_tokens_per_query']:.1f}\")\n",
    "        print(f\"  • Avg docs retrieved: {metrics['avg_docs_retrieved_per_query']:.1f}\")\n",
    "        print(f\"\\n\uD83D\uDCB0 Cost Projection:\")\n",
    "        print(f\"  • Cost per query: ${cost_breakdown['cost_per_query_usd']:.6f}\")\n",
    "        print(f\"  • Est. monthly cost (1000 queries): ${cost_breakdown['estimated_monthly_cost_1000_queries']:.2f}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "        return run.info.run_id, metrics\n",
    "\n",
    "print(\"✓ RAG experiment tracking function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24a4554b-fc1b-4a4a-9ef6-eea8715e164a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run Multiple RAG Experiments\n",
    "\n",
    "We'll run three different RAG configurations to compare performance:\n",
    "1. **Baseline (top_k=2)** - Retrieve fewer documents for faster, cheaper responses\n",
    "2. **Standard (top_k=3)** - Balanced configuration\n",
    "3. **Comprehensive (top_k=5)** - Retrieve more documents for better coverage\n",
    "\n",
    "Each configuration's parameters, metrics, costs, and artifacts will be logged to MLflow for comparison.\n",
    "\n",
    "#### Why Compare Different Configurations?\n",
    "**Cost vs. Quality Trade-off:** More retrieved documents means:\n",
    "- ✅ Better chance of finding relevant information\n",
    "- ✅ More complete answers\n",
    "- ❌ Higher token usage (longer context)\n",
    "- ❌ Higher costs\n",
    "- ❌ Slower response times\n",
    "\n",
    "MLflow tracking allows us to quantify these trade-offs and make data-driven decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56505602-d953-44f2-b708-fec37ddb614c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Experiment 1: Baseline Configuration (top_k=2)\n",
    "baseline_run_id, baseline_metrics = run_rag_experiment(\n",
    "    run_name=\"RAG_Baseline_top_k_2\",\n",
    "    knowledge_base=df_knowledge_base,\n",
    "    eval_questions=df_eval_questions,\n",
    "    top_k=2,\n",
    "    prompt_template=PROMPT_TEMPLATE,\n",
    "    tags={\n",
    "        'configuration': 'baseline',\n",
    "        'optimization_goal': 'cost_efficiency'\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9646928f-efd9-41d4-b97f-6607bd9df785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Experiment 2: Standard Configuration (top_k=3)\n",
    "standard_run_id, standard_metrics = run_rag_experiment(\n",
    "    run_name=\"RAG_Standard_top_k_3\",\n",
    "    knowledge_base=df_knowledge_base,\n",
    "    eval_questions=df_eval_questions,\n",
    "    top_k=3,\n",
    "    prompt_template=PROMPT_TEMPLATE,\n",
    "    tags={\n",
    "        'configuration': 'standard',\n",
    "        'optimization_goal': 'balanced'\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871f585e-eadc-4545-8e65-bfa669067401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Experiment 3: Comprehensive Configuration (top_k=5)\n",
    "comprehensive_run_id, comprehensive_metrics = run_rag_experiment(\n",
    "    run_name=\"RAG_Comprehensive_top_k_5\",\n",
    "    knowledge_base=df_knowledge_base,\n",
    "    eval_questions=df_eval_questions,\n",
    "    top_k=5,\n",
    "    prompt_template=PROMPT_TEMPLATE,\n",
    "    tags={\n",
    "        'configuration': 'comprehensive',\n",
    "        'optimization_goal': 'answer_quality'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0594c005-4a43-4ffe-aad3-ceac5e5243a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Compare RAG Configurations\n",
    "\n",
    "Let's compare all three RAG configurations across key metrics to determine the best balance of cost and quality.\n",
    "\n",
    "#### Decision Criteria\n",
    "When selecting a RAG configuration for production, consider:\n",
    "1. **Cost per query** - Can we afford this at scale?\n",
    "2. **Token usage** - Will we hit rate limits?\n",
    "3. **Answer quality** - Are responses accurate and complete? (requires human evaluation)\n",
    "4. **Latency** - How fast do responses need to be?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70eadd08-3028-4466-a83c-b85a7a865ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Configuration': ['Baseline (top_k=2)', 'Standard (top_k=3)', 'Comprehensive (top_k=5)'],\n",
    "    'Run_ID': [baseline_run_id, standard_run_id, comprehensive_run_id],\n",
    "    'Total_Tokens': [\n",
    "        baseline_metrics['total_tokens_used'],\n",
    "        standard_metrics['total_tokens_used'],\n",
    "        comprehensive_metrics['total_tokens_used']\n",
    "    ],\n",
    "    'Avg_Tokens_Per_Query': [\n",
    "        baseline_metrics['avg_tokens_per_query'],\n",
    "        standard_metrics['avg_tokens_per_query'],\n",
    "        comprehensive_metrics['avg_tokens_per_query']\n",
    "    ],\n",
    "    'Total_Cost_USD': [\n",
    "        baseline_metrics['total_estimated_cost_usd'],\n",
    "        standard_metrics['total_estimated_cost_usd'],\n",
    "        comprehensive_metrics['total_estimated_cost_usd']\n",
    "    ],\n",
    "    'Avg_Docs_Retrieved': [\n",
    "        baseline_metrics['avg_docs_retrieved_per_query'],\n",
    "        standard_metrics['avg_docs_retrieved_per_query'],\n",
    "        comprehensive_metrics['avg_docs_retrieved_per_query']\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Calculate cost per query\n",
    "comparison_df['Cost_Per_Query_USD'] = comparison_df['Total_Cost_USD'] / baseline_metrics['total_questions_evaluated']\n",
    "\n",
    "# Calculate relative cost (baseline = 100%)\n",
    "baseline_cost = comparison_df.loc[0, 'Total_Cost_USD']\n",
    "comparison_df['Relative_Cost_Pct'] = (comparison_df['Total_Cost_USD'] / baseline_cost * 100).round(1)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RAG CONFIGURATION COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identify best configuration based on balanced criteria\n",
    "# For this demo, we'll select standard (top_k=3) as the best balance\n",
    "best_config_idx = 1  # Standard configuration\n",
    "best_config_name = comparison_df.loc[best_config_idx, 'Configuration']\n",
    "best_run_id = comparison_df.loc[best_config_idx, 'Run_ID']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"RECOMMENDED CONFIGURATION: {best_config_name}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Run ID: {best_run_id}\")\n",
    "print(f\"\\n\uD83D\uDCCA Performance:\")\n",
    "print(f\"  • Avg tokens per query: {comparison_df.loc[best_config_idx, 'Avg_Tokens_Per_Query']:.1f}\")\n",
    "print(f\"  • Cost per query: ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD']:.6f}\")\n",
    "print(f\"  • Avg docs retrieved: {comparison_df.loc[best_config_idx, 'Avg_Docs_Retrieved']:.1f}\")\n",
    "print(f\"\\n\uD83D\uDCB0 Cost Analysis:\")\n",
    "print(f\"  • {comparison_df.loc[best_config_idx, 'Relative_Cost_Pct']:.0f}% of baseline cost\")\n",
    "print(f\"  • Estimated monthly cost (1000 queries): ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD'] * 1000:.2f}\")\n",
    "print(f\"\\n\uD83D\uDCA1 Rationale:\")\n",
    "print(f\"  • Balanced cost vs. quality trade-off\")\n",
    "print(f\"  • Retrieves enough context for accurate answers\")\n",
    "print(f\"  • Manageable token usage and costs\")\n",
    "print(f\"  • Good starting point for production deployment\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90c49301-6337-4cbb-8f3e-5d1cc6b05006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4: Model Registration in Unity Catalog\n",
    "\n",
    "### What You'll Learn in This Section\n",
    "In this section, we will:\n",
    "1. Register the best RAG configuration to Unity Catalog Model Registry\n",
    "2. Add comprehensive documentation for compliance and audit\n",
    "3. Track lineage from knowledge base to model\n",
    "4. Apply tags for governance and organization\n",
    "\n",
    "### Why Model Registration Matters for RAG\n",
    "**Centralized Model Storage:** Unity Catalog provides a single source of truth for all model versions across the organization. This prevents:\n",
    "- Shadow deployments (unauthorized models in production)\n",
    "- Version confusion (\"which model is actually serving traffic?\")\n",
    "- Lost models (\"where did we save that experiment from last month?\")\n",
    "\n",
    "**Access Control via RBAC:** Unity Catalog allows fine-grained permissions:\n",
    "- Data scientists can create and update models\n",
    "- ML engineers can promote models to production\n",
    "- Analysts can only read model metadata\n",
    "- Auditors can view all activity logs\n",
    "\n",
    "**Audit Logging:** Every operation is automatically logged:\n",
    "- Who registered the model?\n",
    "- When was it promoted to production?\n",
    "- Who has accessed the model?\n",
    "- What data was it trained on?\n",
    "\n",
    "**Lineage Tracking:** Unity Catalog automatically tracks:\n",
    "- Knowledge base version → Model version\n",
    "- Evaluation data → Model metrics\n",
    "- MLflow run → Registered model\n",
    "- Model version → Serving endpoint\n",
    "\n",
    "This creates a complete audit trail for regulatory compliance.\n",
    "\n",
    "### Modern API: Using Aliases Instead of Stages\n",
    "**Important:** MLflow 2.x deprecated the old \"Staging/Production/Archived\" stages in favor of **aliases**. Aliases provide:\n",
    "- ✅ Flexibility: Create any alias name (Champion, Challenger, Shadow, Canary)\n",
    "- ✅ Multiple aliases per version: A model can be both \"Champion\" and \"Approved\"\n",
    "- ✅ Better A/B testing support: Easy to manage multiple production variants\n",
    "- ✅ Clearer semantics: \"Champion\" is more meaningful than \"Production\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9057b9f-a3ce-42a0-a286-2bab2b6cd39b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the best RAG configuration to Unity Catalog\n",
    "print(\"=\" * 80)\n",
    "print(\"REGISTERING RAG MODEL TO UNITY CATALOG\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Configuration: {best_config_name}\")\n",
    "print(f\"Run ID: {best_run_id}\")\n",
    "print(f\"Model Name: {MODEL_NAME}\")\n",
    "\n",
    "# Create model registry entry using modern MLflow API\n",
    "model_version = mlflow.register_model(\n",
    "    model_uri=f\"runs:/{best_run_id}/model\",\n",
    "    name=MODEL_NAME,\n",
    "    tags={\n",
    "        \"model_type\": \"RAG\",\n",
    "        \"rag_configuration\": best_config_name,\n",
    "        \"training_date\": datetime.now().strftime('%Y-%m-%d'),\n",
    "        \"use_case\": \"customer_support_assistant\",\n",
    "        \"department\": \"data_science\",\n",
    "        \"compliance_status\": \"pending_review\",\n",
    "        \"cost_per_query_usd\": f\"{comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD']:.6f}\",\n",
    "        \"avg_tokens_per_query\": f\"{comparison_df.loc[best_config_idx, 'Avg_Tokens_Per_Query']:.1f}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Model registered successfully!\")\n",
    "print(f\"  • Model Name: {MODEL_NAME}\")\n",
    "print(f\"  • Version: {model_version.version}\")\n",
    "print(f\"  • Run ID: {best_run_id}\")\n",
    "print(f\"  • Status: {model_version.status}\")\n",
    "print(f\"\\n\uD83D\uDCA1 Model is now governed by Unity Catalog:\")\n",
    "print(f\"  • Centralized storage and versioning\")\n",
    "print(f\"  • RBAC-based access control\")\n",
    "print(f\"  • Complete audit logging\")\n",
    "print(f\"  • Automatic lineage tracking\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29d329dc-5c64-4ea2-ad29-b1cf095c0b93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Add Model Description and Documentation\n",
    "\n",
    "#### Why Documentation Matters\n",
    "Proper documentation is **critical** for:\n",
    "- **Compliance**: Regulators need to understand what the model does and how it works\n",
    "- **Governance**: Stakeholders need to approve models before production deployment\n",
    "- **Maintenance**: Future developers need to understand the model's purpose and limitations\n",
    "- **Incident Response**: When something goes wrong, documentation helps diagnose issues quickly\n",
    "\n",
    "#### What to Document for RAG Models\n",
    "RAG models require different documentation than traditional ML models:\n",
    "- ✅ Knowledge base version and source\n",
    "- ✅ Retrieval method and parameters\n",
    "- ✅ LLM model and version\n",
    "- ✅ Prompt template\n",
    "- ✅ Cost per query and scaling considerations\n",
    "- ✅ Evaluation methodology\n",
    "- ✅ Known limitations and failure modes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402da13f-0fda-4de4-a370-8f3fcf34472e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize MLflow client for model management\n",
    "client = MlflowClient()\n",
    "\n",
    "# Create comprehensive model description\n",
    "model_description = f\"\"\"\n",
    "# RAG Customer Support Assistant\n",
    "\n",
    "## Overview\n",
    "This is a Retrieval-Augmented Generation (RAG) system that answers customer support questions\n",
    "by retrieving relevant information from a knowledge base and generating natural language responses.\n",
    "\n",
    "## Architecture\n",
    "- **Retrieval Method**: Keyword-based search (production should use vector embeddings)\n",
    "- **LLM**: Mock LLM (production should use DBRX, GPT-4, or similar)\n",
    "- **Configuration**: {best_config_name}\n",
    "- **Top-K Documents**: {int(comparison_df.loc[best_config_idx, 'Avg_Docs_Retrieved'])}\n",
    "\n",
    "## Training/Development Details\n",
    "- **Development Date**: {datetime.now().strftime('%Y-%m-%d')}\n",
    "- **Knowledge Base**: {CATALOG_NAME}.{SCHEMA_NAME}.{KNOWLEDGE_BASE_TABLE}\n",
    "- **Knowledge Base Version**: {df_knowledge_base['version'].iloc[0]}\n",
    "- **Number of Documents**: {len(df_knowledge_base):,}\n",
    "- **Evaluation Questions**: {len(df_eval_questions):,}\n",
    "- **MLflow Run ID**: {best_run_id}\n",
    "\n",
    "## Performance Metrics\n",
    "- **Avg Tokens per Query**: {comparison_df.loc[best_config_idx, 'Avg_Tokens_Per_Query']:.1f}\n",
    "- **Cost per Query**: ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD']:.6f}\n",
    "- **Avg Documents Retrieved**: {comparison_df.loc[best_config_idx, 'Avg_Docs_Retrieved']:.1f}\n",
    "- **Total Tokens (Eval Set)**: {comparison_df.loc[best_config_idx, 'Total_Tokens']:,}\n",
    "\n",
    "## Cost Projections\n",
    "- **1,000 queries/month**: ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD'] * 1000:.2f}\n",
    "- **10,000 queries/month**: ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD'] * 10000:.2f}\n",
    "- **100,000 queries/month**: ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD'] * 100000:.2f}\n",
    "\n",
    "## Intended Use\n",
    "- Answer customer support questions about financial products and services\n",
    "- Provide accurate, grounded responses based on official documentation\n",
    "- Reduce support ticket volume by enabling self-service\n",
    "- Assist human agents with quick information retrieval\n",
    "\n",
    "## Limitations and Risks\n",
    "- **Mock LLM**: Current implementation uses a mock LLM for demonstration. Production deployment requires a real LLM.\n",
    "- **Keyword Retrieval**: Simple keyword matching may miss semantically similar documents. Production should use vector embeddings.\n",
    "- **No Guardrails**: No content filtering, toxicity detection, or PII redaction implemented.\n",
    "- **No Caching**: Every query hits the LLM. Production should implement caching for common questions.\n",
    "- **Knowledge Base Staleness**: Answers are only as current as the knowledge base. Requires regular updates.\n",
    "- **Hallucination Risk**: LLMs may generate plausible-sounding but incorrect information. Requires human review.\n",
    "\n",
    "## Compliance and Governance\n",
    "- **Data Lineage**: Complete lineage from knowledge base → retrieval → generation → response\n",
    "- **Access Control**: Model governed by Unity Catalog RBAC\n",
    "- **Audit Trail**: All model operations logged in Unity Catalog\n",
    "- **Cost Tracking**: Token usage and costs tracked in MLflow\n",
    "- **Reproducibility**: All experiments tracked with parameters, metrics, and artifacts\n",
    "\n",
    "## Deployment Requirements\n",
    "Before production deployment:\n",
    "1. ✅ Replace mock LLM with production LLM (DBRX, GPT-4, etc.)\n",
    "2. ✅ Implement vector-based retrieval (Databricks Vector Search)\n",
    "3. ✅ Add content filtering and guardrails\n",
    "4. ✅ Implement response caching\n",
    "5. ✅ Set up monitoring and alerting\n",
    "6. ✅ Conduct human evaluation of answer quality\n",
    "7. ✅ Obtain compliance approval\n",
    "8. ✅ Establish knowledge base update process\n",
    "\n",
    "## Maintenance Schedule\n",
    "- **Knowledge Base Updates**: Weekly or as needed\n",
    "- **Model Re-evaluation**: Monthly\n",
    "- **Cost Review**: Monthly\n",
    "- **Compliance Audit**: Quarterly\n",
    "\n",
    "## Contact\n",
    "- **Owner**: {current_user}\n",
    "- **Team**: Data Science\n",
    "- **Slack Channel**: #ml-rag-support (example)\n",
    "\"\"\"\n",
    "\n",
    "# Update model version description\n",
    "client.update_model_version(\n",
    "    name=MODEL_NAME,\n",
    "    version=model_version.version,\n",
    "    description=model_description\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL DOCUMENTATION ADDED\")\n",
    "print(\"=\" * 80)\n",
    "print(\"✓ Comprehensive documentation attached to model version\")\n",
    "print(\"\\n\uD83D\uDCCB Documentation includes:\")\n",
    "print(\"  • Architecture and configuration details\")\n",
    "print(\"  • Performance metrics and cost projections\")\n",
    "print(\"  • Intended use and limitations\")\n",
    "print(\"  • Compliance and governance information\")\n",
    "print(\"  • Deployment requirements and maintenance schedule\")\n",
    "print(\"\\n\uD83D\uDCA1 This documentation is critical for:\")\n",
    "print(\"  • Compliance review and approval\")\n",
    "print(\"  • Production deployment planning\")\n",
    "print(\"  • Future maintenance and updates\")\n",
    "print(\"  • Incident response and debugging\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "854d90ae-3ca9-4377-9686-7a995ad36f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 5: Model Version Management and Promotion\n",
    "\n",
    "### What You'll Learn in This Section\n",
    "In this section, we will:\n",
    "1. Promote the model using the \"Champion\" alias for production deployment\n",
    "2. Understand the modern alias-based workflow (vs. deprecated stages)\n",
    "3. Load the model from the registry for inference\n",
    "4. Demonstrate how production systems would use the model\n",
    "\n",
    "### Understanding Model Aliases\n",
    "**Modern Approach (MLflow 2.x+):** Use **aliases** for model lifecycle management.\n",
    "\n",
    "Common alias patterns:\n",
    "- **Champion**: Current production model serving live traffic\n",
    "- **Challenger**: New model being A/B tested against Champion\n",
    "- **Shadow**: Model receiving traffic for monitoring but not serving responses\n",
    "- **Canary**: Model serving a small percentage of traffic\n",
    "- **Approved**: Model approved by compliance but not yet deployed\n",
    "\n",
    "**Why Aliases > Stages:**\n",
    "- ✅ Multiple aliases per version (a model can be both \"Champion\" and \"Approved\")\n",
    "- ✅ Custom alias names that match your workflow\n",
    "- ✅ Better support for A/B testing and gradual rollouts\n",
    "- ✅ Clearer semantics (\"Champion\" vs. \"Production\")\n",
    "\n",
    "### Promotion Workflow\n",
    "In a real organization, model promotion would involve:\n",
    "1. **Development**: Data scientist creates and evaluates model\n",
    "2. **Review**: ML engineer reviews code, metrics, and documentation\n",
    "3. **Compliance**: Compliance team reviews for regulatory requirements\n",
    "4. **Approval**: Set \"Approved\" alias after compliance sign-off\n",
    "5. **Deployment**: ML engineer promotes to \"Champion\" and deploys to serving infrastructure\n",
    "6. **Monitoring**: Monitor performance, costs, and quality in production\n",
    "\n",
    "For this lab, we'll simulate the promotion to \"Champion\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba89120-f022-439a-af4d-70ce289772da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Promote model to Champion (production)\n",
    "print(\"=\" * 80)\n",
    "print(\"PROMOTING MODEL TO PRODUCTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "client.set_registered_model_alias(\n",
    "    name=MODEL_NAME,\n",
    "    alias=\"Champion\",\n",
    "    version=model_version.version\n",
    ")\n",
    "\n",
    "print(f\"✓ Model version {model_version.version} promoted to 'Champion'\")\n",
    "print(f\"\\n\uD83D\uDCCA Model Details:\")\n",
    "print(f\"  • Model Name: {MODEL_NAME}\")\n",
    "print(f\"  • Version: {model_version.version}\")\n",
    "print(f\"  • Alias: Champion\")\n",
    "print(f\"  • Configuration: {best_config_name}\")\n",
    "print(f\"  • Cost per query: ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD']:.6f}\")\n",
    "print(f\"\\n\uD83D\uDE80 Next Steps:\")\n",
    "print(f\"  1. Deploy to serving endpoint (Model Serving)\")\n",
    "print(f\"  2. Set up monitoring and alerting\")\n",
    "print(f\"  3. Configure autoscaling based on traffic\")\n",
    "print(f\"  4. Implement caching for common questions\")\n",
    "print(f\"  5. Set up cost budgets and alerts\")\n",
    "print(f\"\\n\uD83D\uDCA1 Production systems will load this model using:\")\n",
    "print(f\"  mlflow.pyfunc.load_model('models:/{MODEL_NAME}@Champion')\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38d4e710-6a4e-442e-aff3-4e13ea28583f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Model from Registry for Inference\n",
    "\n",
    "This demonstrates how production systems would load and use the registered model.\n",
    "\n",
    "#### Loading Patterns\n",
    "- **By alias**: `models:/{MODEL_NAME}@Champion` (recommended for production)\n",
    "- **By version**: `models:/{MODEL_NAME}/{version}` (for testing specific versions)\n",
    "- **Latest version**: `models:/{MODEL_NAME}/latest` (not recommended for production)\n",
    "\n",
    "**Best Practice**: Always use aliases in production to enable zero-downtime model updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25932049-df40-4e48-b73a-aac4db2b9aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load model using Champion alias\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING MODEL FROM REGISTRY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"models:/{MODEL_NAME}@Champion\")\n",
    "\n",
    "print(f\"✓ Model loaded successfully!\")\n",
    "print(f\"  • Model URI: models:/{MODEL_NAME}@Champion\")\n",
    "print(f\"  • Model Type: RAG Customer Support Assistant\")\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "sample_questions = pd.DataFrame({\n",
    "    'question': [\n",
    "        'What is the overdraft protection limit?',\n",
    "        'How do I dispute a transaction?',\n",
    "        'What are the benefits of a Premium account?'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(f\"\\n\uD83D\uDD2E Making predictions on {len(sample_questions)} sample questions...\")\n",
    "predictions = loaded_model.predict(sample_questions)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(f\"{'='*80}\")\n",
    "for i, row in predictions.iterrows():\n",
    "    print(f\"\\n❓ Question: {row['question']}\")\n",
    "    print(f\"\uD83D\uDCAC Answer: {row['answer']}\")\n",
    "    print(f\"\uD83D\uDCC4 Retrieved Docs: {', '.join(row['retrieved_doc_ids'])}\")\n",
    "    print(f\"\uD83D\uDCB0 Tokens: {row['total_tokens']}, Cost: ${row['estimated_cost_usd']:.6f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n✓ Model inference successful!\")\n",
    "print(f\"  • Total tokens used: {predictions['total_tokens'].sum()}\")\n",
    "print(f\"  • Total cost: ${predictions['estimated_cost_usd'].sum():.6f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38b3271c-3d13-497d-827e-9a42dbe4d85b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Simulate Model Version Updates\n",
    "\n",
    "#### Why Model Versioning Matters\n",
    "In production RAG systems, you'll continuously improve the model by:\n",
    "- Updating the knowledge base with new documents\n",
    "- Improving retrieval algorithms (keyword → vector search)\n",
    "- Upgrading to better LLMs (mock → GPT-3.5 → GPT-4 → DBRX)\n",
    "- Optimizing prompt templates\n",
    "- Adjusting retrieval parameters (top_k, similarity thresholds)\n",
    "\n",
    "**Unity Catalog tracks all versions**, allowing you to:\n",
    "- Roll back to previous versions if new version underperforms\n",
    "- A/B test new configurations against current production\n",
    "- Maintain multiple versions for different use cases\n",
    "- Audit which version was serving traffic at any point in time\n",
    "\n",
    "Let's simulate creating an improved version by running the comprehensive configuration (top_k=5) and registering it as a \"Challenger\" for A/B testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f429d1d2-cbf0-4bd5-94c7-558ace81b019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The comprehensive configuration (top_k=5) is already trained\n",
    "# Let's register it as a new version for A/B testing\n",
    "print(\"=\" * 80)\n",
    "print(\"REGISTERING CHALLENGER MODEL FOR A/B TESTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model_version_v2 = mlflow.register_model(\n",
    "    model_uri=f\"runs:/{comprehensive_run_id}/model\",\n",
    "    name=MODEL_NAME,\n",
    "    tags={\n",
    "        \"model_type\": \"RAG\",\n",
    "        \"rag_configuration\": \"Comprehensive (top_k=5)\",\n",
    "        \"training_date\": datetime.now().strftime('%Y-%m-%d'),\n",
    "        \"use_case\": \"customer_support_assistant\",\n",
    "        \"version_notes\": \"Higher retrieval coverage (top_k=5) for better answer quality\",\n",
    "        \"department\": \"data_science\",\n",
    "        \"cost_per_query_usd\": f\"{comparison_df.loc[2, 'Cost_Per_Query_USD']:.6f}\",  # Comprehensive is index 2\n",
    "        \"avg_tokens_per_query\": f\"{comparison_df.loc[2, 'Avg_Tokens_Per_Query']:.1f}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"✓ New model version registered: {model_version_v2.version}\")\n",
    "print(f\"  • Configuration: Comprehensive (top_k=5)\")\n",
    "print(f\"  • Run ID: {comprehensive_run_id}\")\n",
    "print(f\"  • Cost per query: ${comparison_df.loc[2, 'Cost_Per_Query_USD']:.6f}\")\n",
    "\n",
    "# Set as Challenger for A/B testing\n",
    "client.set_registered_model_alias(\n",
    "    name=MODEL_NAME,\n",
    "    alias=\"Challenger\",\n",
    "    version=model_version_v2.version\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Model version {model_version_v2.version} set as 'Challenger'\")\n",
    "print(f\"\\n\uD83D\uDD2C A/B Testing Setup:\")\n",
    "print(f\"  • Champion (v{model_version.version}): Standard config (top_k=3)\")\n",
    "print(f\"    - Cost: ${comparison_df.loc[1, 'Cost_Per_Query_USD']:.6f}/query\")\n",
    "print(f\"    - Tokens: {comparison_df.loc[1, 'Avg_Tokens_Per_Query']:.1f}/query\")\n",
    "print(f\"  • Challenger (v{model_version_v2.version}): Comprehensive config (top_k=5)\")\n",
    "print(f\"    - Cost: ${comparison_df.loc[2, 'Cost_Per_Query_USD']:.6f}/query\")\n",
    "print(f\"    - Tokens: {comparison_df.loc[2, 'Avg_Tokens_Per_Query']:.1f}/query\")\n",
    "print(f\"\\n\uD83D\uDCA1 Next Steps:\")\n",
    "print(f\"  1. Deploy both versions to serving endpoints\")\n",
    "print(f\"  2. Route 90% traffic to Champion, 10% to Challenger\")\n",
    "print(f\"  3. Monitor answer quality, costs, and user satisfaction\")\n",
    "print(f\"  4. If Challenger performs better, promote to Champion\")\n",
    "print(f\"  5. If not, delete Challenger alias and stick with Champion\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1164ed93-002a-404e-9269-1b3d13ba0e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### View All Model Versions\n",
    "\n",
    "Let's examine all versions of our registered model, their aliases, and metadata.\n",
    "\n",
    "#### Why Version History Matters\n",
    "**Audit Trail:** Regulators may ask: \"What model was serving traffic on date X?\"\n",
    "- Unity Catalog maintains complete version history\n",
    "- Each version is immutable (cannot be changed after registration)\n",
    "- Timestamps track when each version was created and promoted\n",
    "\n",
    "**Rollback Capability:** If a new version causes issues in production:\n",
    "- Quickly switch the \"Champion\" alias back to previous version\n",
    "- No need to retrain or redeploy\n",
    "- Zero downtime rollback\n",
    "\n",
    "**Cost Tracking:** Compare costs across versions:\n",
    "- Which configuration is most cost-effective?\n",
    "- How much did costs increase with the new LLM?\n",
    "- What's the ROI of upgrading retrieval quality?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b906b1-0be9-4e1f-b410-8f2e0a5d7099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get all versions of the model\n",
    "all_versions = client.search_model_versions(f\"name='{MODEL_NAME}'\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"ALL VERSIONS OF {MODEL_NAME}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sort versions by version number (descending)\n",
    "sorted_versions = sorted(all_versions, key=lambda v: int(v.version), reverse=True)\n",
    "\n",
    "for version in sorted_versions:\n",
    "    print(f\"\\n\uD83D\uDCE6 Version {version.version}\")\n",
    "    print(f\"  • Run ID: {version.run_id}\")\n",
    "    print(f\"  • Status: {version.status}\")\n",
    "\n",
    "    # Handle aliases - get the aliases list\n",
    "    try:\n",
    "        # Try to get aliases as a property or method\n",
    "        if callable(getattr(version, 'aliases', None)):\n",
    "            aliases_list = version.aliases()\n",
    "        else:\n",
    "            aliases_list = version.aliases\n",
    "\n",
    "        # Format aliases for display\n",
    "        if aliases_list and len(aliases_list) > 0:\n",
    "            if isinstance(aliases_list, list):\n",
    "                aliases_str = ', '.join(aliases_list)\n",
    "            else:\n",
    "                aliases_str = str(aliases_list)\n",
    "        else:\n",
    "            aliases_str = 'None'\n",
    "    except Exception as e:\n",
    "        aliases_str = 'None'\n",
    "\n",
    "    print(f\"  • Aliases: {aliases_str}\")\n",
    "    print(f\"  • Created: {datetime.fromtimestamp(version.creation_timestamp/1000).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    # Get tags for this version\n",
    "    try:\n",
    "        # Check if tags is a method or property\n",
    "        if callable(getattr(version, 'tags', None)):\n",
    "            tags_dict = version.tags()\n",
    "        else:\n",
    "            tags_dict = version.tags\n",
    "\n",
    "        # Display tags if they exist\n",
    "        if tags_dict and isinstance(tags_dict, dict):\n",
    "            print(f\"  • Configuration: {tags_dict.get('rag_configuration', 'N/A')}\")\n",
    "            print(f\"  • Cost/query: {tags_dict.get('cost_per_query_usd', 'N/A')}\")\n",
    "            print(f\"  • Avg tokens: {tags_dict.get('avg_tokens_per_query', 'N/A')}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n✓ Total versions: {len(sorted_versions)}\")\n",
    "print(f\"\\n\uD83D\uDCA1 Version Management:\")\n",
    "print(f\"  • All versions are immutable and permanently stored\")\n",
    "print(f\"  • Aliases can be moved between versions instantly\")\n",
    "print(f\"  • Complete audit trail of all version operations\")\n",
    "print(f\"  • Can load any version for inference or comparison\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1301d857-e7a2-47e1-9c08-b9c0df708bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 6: Unity Catalog Governance Controls\n",
    "\n",
    "Unity Catalog provides enterprise-grade governance features:\n",
    "- **RBAC (Role-Based Access Control)**: Control who can read, write, or execute models\n",
    "- **Audit Logging**: Track all operations on models and data\n",
    "- **Data Lineage**: Trace models back to training data\n",
    "\n",
    "Let's explore these governance capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ab0c1c2-6a7a-4630-95c3-2619bd86d270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Access Control with RBAC (Demonstration)\n",
    "\n",
    "Unity Catalog provides enterprise-grade access control through Role-Based Access Control (RBAC).\n",
    "\n",
    "**How RBAC Works in Production:**\n",
    "1. **Account admins** create groups at **account level** (not workspace level)\n",
    "2. **Users are added** to groups based on their roles\n",
    "3. **Permissions are granted** to groups, not individual users\n",
    "4. **Users inherit** permissions from all groups they belong to\n",
    "\n",
    "**Typical Groups in ML Projects:**\n",
    "- `data_analysts` - Read access to data tables\n",
    "- `ml_engineers` - Model execution and deployment rights\n",
    "- `data_scientists` - Full access to develop and train models\n",
    "- `data_engineers` - Data pipeline and table management\n",
    "\n",
    "**Important: Workspace vs. Account Groups**\n",
    "- Unity Catalog requires **account-level groups** (created in Account Console)\n",
    "- Workspace-level groups (created with `CREATE GROUP`) **do NOT work** with Unity Catalog\n",
    "- Only account admins can create account-level groups\n",
    "- This is a common source of confusion!\n",
    "\n",
    "**How to Create Account-Level Groups:**\n",
    "\n",
    "*Azure Databricks Account Console (UI):*\n",
    "1. Sign in to the Databricks account console (not a workspace)\n",
    "2. In Azure, go to **accounts.azuredatabricks.net** (or accounts.cloud.databricks.com for AWS/GCP)\n",
    "3. Log in as an **account admin**\n",
    "4. Navigate to the **User Management** section\n",
    "5. Select **Groups** tab\n",
    "6. Click **Add Group** button\n",
    "7. Enter group name (e.g., `ml_engineers`)\n",
    "8. Press **Add** button\n",
    "9. Repeat for all required groups: `data_analysts`, `ml_engineers`, `data_scientists`, `data_engineers`, `all_users`\n",
    "\n",
    "*Alternative - Databricks CLI:*\n",
    "```\n",
    "databricks account groups create --group-name data_analysts\n",
    "databricks account groups create --group-name ml_engineers\n",
    "databricks account groups create --group-name data_scientists\n",
    "databricks account groups create --group-name data_engineers\n",
    "databricks account groups create --group-name all_users\n",
    "```\n",
    "\n",
    "**For This Lab:**\n",
    "- If you have account-level groups, the notebook will detect and use them\n",
    "- If not, we'll demonstrate the concepts with your current user\n",
    "- Example commands show what admins would run in production\n",
    "- You'll learn the complete RBAC workflow either way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "334139bd-ba88-4b09-97ed-611f3c8fff5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if account-level groups exist\n",
    "print(\"=== Checking for Account-Level Groups ===\\n\")\n",
    "\n",
    "print(\"⚠ Important: Unity Catalog requires ACCOUNT-LEVEL groups\")\n",
    "print(\"  • Workspace groups (CREATE GROUP) do NOT work with Unity Catalog\")\n",
    "print(\"  • Only account admins can create account-level groups\")\n",
    "print(\"  • Groups must be created in the Account Console\\n\")\n",
    "\n",
    "# Define required groups\n",
    "required_groups = {\n",
    "    'data_analysts': 'Group for data analysts with read access to data',\n",
    "    'ml_engineers': 'Group for ML engineers with model execution rights',\n",
    "    'data_scientists': 'Group for data scientists with full schema access',\n",
    "    'data_engineers': 'Group for data engineers with data pipeline management',\n",
    "    'all_users': 'Group for all users with basic catalog access'\n",
    "}\n",
    "\n",
    "print(\"Required groups for this lab:\")\n",
    "for group_name, description in required_groups.items():\n",
    "    print(f\"  • {group_name}: {description}\")\n",
    "\n",
    "# Check if account-level groups exist (read-only check)\n",
    "print(\"\\nChecking if groups exist at account level...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "existing_groups = []\n",
    "missing_groups = []\n",
    "\n",
    "for group_name in required_groups.keys():\n",
    "    try:\n",
    "        # Try to grant a harmless permission to test if group exists\n",
    "        # We'll immediately revoke it, so this is just a test\n",
    "        # If group doesn't exist, this will fail with PRINCIPAL_DOES_NOT_EXIST\n",
    "        test_sql = f\"GRANT USAGE ON CATALOG {CATALOG_NAME} TO `{group_name}`\"\n",
    "        spark.sql(test_sql)\n",
    "\n",
    "        # If we got here, group exists! Now revoke the test grant\n",
    "        try:\n",
    "            spark.sql(f\"REVOKE USAGE ON CATALOG {CATALOG_NAME} FROM `{group_name}`\")\n",
    "        except:\n",
    "            pass  # Revoke might fail if already granted, that's ok\n",
    "\n",
    "        print(f\"✓ {group_name}: Exists (account-level group)\")\n",
    "        existing_groups.append(group_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e).lower()\n",
    "        if \"principal_does_not_exist\" in error_msg or \"does not exist\" in error_msg or \"cannot find\" in error_msg:\n",
    "            print(f\"⊘ {group_name}: Does not exist at account level\")\n",
    "            missing_groups.append(group_name)\n",
    "        elif \"already granted\" in error_msg or \"already has\" in error_msg:\n",
    "            # Group exists, permission was already granted\n",
    "            print(f\"✓ {group_name}: Exists (account-level group)\")\n",
    "            existing_groups.append(group_name)\n",
    "        elif \"permission\" in error_msg or \"privilege\" in error_msg:\n",
    "            # Can't verify due to permissions, but let's assume it might exist\n",
    "            print(f\"? {group_name}: Cannot verify (insufficient permissions)\")\n",
    "            print(f\"  Will attempt to use this group in permission grants\")\n",
    "            existing_groups.append(group_name)  # Optimistically add it\n",
    "        else:\n",
    "            print(f\"? {group_name}: Cannot verify ({str(e)[:80]}...)\")\n",
    "            missing_groups.append(group_name)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GROUP CHECK SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store available groups for later use\n",
    "available_groups = existing_groups\n",
    "\n",
    "if len(existing_groups) > 0:\n",
    "    print(f\"\\n✓ Account-level groups found: {len(existing_groups)}\")\n",
    "    for group in existing_groups:\n",
    "        print(f\"  ✓ {group}\")\n",
    "    print(\"\\n  \uD83C\uDF89 Excellent! These groups will be used for permission grants.\")\n",
    "else:\n",
    "    print(\"\\n⊘ No account-level groups found\")\n",
    "\n",
    "if len(missing_groups) > 0:\n",
    "    print(f\"\\n⊘ Groups not found: {len(missing_groups)}\")\n",
    "    for group in missing_groups:\n",
    "        print(f\"  ⊘ {group}\")\n",
    "\n",
    "    print(\"\\n\uD83D\uDCDD How to Create Account-Level Groups:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Account-level groups MUST be created in the Databricks Account Console:\")\n",
    "    print(\"\")\n",
    "    print(\"Option 1: Azure Databricks Account Console (UI) - Recommended\")\n",
    "    print(\"  1. Sign in to the Databricks account console (not a workspace)\")\n",
    "    print(\"  2. In Azure, go to: accounts.azuredatabricks.net\")\n",
    "    print(\"     (or accounts.cloud.databricks.com for AWS/GCP)\")\n",
    "    print(\"  3. Log in as an account admin\")\n",
    "    print(\"  4. Navigate to: User Management section\")\n",
    "    print(\"  5. Select: Groups tab\")\n",
    "    print(\"  6. Click: Add Group button\")\n",
    "    print(\"  7. Enter group name (e.g., ml_engineers)\")\n",
    "    print(\"  8. Press: Add button\")\n",
    "    print(\"  9. Repeat for all groups: data_analysts, ml_engineers, data_scientists,\")\n",
    "    print(\"     data_engineers, all_users\")\n",
    "    print(\"\")\n",
    "    print(\"Option 2: Databricks CLI (for Account Admins)\")\n",
    "    print(\"  databricks account groups create --group-name data_analysts\")\n",
    "    print(\"  databricks account groups create --group-name ml_engineers\")\n",
    "    print(\"  databricks account groups create --group-name data_scientists\")\n",
    "    print(\"  databricks account groups create --group-name data_engineers\")\n",
    "    print(\"  databricks account groups create --group-name all_users\")\n",
    "    print(\"\")\n",
    "    print(\"⚠ Note: CREATE GROUP in SQL creates workspace groups, NOT account groups\")\n",
    "    print(\"  Workspace groups do NOT work with Unity Catalog permissions!\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA Total available groups for permissions: {len(available_groups)}\")\n",
    "if len(available_groups) > 0:\n",
    "    print(\"  These groups will be used in the permission granting section.\")\n",
    "else:\n",
    "    print(\"  No groups available - will demonstrate with current user only.\")\n",
    "    print(\"  This is normal and the lab will still teach all RBAC concepts.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e35da57-71c1-46f9-867d-8583dfac7cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Understanding Workspace vs. Account Groups\n",
    "\n",
    "**Important Distinction:**\n",
    "- `SHOW GROUPS` displays **workspace-level groups** (created with `CREATE GROUP`)\n",
    "- Unity Catalog requires **account-level groups** (created in Account Console)\n",
    "- These are **completely separate** and cannot be used interchangeably!\n",
    "\n",
    "Let's check both to understand the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27592f8c-5fd4-48fa-bf21-9f89505adc66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display workspace groups vs account groups\n",
    "print(\"=== Understanding Group Types ===\\n\")\n",
    "\n",
    "print(\"⚠ CRITICAL: Workspace Groups ≠ Account Groups\")\n",
    "print(\"  • SHOW GROUPS shows workspace groups\")\n",
    "print(\"  • Unity Catalog needs account groups\")\n",
    "print(\"  • They are completely separate!\\n\")\n",
    "\n",
    "# Check workspace groups\n",
    "print(\"1. Workspace Groups (from SHOW GROUPS):\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    workspace_groups = spark.sql(\"SHOW GROUPS\")\n",
    "    workspace_group_list = [row[0] for row in workspace_groups.collect()]\n",
    "\n",
    "    if len(workspace_group_list) > 0:\n",
    "        print(f\"Found {len(workspace_group_list)} workspace group(s):\")\n",
    "        display(workspace_groups)\n",
    "\n",
    "        print(\"\\nChecking our required groups in workspace:\")\n",
    "        for group_name in required_groups.keys():\n",
    "            if group_name in workspace_group_list:\n",
    "                print(f\"  ✓ {group_name} - Found in workspace\")\n",
    "            else:\n",
    "                print(f\"  ✗ {group_name} - Not in workspace\")\n",
    "\n",
    "        print(\"\\n⚠ WARNING: These are WORKSPACE groups!\")\n",
    "        print(\"  They will NOT work with Unity Catalog permissions.\")\n",
    "        print(\"  Unity Catalog requires ACCOUNT-LEVEL groups.\")\n",
    "    else:\n",
    "        print(\"No workspace groups found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Unable to list workspace groups: {str(e)}\")\n",
    "\n",
    "# Check account groups (the ones that actually work with Unity Catalog)\n",
    "print(\"\\n2. Account Groups (for Unity Catalog):\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Checking if groups exist at ACCOUNT level (required for Unity Catalog)...\\n\")\n",
    "\n",
    "account_groups_found = []\n",
    "account_groups_missing = []\n",
    "\n",
    "for group_name in required_groups.keys():\n",
    "    try:\n",
    "        # Try to grant a test permission to see if group exists\n",
    "        # This is the most reliable way to check across all Databricks versions\n",
    "        test_sql = f\"GRANT USAGE ON CATALOG {CATALOG_NAME} TO `{group_name}`\"\n",
    "        spark.sql(test_sql)\n",
    "\n",
    "        # If we got here, group exists! Revoke the test grant\n",
    "        try:\n",
    "            spark.sql(f\"REVOKE USAGE ON CATALOG {CATALOG_NAME} FROM `{group_name}`\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print(f\"  ✓ {group_name} - EXISTS at account level (works with Unity Catalog)\")\n",
    "        account_groups_found.append(group_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e).lower()\n",
    "        if \"principal_does_not_exist\" in error_msg or \"does not exist\" in error_msg or \"cannot find\" in error_msg:\n",
    "            print(f\"  ✗ {group_name} - DOES NOT EXIST at account level\")\n",
    "            account_groups_missing.append(group_name)\n",
    "        elif \"already granted\" in error_msg or \"already has\" in error_msg:\n",
    "            # Group exists, permission was already there\n",
    "            print(f\"  ✓ {group_name} - EXISTS at account level (works with Unity Catalog)\")\n",
    "            account_groups_found.append(group_name)\n",
    "        elif \"permission\" in error_msg or \"privilege\" in error_msg:\n",
    "            print(f\"  ? {group_name} - Cannot verify (insufficient permissions)\")\n",
    "            account_groups_missing.append(group_name)\n",
    "        else:\n",
    "            print(f\"  ? {group_name} - Cannot verify: {str(e)[:60]}...\")\n",
    "            account_groups_missing.append(group_name)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GROUP TYPE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    if len(workspace_group_list) > 0:\n",
    "        print(f\"\\n\uD83D\uDCCB Workspace Groups: {len(workspace_group_list)} found\")\n",
    "        print(\"  ⚠ These do NOT work with Unity Catalog\")\n",
    "        print(\"  ⚠ Created with: CREATE GROUP\")\n",
    "        print(\"  ⚠ Only work for legacy workspace permissions\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if len(account_groups_found) > 0:\n",
    "    print(f\"\\n✓ Account Groups: {len(account_groups_found)} found\")\n",
    "    print(\"  ✓ These WORK with Unity Catalog\")\n",
    "    for group in account_groups_found:\n",
    "        print(f\"    • {group}\")\n",
    "else:\n",
    "    print(f\"\\n✗ Account Groups: 0 found\")\n",
    "    print(\"  ✗ Unity Catalog permissions will not work\")\n",
    "\n",
    "if len(account_groups_missing) > 0:\n",
    "    print(f\"\\n⊘ Missing Account Groups: {len(account_groups_missing)}\")\n",
    "    for group in account_groups_missing:\n",
    "        print(f\"    • {group}\")\n",
    "    print(\"\\n  \uD83D\uDCA1 To create account-level groups:\")\n",
    "    print(\"     1. Go to: https://accounts.cloud.databricks.com/\")\n",
    "    print(\"     2. User Management → Groups → Add Group\")\n",
    "    print(\"     3. Create each group at ACCOUNT level\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9c61d25-1aee-491b-a0a4-e6dd21b2e954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Grant Permissions (Demonstration)\n",
    "\n",
    "Unity Catalog allows fine-grained permissions. Here's how permissions would be granted in production:\n",
    "\n",
    "**Typical Permission Structure:**\n",
    "- **data_analysts**: SELECT on table (read-only access)\n",
    "- **ml_engineers**: USE SCHEMA on schema (model execution and schema access)\n",
    "- **data_scientists**: ALL PRIVILEGES on schema (full access)\n",
    "- **data_engineers**: MODIFY on table (write access for data pipelines)\n",
    "- **all_users**: USE CATALOG on catalog (basic catalog access)\n",
    "\n",
    "**Note:** This section demonstrates the concepts. In production, your admin would create groups and grant permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3140a814-dcf3-4362-b604-8669ad9d7bee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Demonstrate permission granting concepts\n",
    "print(\"=== Unity Catalog Permissions (Demonstration) ===\\n\")\n",
    "\n",
    "# Get current user\n",
    "current_user = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "print(f\"Current user: {current_user}\\n\")\n",
    "\n",
    "# Define table paths for RBAC examples (using knowledge base as primary table)\n",
    "table_path = kb_table_path  # Primary table for RBAC examples\n",
    "\n",
    "# Show example permission commands\n",
    "print(\"In production, an admin would execute commands like:\\n\")\n",
    "\n",
    "example_grants = [\n",
    "    {\n",
    "        'description': 'Grant read access to data analysts (knowledge base)',\n",
    "        'sql': f\"GRANT SELECT ON TABLE {kb_table_path} TO `data_analysts`;\"\n",
    "    },\n",
    "    {\n",
    "        'description': 'Grant schema usage to ML engineers',\n",
    "        'sql': f\"GRANT USE SCHEMA ON SCHEMA {CATALOG_NAME}.{SCHEMA_NAME} TO `ml_engineers`;\"\n",
    "    },\n",
    "    {\n",
    "        'description': 'Grant full access to data scientists',\n",
    "        'sql': f\"GRANT ALL PRIVILEGES ON SCHEMA {CATALOG_NAME}.{SCHEMA_NAME} TO `data_scientists`;\"\n",
    "    },\n",
    "    {\n",
    "        'description': 'Grant write access to data engineers (for KB updates)',\n",
    "        'sql': f\"GRANT MODIFY ON TABLE {kb_table_path} TO `data_engineers`;\"\n",
    "    },\n",
    "    {\n",
    "        'description': 'Grant catalog usage to all users',\n",
    "        'sql': f\"GRANT USE CATALOG ON CATALOG {CATALOG_NAME} TO `all_users`;\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, grant in enumerate(example_grants, 1):\n",
    "    print(f\"{i}. {grant['description']}\")\n",
    "    print(f\"   {grant['sql']}\")\n",
    "    print()\n",
    "\n",
    "# Try to grant permissions to production groups (if they exist) and current user\n",
    "print(\"=\"*80)\n",
    "print(\"Attempting to grant permissions...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if we have available_groups from earlier section\n",
    "try:\n",
    "    available_groups_list = available_groups\n",
    "    print(f\"\\nℹ Available groups from creation section: {len(available_groups_list)}\")\n",
    "    if len(available_groups_list) > 0:\n",
    "        print(f\"  Groups: {', '.join(available_groups_list)}\")\n",
    "except NameError:\n",
    "    # If available_groups doesn't exist, we'll try all groups and handle errors\n",
    "    available_groups_list = []\n",
    "    print(\"\\nℹ No group information from creation section - will attempt all groups\")\n",
    "\n",
    "successful_grants = []\n",
    "failed_grants = []\n",
    "groups_granted = []\n",
    "groups_not_found = []\n",
    "\n",
    "# Define production permissions to try\n",
    "production_permissions = [\n",
    "    {\n",
    "        'principal': 'data_analysts',\n",
    "        'privilege': 'SELECT',\n",
    "        'object_type': 'TABLE',\n",
    "        'object_name': table_path,\n",
    "        'description': 'Read access to RAG knowledge base'\n",
    "    },\n",
    "    {\n",
    "        'principal': 'ml_engineers',\n",
    "        'privilege': 'USE SCHEMA',\n",
    "        'object_type': 'SCHEMA',\n",
    "        'object_name': f\"{CATALOG_NAME}.{SCHEMA_NAME}\",\n",
    "        'description': 'Schema usage rights for RAG deployment'\n",
    "    },\n",
    "    {\n",
    "        'principal': 'data_scientists',\n",
    "        'privilege': 'ALL PRIVILEGES',\n",
    "        'object_type': 'SCHEMA',\n",
    "        'object_name': f\"{CATALOG_NAME}.{SCHEMA_NAME}\",\n",
    "        'description': 'Full access to RAG schema for experimentation'\n",
    "    },\n",
    "    {\n",
    "        'principal': 'data_engineers',\n",
    "        'privilege': 'MODIFY',\n",
    "        'object_type': 'TABLE',\n",
    "        'object_name': table_path,\n",
    "        'description': 'Write access to update knowledge base'\n",
    "    },\n",
    "    {\n",
    "        'principal': 'all_users',\n",
    "        'privilege': 'USE CATALOG',\n",
    "        'object_type': 'CATALOG',\n",
    "        'object_name': CATALOG_NAME,\n",
    "        'description': 'Catalog usage rights'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Try production groups first\n",
    "print(\"\\n1. Attempting Production Group Grants:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for perm in production_permissions:\n",
    "    group_name = perm['principal']\n",
    "\n",
    "    # Skip if we know the group doesn't exist\n",
    "    if len(available_groups_list) > 0 and group_name not in available_groups_list:\n",
    "        print(f\"\\n⊘ Skipping {group_name}: Group was not created/found in earlier section\")\n",
    "        groups_not_found.append(group_name)\n",
    "        failed_grants.append(perm)\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nGranting {perm['privilege']} on {perm['object_type']} to {group_name}:\")\n",
    "    print(f\"  Object: {perm['object_name']}\")\n",
    "    print(f\"  Purpose: {perm['description']}\")\n",
    "\n",
    "    try:\n",
    "        grant_sql = f\"GRANT {perm['privilege']} ON {perm['object_type']} {perm['object_name']} TO `{group_name}`\"\n",
    "        spark.sql(grant_sql)\n",
    "        print(f\"  ✓ Status: Success - Group exists and grant applied!\")\n",
    "        successful_grants.append(perm)\n",
    "        groups_granted.append(group_name)\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"already has\" in error_msg.lower() or \"already granted\" in error_msg.lower():\n",
    "            print(f\"  ✓ Status: Already granted - Group exists!\")\n",
    "            successful_grants.append(perm)\n",
    "            groups_granted.append(group_name)\n",
    "        elif \"principal_does_not_exist\" in error_msg.lower() or \"does not exist\" in error_msg.lower() or \"cannot find\" in error_msg.lower():\n",
    "            print(f\"  ⊘ Status: Group '{group_name}' does not exist\")\n",
    "            print(f\"  Note: Group creation failed or requires account admin privileges\")\n",
    "            groups_not_found.append(group_name)\n",
    "            failed_grants.append(perm)\n",
    "        elif \"insufficient\" in error_msg.lower() or \"permission\" in error_msg.lower():\n",
    "            print(f\"  ⚠ Status: Insufficient privileges (requires admin)\")\n",
    "            print(f\"  Note: Group may exist but you need admin rights to grant\")\n",
    "            failed_grants.append(perm)\n",
    "        else:\n",
    "            print(f\"  ⚠ Status: {error_msg[:150]}...\")\n",
    "            failed_grants.append(perm)\n",
    "\n",
    "# Also grant to current user for demonstration\n",
    "print(\"\\n2. Granting to Current User (for demonstration):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "user_permissions = [\n",
    "    {\n",
    "        'principal': current_user,\n",
    "        'privilege': 'SELECT',\n",
    "        'object_type': 'TABLE',\n",
    "        'object_name': table_path,\n",
    "        'description': 'Read access to RAG knowledge base'\n",
    "    },\n",
    "    {\n",
    "        'principal': current_user,\n",
    "        'privilege': 'USE SCHEMA',\n",
    "        'object_type': 'SCHEMA',\n",
    "        'object_name': f\"{CATALOG_NAME}.{SCHEMA_NAME}\",\n",
    "        'description': 'Schema usage rights for RAG'\n",
    "    }\n",
    "]\n",
    "\n",
    "for perm in user_permissions:\n",
    "    print(f\"\\nGranting {perm['privilege']} on {perm['object_type']}:\")\n",
    "    print(f\"  Object: {perm['object_name']}\")\n",
    "\n",
    "    try:\n",
    "        grant_sql = f\"GRANT {perm['privilege']} ON {perm['object_type']} {perm['object_name']} TO `{current_user}`\"\n",
    "        spark.sql(grant_sql)\n",
    "        print(f\"  ✓ Status: Success\")\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"already has\" in error_msg.lower() or \"already granted\" in error_msg.lower():\n",
    "            print(f\"  ✓ Status: Already granted\")\n",
    "        else:\n",
    "            print(f\"  ⚠ Status: {error_msg[:80]}...\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERMISSION GRANT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(groups_granted) > 0:\n",
    "    print(f\"\\n✓ Production groups successfully granted: {len(set(groups_granted))}\")\n",
    "    for group in set(groups_granted):\n",
    "        print(f\"  ✓ {group}\")\n",
    "    print(\"\\n  \uD83C\uDF89 Excellent! Your workspace has production groups configured!\")\n",
    "    print(\"  The verification section will show these grants.\")\n",
    "\n",
    "if len(groups_not_found) > 0:\n",
    "    print(f\"\\n⊘ Groups not found: {len(set(groups_not_found))}\")\n",
    "    for group in set(groups_not_found):\n",
    "        print(f\"  ⊘ {group}\")\n",
    "    print(\"\\n  \uD83D\uDCDD Why groups don't exist:\")\n",
    "    print(\"  • Group creation requires account admin privileges\")\n",
    "    print(\"  • You may not have permission to create groups\")\n",
    "    print(\"  • Groups may need to be created at account level\")\n",
    "    print(\"\\n  \uD83D\uDCA1 Solution:\")\n",
    "    print(\"  • Contact your Databricks account admin\")\n",
    "    print(\"  • Request creation of: data_analysts, ml_engineers, data_scientists, all_users\")\n",
    "    print(\"  • Or use this lab in demonstration mode (grants to current user)\")\n",
    "\n",
    "if successful_grants:\n",
    "    print(f\"\\n✓ Total successful grants: {len(successful_grants)}\")\n",
    "    for perm in successful_grants:\n",
    "        principal = perm.get('principal', 'current_user')\n",
    "        print(f\"  - {principal}: {perm['privilege']} on {perm['object_type']}\")\n",
    "\n",
    "if len(groups_granted) == 0:\n",
    "    print(\"\\n\uD83D\uDCCB Demonstration Mode:\")\n",
    "    print(\"  Since production groups don't exist, this lab will:\")\n",
    "    print(\"  • Grant permissions to your current user\")\n",
    "    print(\"  • Show example commands for production\")\n",
    "    print(\"  • Explain what production would look like\")\n",
    "    print(\"  • Teach RBAC concepts effectively\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY CONCEPTS - Unity Catalog Permissions\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. **Hierarchical Permissions**\n",
    "   - CATALOG → SCHEMA → TABLE/MODEL\n",
    "   - Permissions inherit down the hierarchy\n",
    "\n",
    "2. **Common Permission Types**\n",
    "   - USE CATALOG: Access to catalog\n",
    "   - USE SCHEMA: Access to schema\n",
    "   - SELECT: Read data from tables\n",
    "   - MODIFY: Write/update data\n",
    "   - EXECUTE: Run models/functions\n",
    "   - ALL PRIVILEGES: Full access\n",
    "\n",
    "3. **Role-Based Access Control (RBAC)**\n",
    "   - Create groups for different roles (e.g., data_analysts, ml_engineers)\n",
    "   - Grant permissions to groups, not individuals\n",
    "   - Users inherit permissions from their groups\n",
    "\n",
    "4. **Production Setup (Admin Tasks)**\n",
    "   - Create groups: CREATE GROUP data_analysts;\n",
    "   - Add users to groups: ALTER GROUP data_analysts ADD USER user@company.com;\n",
    "   - Grant permissions: GRANT SELECT ON TABLE ... TO data_analysts;\n",
    "\n",
    "5. **Best Practices**\n",
    "   - Use groups for permission management\n",
    "   - Follow principle of least privilege\n",
    "   - Document permission decisions\n",
    "   - Review permissions regularly\n",
    "   - All changes are automatically audited\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✓ Permission concepts demonstrated\")\n",
    "print(\"\\nIn production environments:\")\n",
    "print(\"  • Workspace admins create and manage groups\")\n",
    "print(\"  • Permissions are granted based on job roles\")\n",
    "print(\"  • All changes are tracked in audit logs\")\n",
    "print(\"  • Regular access reviews ensure compliance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6da81173-e615-4ef7-899d-576ab411cef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verify Granted Permissions\n",
    "\n",
    "Let's verify the permissions were granted successfully by viewing grants on each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ded310e9-ae33-4a6c-ac37-68f2efb0f4e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify permissions\n",
    "print(\"=== Verifying Granted Permissions ===\\n\")\n",
    "print(\"Checking what permissions exist vs. what was demonstrated...\\n\")\n",
    "\n",
    "# Define what we expect in production\n",
    "expected_grants = {\n",
    "    'table': [\n",
    "        {'principal': 'data_analysts', 'privilege': 'SELECT', 'description': 'Read access to RAG knowledge base'}\n",
    "    ],\n",
    "    'schema': [\n",
    "        {'principal': 'ml_engineers', 'privilege': 'USE SCHEMA', 'description': 'Schema usage rights for RAG'},\n",
    "        {'principal': 'data_scientists', 'privilege': 'ALL PRIVILEGES', 'description': 'Full access to RAG schema'}\n",
    "    ],\n",
    "    'catalog': [\n",
    "        {'principal': 'all_users', 'privilege': 'USE CATALOG', 'description': 'Catalog usage rights'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Check table permissions\n",
    "print(\"1. Table Permissions (RAG Knowledge Base):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Expected in production: GRANT SELECT ON TABLE {kb_table_path} TO `data_analysts`\\n\")\n",
    "\n",
    "try:\n",
    "    table_grants = spark.sql(f\"SHOW GRANTS ON TABLE {table_path}\")\n",
    "    grants_list = table_grants.collect()\n",
    "\n",
    "    if len(grants_list) > 0:\n",
    "        print(f\"✓ Found {len(grants_list)} grant(s) on table:\")\n",
    "        display(table_grants)\n",
    "\n",
    "        # Check for expected permissions\n",
    "        grants_text = ' '.join([str(row) for row in grants_list]).lower()\n",
    "\n",
    "        print(\"\\nGrant Analysis:\")\n",
    "\n",
    "        # Check for production groups\n",
    "        data_analysts_found = False\n",
    "        for row in grants_list:\n",
    "            row_str = str(row).lower()\n",
    "            if 'data_analysts' in row_str and 'select' in row_str:\n",
    "                print(\"  ✓ data_analysts has SELECT permission (PRODUCTION GRANT)\")\n",
    "                print(f\"     - {row}\")\n",
    "                data_analysts_found = True\n",
    "                break\n",
    "\n",
    "        if not data_analysts_found:\n",
    "            print(\"  ⊘ data_analysts: Not found (would exist in production)\")\n",
    "\n",
    "        # Check for data_engineers\n",
    "        data_engineers_found = False\n",
    "        for row in grants_list:\n",
    "            row_str = str(row).lower()\n",
    "            if 'data_engineers' in row_str and 'modify' in row_str:\n",
    "                print(\"  ✓ data_engineers has MODIFY permission (PRODUCTION GRANT)\")\n",
    "                print(f\"     - {row}\")\n",
    "                data_engineers_found = True\n",
    "                break\n",
    "\n",
    "        if not data_engineers_found:\n",
    "            print(\"  ⊘ data_engineers: Not found (would exist in production)\")\n",
    "\n",
    "        # Check for current user\n",
    "        current_user_found = False\n",
    "        for row in grants_list:\n",
    "            row_str = str(row).lower()\n",
    "            if current_user.lower() in row_str:\n",
    "                if not current_user_found:\n",
    "                    print(f\"  ✓ {current_user} has permissions on table (DEMONSTRATION GRANT)\")\n",
    "                    current_user_found = True\n",
    "                print(f\"     - {row}\")\n",
    "\n",
    "        production_groups_found = data_analysts_found or data_engineers_found\n",
    "        if production_groups_found:\n",
    "            print(\"\\n  \uD83D\uDCDD Note: Production groups found with correct permissions!\")\n",
    "        else:\n",
    "            print(\"\\n  \uD83D\uDCDD Note: In production, you would see 'data_analysts' and 'data_engineers' groups here\")\n",
    "    else:\n",
    "        print(\"⊘ No explicit grants on table\")\n",
    "        print(\"\\n\uD83D\uDCCB What You Would See in Production:\")\n",
    "        print(\"  ✓ data_analysts: SELECT permission\")\n",
    "        print(\"  ✓ Other relevant groups with appropriate permissions\")\n",
    "        print(\"\\nℹ Current Status:\")\n",
    "        print(\"  • Permissions are inherited from schema or catalog level\")\n",
    "        print(\"  • This is normal in learning environments\")\n",
    "        print(\"  • You can still access the table through inherited permissions\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Unable to show table grants: {str(e)}\")\n",
    "    print(\"Note: This may be normal if grants are inherited from parent objects\")\n",
    "\n",
    "print(f\"\\n2. Schema Permissions ({SCHEMA_NAME}):\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Expected in production:\")\n",
    "print(\"  • GRANT USE SCHEMA ON SCHEMA ... TO `ml_engineers`\")\n",
    "print(\"  • GRANT ALL PRIVILEGES ON SCHEMA ... TO `data_scientists`\\n\")\n",
    "\n",
    "try:\n",
    "    schema_grants = spark.sql(f\"SHOW GRANTS ON SCHEMA {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "    grants_list = schema_grants.collect()\n",
    "\n",
    "    if len(grants_list) > 0:\n",
    "        print(f\"✓ Found {len(grants_list)} grant(s) on schema:\")\n",
    "        display(schema_grants)\n",
    "\n",
    "        # Check for expected permissions\n",
    "        grants_text = ' '.join([str(row) for row in grants_list]).lower()\n",
    "\n",
    "        print(\"\\nGrant Analysis:\")\n",
    "\n",
    "        # Check for production groups\n",
    "        ml_engineers_found = False\n",
    "        data_scientists_found = False\n",
    "\n",
    "        for row in grants_list:\n",
    "            row_str = str(row).lower()\n",
    "\n",
    "            if 'ml_engineers' in row_str and 'use schema' in row_str:\n",
    "                print(\"  ✓ ml_engineers has USE SCHEMA permission (PRODUCTION GRANT)\")\n",
    "                print(f\"     - {row}\")\n",
    "                ml_engineers_found = True\n",
    "\n",
    "            if 'data_scientists' in row_str and 'all' in row_str:\n",
    "                print(\"  ✓ data_scientists has ALL PRIVILEGES (PRODUCTION GRANT)\")\n",
    "                print(f\"     - {row}\")\n",
    "                data_scientists_found = True\n",
    "\n",
    "        if not ml_engineers_found:\n",
    "            print(\"  ⊘ ml_engineers: Not found (would exist in production)\")\n",
    "        if not data_scientists_found:\n",
    "            print(\"  ⊘ data_scientists: Not found (would exist in production)\")\n",
    "\n",
    "        # Check for current user\n",
    "        current_user_found = False\n",
    "        for row in grants_list:\n",
    "            row_str = str(row).lower()\n",
    "            if current_user.lower() in row_str:\n",
    "                if not current_user_found:\n",
    "                    print(f\"  ✓ {current_user} has permissions on schema (DEMONSTRATION GRANT)\")\n",
    "                    current_user_found = True\n",
    "                print(f\"     - {row}\")\n",
    "\n",
    "        if ml_engineers_found or data_scientists_found:\n",
    "            print(\"\\n  \uD83D\uDCDD Note: Production groups found with correct permissions!\")\n",
    "        else:\n",
    "            print(\"\\n  \uD83D\uDCDD Note: In production, you would see 'ml_engineers' and 'data_scientists' groups here\")\n",
    "    else:\n",
    "        print(\"⊘ No explicit grants on schema\")\n",
    "        print(\"\\n\uD83D\uDCCB What You Would See in Production:\")\n",
    "        print(\"  ✓ ml_engineers: USE SCHEMA permission\")\n",
    "        print(\"  ✓ data_scientists: ALL PRIVILEGES\")\n",
    "        print(\"  ✓ Other relevant groups with appropriate permissions\")\n",
    "        print(\"\\nℹ Current Status:\")\n",
    "        print(\"  • Permissions are inherited from catalog or account level\")\n",
    "        print(\"  • This is normal in shared Databricks workspaces\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Unable to show schema grants: {str(e)}\")\n",
    "    print(\"Note: This may require additional permissions\")\n",
    "\n",
    "print(\"\\n3. Catalog Permissions (financial_services):\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Expected in production: GRANT USE CATALOG ON CATALOG ... TO `all_users`\\n\")\n",
    "\n",
    "try:\n",
    "    catalog_grants = spark.sql(f\"SHOW GRANTS ON CATALOG {CATALOG_NAME}\")\n",
    "    grants_list = catalog_grants.collect()\n",
    "\n",
    "    if len(grants_list) > 0:\n",
    "        print(f\"✓ Found {len(grants_list)} grant(s) on catalog:\")\n",
    "        display(catalog_grants)\n",
    "\n",
    "        # Check for expected permissions\n",
    "        grants_text = ' '.join([str(row) for row in grants_list]).lower()\n",
    "\n",
    "        print(\"\\nGrant Analysis:\")\n",
    "\n",
    "        # Check for production groups\n",
    "        all_users_found = False\n",
    "\n",
    "        for row in grants_list:\n",
    "            row_str = str(row).lower()\n",
    "            if 'all_users' in row_str and 'use catalog' in row_str:\n",
    "                print(\"  ✓ all_users has USE CATALOG permission (PRODUCTION GRANT)\")\n",
    "                print(f\"     - {row}\")\n",
    "                all_users_found = True\n",
    "\n",
    "        if not all_users_found:\n",
    "            print(\"  ⊘ all_users: Not found (would exist in production)\")\n",
    "\n",
    "        # Show all other grants\n",
    "        print(\"\\n  All grants on catalog:\")\n",
    "        for row in grants_list:\n",
    "            row_str = str(row).lower()\n",
    "            if 'all_users' not in row_str:  # Don't duplicate all_users\n",
    "                print(f\"  • {row}\")\n",
    "\n",
    "        if all_users_found:\n",
    "            print(\"\\n  \uD83D\uDCDD Note: Production group 'all_users' found with correct permissions!\")\n",
    "        else:\n",
    "            print(\"\\n  \uD83D\uDCDD Note: In production, you would see 'all_users' group here\")\n",
    "    else:\n",
    "        print(\"⊘ No explicit grants on catalog\")\n",
    "        print(\"\\n\uD83D\uDCCB What You Would See in Production:\")\n",
    "        print(\"  ✓ all_users: USE CATALOG permission\")\n",
    "        print(\"  ✓ Admin groups with full privileges\")\n",
    "        print(\"  ✓ Other relevant groups with appropriate permissions\")\n",
    "        print(\"\\nℹ Current Status:\")\n",
    "        print(\"  • Permissions are managed at account level\")\n",
    "        print(\"  • Users have default workspace access\")\n",
    "        print(\"  • Catalog is accessible to all workspace users\")\n",
    "        print(\"\\n✓ You can still use the catalog - access is inherited from workspace/account level\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Unable to show catalog grants: {str(e)}\")\n",
    "    print(\"Note: This may require additional permissions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RBAC VERIFICATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Summary of what was verified\n",
    "print(\"\\n✓ Permissions Verified:\")\n",
    "print(f\"  - Table grants checked: {table_path}\")\n",
    "print(f\"  - Schema grants checked: {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "print(f\"  - Catalog grants checked: {CATALOG_NAME}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: Demonstration vs. Production\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check what was actually granted by reviewing the grants\n",
    "try:\n",
    "    table_check = spark.sql(f\"SHOW GRANTS ON TABLE {table_path}\").collect()\n",
    "    schema_check = spark.sql(f\"SHOW GRANTS ON SCHEMA {CATALOG_NAME}.{SCHEMA_NAME}\").collect()\n",
    "    catalog_check = spark.sql(f\"SHOW GRANTS ON CATALOG {CATALOG_NAME}\").collect()\n",
    "\n",
    "    # Determine which groups were found\n",
    "    all_grants_text = ' '.join([str(row) for row in table_check + schema_check + catalog_check]).lower()\n",
    "\n",
    "    data_analysts_exists = 'data_analysts' in all_grants_text\n",
    "    ml_engineers_exists = 'ml_engineers' in all_grants_text\n",
    "    data_scientists_exists = 'data_scientists' in all_grants_text\n",
    "    all_users_exists = 'all_users' in all_grants_text\n",
    "\n",
    "except:\n",
    "    data_analysts_exists = False\n",
    "    ml_engineers_exists = False\n",
    "    data_scientists_exists = False\n",
    "    all_users_exists = False\n",
    "\n",
    "print(\"\\n\uD83D\uDCCB What Was Demonstrated (Example Commands):\")\n",
    "print(\"-\" * 80)\n",
    "print(\"1. GRANT SELECT ON TABLE ... TO `data_analysts`\")\n",
    "print(\"   Purpose: Read access to RAG knowledge base\")\n",
    "print(f\"   Status: {'✓ Successfully granted!' if data_analysts_exists else '⊘ Group does not exist in this environment'}\")\n",
    "print(\"\")\n",
    "print(\"2. GRANT USE SCHEMA ON SCHEMA ... TO `ml_engineers`\")\n",
    "print(\"   Purpose: Schema usage rights for RAG deployment\")\n",
    "print(f\"   Status: {'✓ Successfully granted!' if ml_engineers_exists else '⊘ Group does not exist in this environment'}\")\n",
    "print(\"\")\n",
    "print(\"3. GRANT ALL PRIVILEGES ON SCHEMA ... TO `data_scientists`\")\n",
    "print(\"   Purpose: Full access to RAG schema for experimentation\")\n",
    "print(f\"   Status: {'✓ Successfully granted!' if data_scientists_exists else '⊘ Group does not exist in this environment'}\")\n",
    "print(\"\")\n",
    "print(\"4. GRANT USE CATALOG ON CATALOG ... TO `all_users`\")\n",
    "print(\"   Purpose: Catalog usage rights\")\n",
    "print(f\"   Status: {'✓ Successfully granted!' if all_users_exists else '⊘ Group does not exist in this environment'}\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCCB What Actually Exists (Verification Results):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"✓ {current_user}: SELECT on TABLE (demonstration grant)\")\n",
    "print(f\"✓ {current_user}: USE SCHEMA on SCHEMA (demonstration grant)\")\n",
    "\n",
    "if data_analysts_exists:\n",
    "    print(\"✓ data_analysts: SELECT on TABLE (production grant)\")\n",
    "else:\n",
    "    print(\"⊘ data_analysts: Not found (would exist in production)\")\n",
    "\n",
    "if ml_engineers_exists:\n",
    "    print(\"✓ ml_engineers: USE SCHEMA on SCHEMA (production grant)\")\n",
    "else:\n",
    "    print(\"⊘ ml_engineers: Not found (would exist in production)\")\n",
    "\n",
    "if data_scientists_exists:\n",
    "    print(\"✓ data_scientists: ALL PRIVILEGES on SCHEMA (production grant)\")\n",
    "else:\n",
    "    print(\"⊘ data_scientists: Not found (would exist in production)\")\n",
    "\n",
    "if all_users_exists:\n",
    "    print(\"✓ all_users: USE CATALOG on CATALOG (production grant)\")\n",
    "else:\n",
    "    print(\"⊘ all_users: Not found (would exist in production)\")\n",
    "\n",
    "# Summary message\n",
    "if data_analysts_exists or ml_engineers_exists or data_scientists_exists or all_users_exists:\n",
    "    print(\"\\n\uD83C\uDF89 Excellent! Your workspace has production groups configured and grants were successful!\")\n",
    "else:\n",
    "    print(\"\\nℹ Note: This is a learning environment without pre-configured production groups.\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCCB What You Would See in Production:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"\"\"\n",
    "Table Level (RAG Knowledge Base - {KNOWLEDGE_BASE_TABLE}):\n",
    "  ✓ data_analysts: SELECT (read knowledge base)\n",
    "  ✓ data_scientists: ALL PRIVILEGES (inherited from schema)\n",
    "  ✓ data_engineers: MODIFY (update knowledge base)\n",
    "\n",
    "Schema Level ({SCHEMA_NAME}):\n",
    "  ✓ ml_engineers: USE SCHEMA (deploy RAG models)\n",
    "  ✓ data_scientists: ALL PRIVILEGES (experiment with RAG)\n",
    "  ✓ data_analysts: USE SCHEMA (if granted)\n",
    "\n",
    "Catalog Level ({CATALOG_NAME}):\n",
    "  ✓ all_users: USE CATALOG\n",
    "  ✓ admins: ALL PRIVILEGES\n",
    "  ✓ Other groups as needed\n",
    "\n",
    "Each grant would show:\n",
    "  • Principal (group name)\n",
    "  • ActionType (SELECT, USE SCHEMA, etc.)\n",
    "  • ObjectType (TABLE, SCHEMA, CATALOG)\n",
    "  • ObjectKey (full path to object)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA Understanding the Results:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\"\"\n",
    "If you see \"No explicit grants\" or \"0 grants\", this is NORMAL and EXPECTED in:\n",
    "  • Shared Databricks workspaces\n",
    "  • Learning/training environments\n",
    "  • Workspaces with default access policies\n",
    "\n",
    "How Access Works Without Explicit Grants:\n",
    "  1. Workspace-level permissions grant default access\n",
    "  2. Account-level permissions provide inherited access\n",
    "  3. You're the creator/owner of the objects (automatic access)\n",
    "  4. Unity Catalog uses hierarchical permission inheritance\n",
    "\n",
    "What This Means:\n",
    "  ✓ You CAN access and use the data/models\n",
    "  ✓ Permissions are inherited from parent levels\n",
    "  ✓ This is a secure and common configuration\n",
    "  ✓ In production, explicit grants would be added for other users/groups\n",
    "\n",
    "Production Difference:\n",
    "  • Admins would create explicit grants for each group\n",
    "  • You would see rows in the SHOW GRANTS output\n",
    "  • Each user/group would have specific permissions listed\n",
    "  • Audit logs would track all grant operations\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY TAKEAWAYS - Unity Catalog RBAC\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. ✓ Unity Catalog provides fine-grained access control\n",
    "   - Permissions at catalog, schema, table, and column levels\n",
    "   - Hierarchical inheritance of permissions\n",
    "\n",
    "2. ✓ Groups enable scalable permission management\n",
    "   - Create groups for different roles\n",
    "   - Grant permissions to groups, not individuals\n",
    "   - Users inherit from all their groups\n",
    "\n",
    "3. ✓ Production RBAC Workflow:\n",
    "   - Account admins create groups\n",
    "   - Users are assigned to groups based on roles\n",
    "   - Permissions follow principle of least privilege\n",
    "   - Regular audits ensure compliance\n",
    "\n",
    "4. ✓ All permission changes are automatically logged\n",
    "   - Complete audit trail for compliance\n",
    "   - Track who granted what to whom\n",
    "   - Query audit logs for security reviews\n",
    "\n",
    "5. ✓ RBAC is essential for enterprise governance\n",
    "   - Meets regulatory requirements\n",
    "   - Enables secure collaboration\n",
    "   - Supports data governance policies\n",
    "\n",
    "Example Production Commands:\n",
    "  CREATE GROUP data_analysts;\n",
    "  ALTER GROUP data_analysts ADD USER user@company.com;\n",
    "  GRANT SELECT ON TABLE ... TO data_analysts;\n",
    "  SHOW GRANTS ON TABLE ...;\n",
    "\"\"\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\uD83D\uDCA1 Next Steps for Production RBAC:\")\n",
    "print(\"  1. Work with admin to create proper groups\")\n",
    "print(\"  2. Map organizational roles to Unity Catalog groups\")\n",
    "print(\"  3. Document permission policies\")\n",
    "print(\"  4. Set up regular permission audits\")\n",
    "print(\"  5. Train users on data access procedures\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cf62947-8fd0-4233-81da-7cd5cc610a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Audit Logging\n",
    "\n",
    "Unity Catalog automatically logs all operations. Let's query the audit logs to see model operations.\n",
    "\n",
    "**Compliance Value:** Audit logs provide a complete trail for regulatory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ad092c9-2ad3-433d-9f28-49c54d0fdbe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query audit logs for model operations\n",
    "print(\"=== Audit Logging Demonstration ===\\n\")\n",
    "\n",
    "# Check if system catalog is accessible\n",
    "print(\"Checking audit log access...\")\n",
    "audit_available = False\n",
    "\n",
    "try:\n",
    "    # Try to access system catalog\n",
    "    spark.sql(\"USE CATALOG system\")\n",
    "    spark.sql(\"SHOW TABLES IN system.access\").collect()\n",
    "    audit_available = True\n",
    "    print(\"✓ System catalog is accessible\")\n",
    "except Exception as e:\n",
    "    print(\"⚠ System catalog not accessible in this workspace\")\n",
    "    print(f\"  Reason: {str(e)[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "if audit_available:\n",
    "    print(\"\\nQuerying audit logs for recent operations...\")\n",
    "    print(\"(This may take a moment...)\\n\")\n",
    "\n",
    "    # Try multiple queries to find audit data\n",
    "    queries_to_try = [\n",
    "        {\n",
    "            'name': 'Unity Catalog operations in this session',\n",
    "            'query': f\"\"\"\n",
    "                SELECT\n",
    "                    event_time,\n",
    "                    user_identity.email as user,\n",
    "                    action_name,\n",
    "                    request_params.full_name_arg as object_name,\n",
    "                    response.status_code\n",
    "                FROM system.access.audit\n",
    "                WHERE event_date >= current_date() - INTERVAL 1 DAY\n",
    "                    AND user_identity.email = '{current_user}'\n",
    "                    AND (\n",
    "                        action_name IN ('createTable', 'createSchema', 'createCatalog',\n",
    "                                       'getTable', 'getSchema', 'getCatalog',\n",
    "                                       'createRegisteredModelVersion', 'updateRegisteredModel')\n",
    "                        OR request_params.full_name_arg LIKE '%{CATALOG_NAME}%'\n",
    "                        OR request_params.full_name_arg LIKE '%{SCHEMA_NAME}%'\n",
    "                    )\n",
    "                ORDER BY event_time DESC\n",
    "                LIMIT 20\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'name': 'Recent table operations',\n",
    "            'query': f\"\"\"\n",
    "                SELECT\n",
    "                    event_time,\n",
    "                    user_identity.email as user,\n",
    "                    action_name,\n",
    "                    request_params.full_name_arg as object_name\n",
    "                FROM system.access.audit\n",
    "                WHERE event_date >= current_date() - INTERVAL 1 DAY\n",
    "                    AND action_name IN ('createTable', 'getTable', 'readTable')\n",
    "                ORDER BY event_time DESC\n",
    "                LIMIT 10\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'name': 'Any recent operations by current user',\n",
    "            'query': f\"\"\"\n",
    "                SELECT\n",
    "                    event_time,\n",
    "                    user_identity.email as user,\n",
    "                    action_name,\n",
    "                    request_params.full_name_arg as object_name\n",
    "                FROM system.access.audit\n",
    "                WHERE event_date >= current_date()\n",
    "                    AND user_identity.email = '{current_user}'\n",
    "                ORDER BY event_time DESC\n",
    "                LIMIT 10\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    audit_found = False\n",
    "\n",
    "    for query_info in queries_to_try:\n",
    "        if audit_found:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            print(f\"Trying: {query_info['name']}...\")\n",
    "            audit_logs = spark.sql(query_info['query'])\n",
    "            audit_count = audit_logs.count()\n",
    "\n",
    "            if audit_count > 0:\n",
    "                print(f\"✓ Found {audit_count} audit log entries!\\n\")\n",
    "                print(f\"Showing: {query_info['name']}\")\n",
    "                display(audit_logs)\n",
    "                audit_found = True\n",
    "\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"AUDIT LOG ANALYSIS\")\n",
    "                print(\"=\"*80)\n",
    "                print(f\"\"\"\n",
    "✓ Successfully retrieved audit logs from Unity Catalog\n",
    "\n",
    "What These Logs Show:\n",
    "  • event_time: When the operation occurred\n",
    "  • user: Who performed the operation ({current_user})\n",
    "  • action_name: What operation was performed (createTable, getTable, etc.)\n",
    "  • object_name: Which object was accessed\n",
    "  • status_code: Success (200) or error codes\n",
    "\n",
    "Compliance Value:\n",
    "  ✓ Complete audit trail of all operations\n",
    "  ✓ Track who accessed what data and when\n",
    "  ✓ Investigate security incidents\n",
    "  ✓ Meet regulatory requirements (SOX, GDPR, HIPAA)\n",
    "  ✓ Generate compliance reports\n",
    "\"\"\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"  No results for this query\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Query failed: {str(e)[:80]}...\")\n",
    "            continue\n",
    "\n",
    "    if not audit_found:\n",
    "        print(\"\\n⚠ No audit logs found with any query\")\n",
    "        print(\"\\nPossible reasons:\")\n",
    "        print(\"  • Audit logs may have a delay before appearing (up to 1 hour)\")\n",
    "        print(\"  • Logs may be retained for limited time\")\n",
    "        print(\"  • Some operations may not be logged in this workspace type\")\n",
    "        print(\"  • Filters may not match recent operations\")\n",
    "        audit_available = False\n",
    "\n",
    "if not audit_available:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AUDIT LOG DEMONSTRATION (Simulated)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nSince audit logs aren't available, here's what they would show for this lab:\\n\")\n",
    "\n",
    "    # Create simulated audit log data\n",
    "    from datetime import datetime, timedelta\n",
    "    import pandas as pd\n",
    "\n",
    "    current_time = datetime.now()\n",
    "\n",
    "    simulated_logs = [\n",
    "        {\n",
    "            'event_time': (current_time - timedelta(minutes=10)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'user': current_user,\n",
    "            'action_name': 'createCatalog',\n",
    "            'object_name': CATALOG_NAME,\n",
    "            'status_code': 200\n",
    "        },\n",
    "        {\n",
    "            'event_time': (current_time - timedelta(minutes=9)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'user': current_user,\n",
    "            'action_name': 'createSchema',\n",
    "            'object_name': f'{CATALOG_NAME}.{SCHEMA_NAME}',\n",
    "            'status_code': 200\n",
    "        },\n",
    "        {\n",
    "            'event_time': (current_time - timedelta(minutes=8)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'user': current_user,\n",
    "            'action_name': 'createTable',\n",
    "            'object_name': table_path,\n",
    "            'status_code': 200\n",
    "        },\n",
    "        {\n",
    "            'event_time': (current_time - timedelta(minutes=5)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'user': current_user,\n",
    "            'action_name': 'createRegisteredModelVersion',\n",
    "            'object_name': MODEL_NAME,\n",
    "            'status_code': 200\n",
    "        },\n",
    "        {\n",
    "            'event_time': (current_time - timedelta(minutes=3)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'user': current_user,\n",
    "            'action_name': 'setRegisteredModelAlias',\n",
    "            'object_name': f'{MODEL_NAME} (Champion)',\n",
    "            'status_code': 200\n",
    "        },\n",
    "        {\n",
    "            'event_time': (current_time - timedelta(minutes=2)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'user': current_user,\n",
    "            'action_name': 'grantPrivileges',\n",
    "            'object_name': f'USE SCHEMA on {CATALOG_NAME}.{SCHEMA_NAME}',\n",
    "            'status_code': 200\n",
    "        },\n",
    "        {\n",
    "            'event_time': (current_time - timedelta(minutes=1)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'user': current_user,\n",
    "            'action_name': 'getTable',\n",
    "            'object_name': table_path,\n",
    "            'status_code': 200\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    simulated_df = pd.DataFrame(simulated_logs)\n",
    "    print(\"Simulated Audit Log Entries (What Would Appear in Production):\")\n",
    "    print(\"-\"*80)\n",
    "    display(simulated_df)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AUDIT LOG ANALYSIS (Simulated)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\"\"\n",
    "What These Logs Show:\n",
    "  ✓ Catalog creation: {CATALOG_NAME}\n",
    "  ✓ Schema creation: {SCHEMA_NAME}\n",
    "  ✓ Table creation: {TABLE_NAME}\n",
    "  ✓ Model registration: {MODEL_NAME}\n",
    "  ✓ Model alias assignment: Champion\n",
    "  ✓ Permission grant: USE SCHEMA\n",
    "  ✓ Data access: getTable operation\n",
    "\n",
    "All operations performed by: {current_user}\n",
    "All operations successful: status_code = 200\n",
    "\n",
    "Compliance Value:\n",
    "  ✓ Complete audit trail of all operations\n",
    "  ✓ Track who accessed what data and when\n",
    "  ✓ Investigate security incidents\n",
    "  ✓ Meet regulatory requirements (SOX, GDPR, HIPAA)\n",
    "  ✓ Generate compliance reports\n",
    "  ✓ Retention: 90+ days (configurable)\n",
    "\"\"\")\n",
    "\n",
    "    print(\"\\n\uD83D\uDCDA About Unity Catalog Audit Logs:\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"\"\"\n",
    "Audit logs in Unity Catalog track ALL operations including:\n",
    "\n",
    "1. **Data Access**\n",
    "   - Table reads and writes (getTable, readTable)\n",
    "   - Schema and catalog access\n",
    "   - Column-level access (if enabled)\n",
    "\n",
    "2. **Model Operations**\n",
    "   - Model registration (createRegisteredModelVersion)\n",
    "   - Version creation and updates\n",
    "   - Model alias changes (setRegisteredModelAlias)\n",
    "   - Model downloads and deployments\n",
    "\n",
    "3. **Permission Changes**\n",
    "   - GRANT and REVOKE operations (grantPrivileges, revokePrivileges)\n",
    "   - Group membership changes\n",
    "   - Role assignments\n",
    "\n",
    "4. **Administrative Actions**\n",
    "   - Catalog/schema creation (createCatalog, createSchema)\n",
    "   - Table modifications (createTable, alterTable)\n",
    "   - Policy updates\n",
    "\n",
    "Example Audit Log Query:\n",
    "\"\"\")\n",
    "\n",
    "    print(\"\"\"\n",
    "-- Query all operations on a specific model\n",
    "SELECT\n",
    "    event_time,\n",
    "    user_identity.email,\n",
    "    action_name,\n",
    "    request_params.name,\n",
    "    response.status_code\n",
    "FROM system.access.audit\n",
    "WHERE request_params.name = 'catalog.schema.model_name'\n",
    "ORDER BY event_time DESC;\n",
    "\n",
    "-- Query all permission grants\n",
    "SELECT\n",
    "    event_time,\n",
    "    user_identity.email,\n",
    "    action_name,\n",
    "    request_params.privilege,\n",
    "    request_params.principal\n",
    "FROM system.access.audit\n",
    "WHERE action_name LIKE '%GRANT%'\n",
    "ORDER BY event_time DESC;\n",
    "\n",
    "-- Query all data access\n",
    "SELECT\n",
    "    event_time,\n",
    "    user_identity.email,\n",
    "    action_name,\n",
    "    request_params.full_name_arg\n",
    "FROM system.access.audit\n",
    "WHERE action_name = 'getTable'\n",
    "ORDER BY event_time DESC;\n",
    "\"\"\")\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"In Production Environments:\")\n",
    "    print(\"  ✓ Audit logs are automatically enabled\")\n",
    "    print(\"  ✓ Logs are retained for 90+ days (configurable)\")\n",
    "    print(\"  ✓ Can be exported to external systems (S3, Azure, etc.)\")\n",
    "    print(\"  ✓ Used for compliance reporting and security monitoring\")\n",
    "    print(\"  ✓ Integrated with SIEM tools for real-time alerting\")\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"What Audit Logs Would Show for This Lab:\")\n",
    "    print(\"  • Model registration: \" + MODEL_NAME)\n",
    "    print(\"  • Version creation: Versions 1, 2, etc.\")\n",
    "    print(\"  • Alias assignments: Champion, Challenger\")\n",
    "    print(\"  • Table creation: \" + table_path)\n",
    "    print(\"  • All by user: \" + current_user)\n",
    "    print(\"  • Timestamps for each operation\")\n",
    "    print(\"  • Success/failure status codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4b226d9-4c9f-48db-91fd-f747d51b6a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Lineage Tracking\n",
    "\n",
    "Unity Catalog automatically tracks lineage from data to models. This shows:\n",
    "- Which tables were used to train the model\n",
    "- Which notebooks/jobs created the model\n",
    "- Downstream dependencies\n",
    "\n",
    "**Governance Benefit:** Complete transparency for auditors and stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9081067-9392-45a9-b796-752bcc03dd87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Demonstrate lineage information\n",
    "print(\"=== Model Lineage Information ===\\n\")\n",
    "\n",
    "# Get model details\n",
    "model_details = client.get_registered_model(MODEL_NAME)\n",
    "\n",
    "print(f\"Model: {model_details.name}\")\n",
    "print(f\"Description: {model_details.description[:100] if model_details.description else 'N/A'}...\")\n",
    "print(f\"\\nLineage:\")\n",
    "print(f\"  - Source Data: {table_path}\")\n",
    "print(f\"  - Training Notebook: {experiment_name}\")\n",
    "print(f\"  - Total Versions: {len(all_versions)}\")\n",
    "print(f\"  - Current Champion: Version {model_version.version}\")\n",
    "print(f\"  - Current Challenger: Version {model_version_v2.version}\")\n",
    "\n",
    "# Show data lineage through Unity Catalog\n",
    "print(f\"\\n✓ Unity Catalog tracks complete lineage:\")\n",
    "print(f\"  Data → Model → Deployment\")\n",
    "print(f\"  All accessible through the Unity Catalog UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e78be061-b6ee-4a7b-b95f-e312455019f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 7: Model Monitoring and Reproducibility\n",
    "\n",
    "For production models, we need:\n",
    "- **Reproducibility**: Ability to recreate any model version\n",
    "- **Monitoring**: Track model performance over time\n",
    "- **Documentation**: Clear records of all decisions\n",
    "\n",
    "Let's implement these best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9925c524-688d-47cf-af47-7acce86a250e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reproducibility: Recreate Model from Registry\n",
    "\n",
    "Demonstrate how to fully reproduce a model using MLflow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "523f8202-d038-4166-b9b9-62115fe92c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get run information for reproducibility\n",
    "run_info = client.get_run(best_run_id)\n",
    "\n",
    "print(\"=== Model Reproducibility Information ===\\n\")\n",
    "print(f\"Run ID: {run_info.info.run_id}\")\n",
    "print(f\"Experiment ID: {run_info.info.experiment_id}\")\n",
    "print(f\"Start Time: {datetime.fromtimestamp(run_info.info.start_time/1000)}\")\n",
    "print(f\"End Time: {datetime.fromtimestamp(run_info.info.end_time/1000)}\")\n",
    "\n",
    "print(\"\\nLogged Parameters:\")\n",
    "for key, value in run_info.data.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nLogged Metrics:\")\n",
    "for key, value in run_info.data.metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nLogged Tags:\")\n",
    "for key, value in run_info.data.tags.items():\n",
    "    if not key.startswith('mlflow.'):\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n✓ All information needed to reproduce this model is logged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9085cc21-b630-4db5-9ced-ef36e5c354d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Model Performance Report\n",
    "\n",
    "Generate a comprehensive report for stakeholders and compliance.\n",
    "\n",
    "#### Why Reports Matter\n",
    "**Stakeholder Communication:** Non-technical stakeholders need clear, concise reports:\n",
    "- Business leaders: Cost projections and ROI\n",
    "- Compliance teams: Governance and audit trail\n",
    "- Product managers: Performance metrics and capabilities\n",
    "- Finance: Budget impact and cost forecasting\n",
    "\n",
    "**Compliance Documentation:** Regulators require:\n",
    "- Model documentation and validation\n",
    "- Performance metrics and limitations\n",
    "- Data lineage and governance controls\n",
    "- Audit trail of all operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e05080d2-d3dd-4d0c-a689-82211bd03ac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create comprehensive RAG performance report\n",
    "report = f\"\"\"\n",
    "{'='*80}\n",
    "RAG CUSTOMER SUPPORT ASSISTANT - PERFORMANCE REPORT\n",
    "{'='*80}\n",
    "\n",
    "Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Model Name: {MODEL_NAME}\n",
    "Champion Version: {model_version.version}\n",
    "Challenger Version: {model_version_v2.version}\n",
    "Report Author: {current_user}\n",
    "\n",
    "{'='*80}\n",
    "EXECUTIVE SUMMARY\n",
    "{'='*80}\n",
    "\n",
    "This RAG (Retrieval-Augmented Generation) system answers customer support questions\n",
    "by retrieving relevant information from a knowledge base and generating natural\n",
    "language responses. The system has been evaluated on {len(df_eval_questions)} test questions\n",
    "across multiple configurations.\n",
    "\n",
    "Key Findings:\n",
    "  ✓ Champion configuration (top_k=3) provides optimal cost/quality balance\n",
    "  ✓ Average cost per query: ${comparison_df.loc[1, 'Cost_Per_Query_USD']:.6f}\n",
    "  ✓ Estimated monthly cost (1000 queries): ${comparison_df.loc[1, 'Cost_Per_Query_USD'] * 1000:.2f}\n",
    "  ✓ All governance controls in place and operational\n",
    "  ✓ Ready for production deployment with proper LLM integration\n",
    "\n",
    "{'='*80}\n",
    "CHAMPION MODEL CONFIGURATION\n",
    "{'='*80}\n",
    "\n",
    "Configuration: {best_config_name}\n",
    "Retrieval Method: Keyword-based (production should use vector embeddings)\n",
    "LLM: Mock LLM (production should use DBRX, GPT-4, or similar)\n",
    "Top-K Documents: {int(comparison_df.loc[best_config_idx, 'Avg_Docs_Retrieved'])}\n",
    "\n",
    "Knowledge Base:\n",
    "  - Source: {CATALOG_NAME}.{SCHEMA_NAME}.{KNOWLEDGE_BASE_TABLE}\n",
    "  - Version: {df_knowledge_base['version'].iloc[0]}\n",
    "  - Total Documents: {len(df_knowledge_base):,}\n",
    "  - Categories: {', '.join(df_knowledge_base['category'].unique())}\n",
    "\n",
    "Evaluation Dataset:\n",
    "  - Source: {CATALOG_NAME}.{SCHEMA_NAME}.{EVAL_QUESTIONS_TABLE}\n",
    "  - Total Questions: {len(df_eval_questions):,}\n",
    "  - Question Types: {', '.join(df_eval_questions['category'].unique())}\n",
    "\n",
    "Performance Metrics:\n",
    "  - Avg Tokens per Query: {comparison_df.loc[best_config_idx, 'Avg_Tokens_Per_Query']:.1f}\n",
    "  - Total Tokens (Eval Set): {comparison_df.loc[best_config_idx, 'Total_Tokens']:,}\n",
    "  - Cost per Query: ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD']:.6f}\n",
    "  - Total Cost (Eval Set): ${comparison_df.loc[best_config_idx, 'Total_Cost_USD']:.4f}\n",
    "  - Avg Docs Retrieved: {comparison_df.loc[best_config_idx, 'Avg_Docs_Retrieved']:.1f}\n",
    "\n",
    "{'='*80}\n",
    "CHALLENGER MODEL CONFIGURATION\n",
    "{'='*80}\n",
    "\n",
    "Configuration: Comprehensive (top_k=5)\n",
    "Top-K Documents: 5\n",
    "Purpose: Higher retrieval coverage for improved answer quality\n",
    "\n",
    "Performance Metrics:\n",
    "  - Avg Tokens per Query: {comparison_df.loc[2, 'Avg_Tokens_Per_Query']:.1f}\n",
    "  - Cost per Query: ${comparison_df.loc[2, 'Cost_Per_Query_USD']:.6f}\n",
    "  - Relative Cost: {comparison_df.loc[2, 'Relative_Cost_Pct']:.0f}% of baseline\n",
    "\n",
    "Trade-offs:\n",
    "  ✓ Pros: More comprehensive context, potentially better answers\n",
    "  ✗ Cons: {comparison_df.loc[2, 'Relative_Cost_Pct']:.0f}% higher cost, more tokens, slower responses\n",
    "\n",
    "{'='*80}\n",
    "COST ANALYSIS & PROJECTIONS\n",
    "{'='*80}\n",
    "\n",
    "Champion Configuration Cost Projections:\n",
    "  • 100 queries/month:    ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD'] * 100:.2f}\n",
    "  • 1,000 queries/month:  ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD'] * 1000:.2f}\n",
    "  • 10,000 queries/month: ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD'] * 10000:.2f}\n",
    "  • 100,000 queries/month: ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD'] * 100000:.2f}\n",
    "\n",
    "Challenger Configuration Cost Projections:\n",
    "  • 1,000 queries/month:  ${comparison_df.loc[2, 'Cost_Per_Query_USD'] * 1000:.2f}\n",
    "  • 10,000 queries/month: ${comparison_df.loc[2, 'Cost_Per_Query_USD'] * 10000:.2f}\n",
    "\n",
    "Cost Optimization Opportunities:\n",
    "  1. Implement response caching for common questions (est. 30-50% cost reduction)\n",
    "  2. Use cheaper LLM for simple questions (tiered routing)\n",
    "  3. Optimize prompt templates to reduce token usage\n",
    "  4. Implement query deduplication\n",
    "\n",
    "{'='*80}\n",
    "GOVERNANCE & COMPLIANCE\n",
    "{'='*80}\n",
    "\n",
    "✓ Data Governance:\n",
    "  • All data stored in Unity Catalog with RBAC\n",
    "  • Knowledge base versioned and tracked\n",
    "  • Complete data lineage from source to model\n",
    "\n",
    "✓ Experiment Tracking:\n",
    "  • All experiments logged in MLflow\n",
    "  • Parameters, metrics, and artifacts tracked\n",
    "  • Complete reproducibility guaranteed\n",
    "\n",
    "✓ Model Registry:\n",
    "  • Models registered in Unity Catalog Model Registry\n",
    "  • Comprehensive documentation attached\n",
    "  • Version history maintained\n",
    "\n",
    "✓ Access Control:\n",
    "  • RBAC implemented for data and models\n",
    "  • Permissions follow principle of least privilege\n",
    "  • Group-based access management\n",
    "\n",
    "✓ Audit Trail:\n",
    "  • All operations logged in Unity Catalog audit logs\n",
    "  • Complete trail for regulatory compliance\n",
    "  • Queryable for security reviews\n",
    "\n",
    "✓ Cost Tracking:\n",
    "  • Token usage logged for every query\n",
    "  • Cost projections calculated\n",
    "  • Budget monitoring enabled\n",
    "\n",
    "{'='*80}\n",
    "PRODUCTION DEPLOYMENT REQUIREMENTS\n",
    "{'='*80}\n",
    "\n",
    "Before deploying to production, complete these tasks:\n",
    "\n",
    "1. ✅ Replace Mock LLM with Production LLM\n",
    "   - Options: DBRX, GPT-4, GPT-3.5-turbo, Llama 2/3\n",
    "   - Configure API keys and endpoints\n",
    "   - Test integration thoroughly\n",
    "\n",
    "2. ✅ Upgrade Retrieval to Vector Search\n",
    "   - Implement vector embeddings (e.g., sentence-transformers)\n",
    "   - Deploy Databricks Vector Search index\n",
    "   - Benchmark retrieval quality\n",
    "\n",
    "3. ✅ Implement Guardrails\n",
    "   - Content filtering (toxicity, PII)\n",
    "   - Input validation and sanitization\n",
    "   - Output verification\n",
    "\n",
    "4. ✅ Add Caching Layer\n",
    "   - Cache common questions and responses\n",
    "   - Implement cache invalidation strategy\n",
    "   - Monitor cache hit rate\n",
    "\n",
    "5. ✅ Set Up Monitoring\n",
    "   - Track answer quality (human feedback)\n",
    "   - Monitor costs and token usage\n",
    "   - Alert on anomalies\n",
    "\n",
    "6. ✅ Conduct Human Evaluation\n",
    "   - Evaluate answer quality on test set\n",
    "   - Measure accuracy, completeness, helpfulness\n",
    "   - Iterate on prompt templates\n",
    "\n",
    "7. ✅ Obtain Compliance Approval\n",
    "   - Submit for compliance review\n",
    "   - Address any concerns\n",
    "   - Document approval\n",
    "\n",
    "8. ✅ Establish Maintenance Process\n",
    "   - Knowledge base update workflow\n",
    "   - Model re-evaluation schedule\n",
    "   - Incident response procedures\n",
    "\n",
    "{'='*80}\n",
    "RECOMMENDATIONS\n",
    "{'='*80}\n",
    "\n",
    "Immediate Actions:\n",
    "  1. ✓ Deploy Champion model (v{model_version.version}) to staging environment\n",
    "  2. ✓ Integrate production LLM (recommend DBRX for cost-effectiveness)\n",
    "  3. ✓ Implement vector-based retrieval\n",
    "  4. ✓ Conduct human evaluation on 100 test questions\n",
    "  5. ✓ Set up cost monitoring and alerts\n",
    "\n",
    "A/B Testing Plan:\n",
    "  1. Deploy Champion (top_k=3) to 90% of traffic\n",
    "  2. Deploy Challenger (top_k=5) to 10% of traffic\n",
    "  3. Monitor for 2 weeks:\n",
    "     - Answer quality (user feedback)\n",
    "     - Cost per query\n",
    "     - Response latency\n",
    "  4. Promote Challenger if quality improvement justifies cost increase\n",
    "\n",
    "Ongoing Maintenance:\n",
    "  1. Update knowledge base weekly (or as needed)\n",
    "  2. Re-evaluate model monthly\n",
    "  3. Review costs and optimize quarterly\n",
    "  4. Conduct compliance audit quarterly\n",
    "\n",
    "{'='*80}\n",
    "RISK ASSESSMENT\n",
    "{'='*80}\n",
    "\n",
    "Technical Risks:\n",
    "  ⚠ Mock LLM: Must be replaced before production (HIGH PRIORITY)\n",
    "  ⚠ Keyword Retrieval: May miss semantically similar documents (MEDIUM)\n",
    "  ⚠ No Guardrails: Risk of inappropriate responses (HIGH)\n",
    "  ⚠ No Caching: Higher costs and latency (MEDIUM)\n",
    "\n",
    "Mitigation Strategies:\n",
    "  ✓ Replace mock LLM with production LLM (in progress)\n",
    "  ✓ Implement vector search (planned)\n",
    "  ✓ Add content filtering and validation (planned)\n",
    "  ✓ Implement caching layer (planned)\n",
    "\n",
    "Business Risks:\n",
    "  ⚠ Cost Escalation: LLM costs can increase with usage\n",
    "  ⚠ Answer Quality: Hallucinations or incorrect information\n",
    "  ⚠ Knowledge Staleness: Outdated information in knowledge base\n",
    "\n",
    "Mitigation Strategies:\n",
    "  ✓ Cost monitoring and budgets\n",
    "  ✓ Human evaluation and feedback loops\n",
    "  ✓ Regular knowledge base updates\n",
    "\n",
    "{'='*80}\n",
    "CONCLUSION\n",
    "{'='*80}\n",
    "\n",
    "The RAG Customer Support Assistant is ready for production deployment pending:\n",
    "  1. Integration with production LLM\n",
    "  2. Implementation of vector-based retrieval\n",
    "  3. Addition of guardrails and safety measures\n",
    "  4. Human evaluation and quality validation\n",
    "\n",
    "All governance controls are in place:\n",
    "  ✓ Data governance via Unity Catalog\n",
    "  ✓ Experiment tracking via MLflow\n",
    "  ✓ Model versioning and documentation\n",
    "  ✓ Access control and audit logging\n",
    "  ✓ Cost tracking and projections\n",
    "\n",
    "Estimated production cost: ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD'] * 1000:.2f}/month (1000 queries)\n",
    "\n",
    "Recommended next step: Deploy to staging environment for integration testing.\n",
    "\n",
    "{'='*80}\n",
    "APPROVAL SIGNATURES\n",
    "{'='*80}\n",
    "\n",
    "Data Science Lead: _________________ Date: _________\n",
    "ML Engineering Lead: _________________ Date: _________\n",
    "Compliance Officer: _________________ Date: _________\n",
    "Product Manager: _________________ Date: _________\n",
    "\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report as artifact\n",
    "with open('/tmp/rag_performance_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"\\n✓ Performance report saved to /tmp/rag_performance_report.txt\")\n",
    "\n",
    "print(\"\\n✓ Report saved to /tmp/model_performance_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bb7bc7f-0e9b-4eff-aa1a-1c445aebad72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 8: Model Archiving and Cleanup Policies\n",
    "\n",
    "As models accumulate, we need policies for:\n",
    "- **Archiving old versions** that are no longer in use\n",
    "- **Cleaning up experiments** to maintain organization\n",
    "- **Retaining compliance records** per regulatory requirements\n",
    "\n",
    "**Best Practice:** Archive models rather than delete them to maintain audit trails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33047538-7867-4a38-8ddc-ebef3dd5ca7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Archive Old Model Versions\n",
    "\n",
    "Let's demonstrate archiving a model version that's no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e3273a-aecf-4981-a567-62faa491585b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to archive old model versions\n",
    "def archive_model_version(model_name, version, reason):\n",
    "    \"\"\"\n",
    "    Archive a model version by adding archive tags and documentation.\n",
    "\n",
    "    Args:\n",
    "        model_name: Full model name in Unity Catalog\n",
    "        version: Version number to archive\n",
    "        reason: Reason for archiving\n",
    "    \"\"\"\n",
    "    client.set_model_version_tag(\n",
    "        name=model_name,\n",
    "        version=version,\n",
    "        key=\"archived\",\n",
    "        value=\"true\"\n",
    "    )\n",
    "\n",
    "    client.set_model_version_tag(\n",
    "        name=model_name,\n",
    "        version=version,\n",
    "        key=\"archive_date\",\n",
    "        value=datetime.now().strftime('%Y-%m-%d')\n",
    "    )\n",
    "\n",
    "    client.set_model_version_tag(\n",
    "        name=model_name,\n",
    "        version=version,\n",
    "        key=\"archive_reason\",\n",
    "        value=reason\n",
    "    )\n",
    "\n",
    "    print(f\"✓ Model version {version} archived\")\n",
    "    print(f\"  Reason: {reason}\")\n",
    "    print(f\"  Date: {datetime.now().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Example: Archive the first version if we have multiple versions\n",
    "if len(all_versions) > 2:\n",
    "    archive_model_version(\n",
    "        MODEL_NAME,\n",
    "        all_versions[-1].version,  # Oldest version\n",
    "        \"Superseded by improved models with better performance\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Note: Archiving demonstration - would archive older versions in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bc53247-70c8-4539-a187-2276f1ae58de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cleanup Policy Implementation\n",
    "\n",
    "Define and implement cleanup policies for model registry maintenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd91aaf2-3cf7-4b66-84e5-57b52ab83072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define cleanup policy\n",
    "cleanup_policy = {\n",
    "    'retain_champion': True,  # Always keep Champion model\n",
    "    'retain_challenger': True,  # Always keep Challenger model\n",
    "    'archive_after_days': 90,  # Archive versions older than 90 days\n",
    "    'max_versions': 10,  # Keep maximum 10 versions\n",
    "    'require_documentation': True  # All versions must have documentation\n",
    "}\n",
    "\n",
    "print(\"=== Model Registry Cleanup Policy ===\\n\")\n",
    "for key, value in cleanup_policy.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "# Implement cleanup check\n",
    "def check_cleanup_needed(model_name, policy):\n",
    "    \"\"\"\n",
    "    Check if cleanup is needed based on policy.\n",
    "\n",
    "    Args:\n",
    "        model_name: Full model name in Unity Catalog\n",
    "        policy: Dictionary of cleanup policies\n",
    "\n",
    "    Returns:\n",
    "        List of versions that can be archived\n",
    "    \"\"\"\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "\n",
    "    # Get versions with aliases (Champion, Challenger)\n",
    "    protected_versions = set()\n",
    "    for version in versions:\n",
    "        if hasattr(version, 'aliases') and version.aliases:\n",
    "            protected_versions.add(version.version)\n",
    "\n",
    "    # Find versions that can be archived\n",
    "    archivable = []\n",
    "    for version in versions:\n",
    "        # Skip protected versions\n",
    "        if version.version in protected_versions:\n",
    "            continue\n",
    "\n",
    "        # Check age\n",
    "        created_time = datetime.fromtimestamp(version.creation_timestamp / 1000)\n",
    "        age_days = (datetime.now() - created_time).days\n",
    "\n",
    "        if age_days > policy['archive_after_days']:\n",
    "            archivable.append({\n",
    "                'version': version.version,\n",
    "                'age_days': age_days,\n",
    "                'created': created_time\n",
    "            })\n",
    "\n",
    "    return archivable\n",
    "\n",
    "# Check cleanup\n",
    "archivable_versions = check_cleanup_needed(MODEL_NAME, cleanup_policy)\n",
    "\n",
    "print(f\"\\n=== Cleanup Analysis ===\")\n",
    "print(f\"Total versions: {len(all_versions)}\")\n",
    "print(f\"Archivable versions: {len(archivable_versions)}\")\n",
    "\n",
    "if archivable_versions:\n",
    "    print(\"\\nVersions eligible for archiving:\")\n",
    "    for v in archivable_versions:\n",
    "        print(f\"  Version {v['version']}: {v['age_days']} days old (created {v['created']})\")\n",
    "else:\n",
    "    print(\"\\n✓ No versions need archiving at this time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2f2cec5-7fb8-4d7d-888d-7dd7e7655686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 9: End-to-End Workflow Summary\n",
    "\n",
    "Let's create a comprehensive summary of everything we've accomplished in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5dbedcd-3709-4c64-a950-270fc47dd1e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary = f\"\"\"\n",
    "{'='*80}\n",
    "MLflow & UNITY CATALOG FOR RAG SYSTEMS - COMPLETE WORKFLOW SUMMARY\n",
    "{'='*80}\n",
    "\n",
    "Lab Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "User: {current_user}\n",
    "Model: {MODEL_NAME}\n",
    "Use Case: RAG Customer Support Assistant\n",
    "\n",
    "{'='*80}\n",
    "SECTION 1: ENVIRONMENT SETUP\n",
    "{'='*80}\n",
    "\n",
    "✓ Unity Catalog configured:\n",
    "  - Catalog: {CATALOG_NAME}\n",
    "  - Schema: {SCHEMA_NAME}\n",
    "  - Model Registry: Unity Catalog (databricks-uc)\n",
    "\n",
    "✓ MLflow configured:\n",
    "  - Experiment: {experiment_name}\n",
    "  - Registry URI: databricks-uc\n",
    "  - Tracking enabled for all experiments\n",
    "\n",
    "✓ RAG Components Initialized:\n",
    "  - Knowledge base table created\n",
    "  - Evaluation questions table created\n",
    "  - Retrieval function implemented\n",
    "  - Mock LLM generator created\n",
    "\n",
    "{'='*80}\n",
    "SECTION 2: DATA PREPARATION\n",
    "{'='*80}\n",
    "\n",
    "✓ Knowledge Base:\n",
    "  - Table: {CATALOG_NAME}.{SCHEMA_NAME}.{KNOWLEDGE_BASE_TABLE}\n",
    "  - Documents: {len(df_knowledge_base):,} documents\n",
    "  - Categories: {', '.join(df_knowledge_base['category'].unique())}\n",
    "  - Version: {df_knowledge_base['version'].iloc[0]}\n",
    "\n",
    "✓ Evaluation Dataset:\n",
    "  - Table: {CATALOG_NAME}.{SCHEMA_NAME}.{EVAL_QUESTIONS_TABLE}\n",
    "  - Questions: {len(df_eval_questions):,} test questions\n",
    "  - Categories: {', '.join(df_eval_questions['category'].unique())}\n",
    "\n",
    "✓ RAG Pipeline:\n",
    "  - Retrieval: Keyword-based search (production: vector search)\n",
    "  - Generation: Mock LLM (production: DBRX, GPT-4, etc.)\n",
    "  - Prompt template: Defined and versioned\n",
    "\n",
    "{'='*80}\n",
    "SECTION 3: EXPERIMENT TRACKING\n",
    "{'='*80}\n",
    "\n",
    "✓ Ran 3 RAG experiments with full MLflow tracking:\n",
    "  1. Baseline (top_k=2) - Cost-optimized configuration\n",
    "  2. Standard (top_k=3) - Balanced configuration\n",
    "  3. Comprehensive (top_k=5) - Quality-optimized configuration\n",
    "\n",
    "✓ Logged for each experiment:\n",
    "  - Parameters (top_k, retrieval_method, llm_model, etc.)\n",
    "  - Metrics (tokens, costs, retrieval stats)\n",
    "  - Artifacts (prompt templates, sample predictions, cost analysis)\n",
    "  - Tags (configuration, optimization_goal, developer, etc.)\n",
    "\n",
    "✓ Best configuration: {best_config_name}\n",
    "  - Avg tokens/query: {comparison_df.loc[best_config_idx, 'Avg_Tokens_Per_Query']:.1f}\n",
    "  - Cost/query: ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD']:.6f}\n",
    "  - Avg docs retrieved: {comparison_df.loc[best_config_idx, 'Avg_Docs_Retrieved']:.1f}\n",
    "\n",
    "✓ Cost Projections (Champion):\n",
    "  - 1,000 queries/month: ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD'] * 1000:.2f}\n",
    "  - 10,000 queries/month: ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD'] * 10000:.2f}\n",
    "\n",
    "{'='*80}\n",
    "SECTION 4: MODEL REGISTRATION\n",
    "{'='*80}\n",
    "\n",
    "✓ Registered best RAG configuration to Unity Catalog\n",
    "✓ Model name: {MODEL_NAME}\n",
    "✓ Champion version: {model_version.version}\n",
    "✓ Added comprehensive documentation including:\n",
    "  - Architecture and configuration details\n",
    "  - Performance metrics and cost projections\n",
    "  - Intended use and limitations\n",
    "  - Deployment requirements\n",
    "  - Compliance and governance information\n",
    "\n",
    "{'='*80}\n",
    "SECTION 5: VERSION MANAGEMENT\n",
    "{'='*80}\n",
    "\n",
    "✓ Registered Challenger configuration (top_k=5)\n",
    "✓ Challenger version: {model_version_v2.version}\n",
    "✓ Set model aliases:\n",
    "  - Champion (Production): Version {model_version.version} (top_k=3)\n",
    "  - Challenger (A/B Test): Version {model_version_v2.version} (top_k=5)\n",
    "✓ Demonstrated version comparison and A/B testing setup\n",
    "✓ Complete version history maintained\n",
    "\n",
    "{'='*80}\n",
    "SECTION 6: GOVERNANCE & COMPLIANCE\n",
    "{'='*80}\n",
    "\n",
    "✓ Unity Catalog RBAC:\n",
    "  - Fine-grained access control on knowledge base and models\n",
    "  - Role-based permissions for different teams\n",
    "  - Group-based access management demonstrated\n",
    "\n",
    "✓ Audit Logging:\n",
    "  - All operations automatically logged\n",
    "  - Complete trail for regulatory compliance\n",
    "  - Queryable for security reviews\n",
    "\n",
    "✓ Data Lineage:\n",
    "  - Full traceability: Knowledge Base → RAG Model → Responses\n",
    "  - Accessible through Unity Catalog UI\n",
    "  - Version tracking for all components\n",
    "\n",
    "✓ Cost Governance:\n",
    "  - Token usage tracked for every query\n",
    "  - Cost projections calculated\n",
    "  - Budget monitoring enabled\n",
    "\n",
    "{'='*80}\n",
    "SECTION 7: REPRODUCIBILITY\n",
    "{'='*80}\n",
    "\n",
    "✓ All experiments fully reproducible via MLflow:\n",
    "  - Complete parameter logging (top_k, prompt templates, etc.)\n",
    "  - All metrics tracked (tokens, costs, retrieval stats)\n",
    "  - Artifacts saved (prompts, predictions, analysis)\n",
    "  - Knowledge base versioned in Unity Catalog\n",
    "\n",
    "✓ Generated comprehensive performance report:\n",
    "  - Executive summary\n",
    "  - Configuration details\n",
    "  - Cost analysis and projections\n",
    "  - Deployment requirements\n",
    "  - Risk assessment\n",
    "\n",
    "{'='*80}\n",
    "SECTION 8: ARCHIVING & CLEANUP\n",
    "{'='*80}\n",
    "\n",
    "✓ Defined cleanup policies:\n",
    "  - Retain Champion and Challenger models\n",
    "  - Archive versions older than 90 days\n",
    "  - Maximum 10 versions per model\n",
    "  - Maintain audit trail for archived models\n",
    "\n",
    "✓ Implemented archiving workflow\n",
    "✓ Version management best practices established\n",
    "\n",
    "{'='*80}\n",
    "KEY ACHIEVEMENTS - RAG GOVERNANCE\n",
    "{'='*80}\n",
    "\n",
    "1. ✓ Complete MLflow experiment tracking for RAG systems\n",
    "2. ✓ Unity Catalog integration for RAG governance\n",
    "3. ✓ Cost tracking and optimization for LLM-based systems\n",
    "4. ✓ Model versioning and lifecycle management\n",
    "5. ✓ RBAC and access control for knowledge bases\n",
    "6. ✓ Audit logging and compliance readiness\n",
    "7. ✓ Data lineage from knowledge base to responses\n",
    "8. ✓ Reproducibility best practices for RAG\n",
    "9. ✓ Comprehensive documentation and reporting\n",
    "\n",
    "{'='*80}\n",
    "PRODUCTION READINESS CHECKLIST\n",
    "{'='*80}\n",
    "\n",
    "Completed:\n",
    "  ✓ RAG pipeline developed and tested\n",
    "  ✓ Best configuration registered in Unity Catalog\n",
    "  ✓ Comprehensive documentation complete\n",
    "  ✓ Governance controls in place\n",
    "  ✓ Audit trail established\n",
    "  ✓ Cost tracking and projections calculated\n",
    "  ✓ Cleanup policies implemented\n",
    "  ✓ Team access controls configured\n",
    "\n",
    "Pending (Before Production Deployment):\n",
    "  ⚠ Replace mock LLM with production LLM (DBRX, GPT-4, etc.)\n",
    "  ⚠ Implement vector-based retrieval (Databricks Vector Search)\n",
    "  ⚠ Add content filtering and guardrails\n",
    "  ⚠ Implement response caching\n",
    "  ⚠ Conduct human evaluation of answer quality\n",
    "  ⚠ Set up monitoring and alerting\n",
    "  ⚠ Obtain compliance approval\n",
    "  ⚠ Deploy to Model Serving endpoint\n",
    "\n",
    "{'='*80}\n",
    "NEXT STEPS\n",
    "{'='*80}\n",
    "\n",
    "Immediate (Week 1-2):\n",
    "  1. Integrate production LLM (DBRX recommended for cost-effectiveness)\n",
    "  2. Implement vector embeddings and Databricks Vector Search\n",
    "  3. Add content filtering and input validation\n",
    "  4. Deploy to staging environment\n",
    "\n",
    "Short-term (Week 3-4):\n",
    "  5. Conduct human evaluation on 100+ test questions\n",
    "  6. Implement response caching for common questions\n",
    "  7. Set up monitoring dashboard (costs, quality, latency)\n",
    "  8. Configure alerting for anomalies\n",
    "\n",
    "Medium-term (Month 2-3):\n",
    "  9. Deploy Champion to production with 100% traffic\n",
    "  10. Set up A/B test: Champion (90%) vs Challenger (10%)\n",
    "  11. Establish knowledge base update workflow\n",
    "  12. Create runbook for incident response\n",
    "\n",
    "Long-term (Ongoing):\n",
    "  13. Monthly model re-evaluation\n",
    "  14. Quarterly compliance audits\n",
    "  15. Continuous knowledge base updates\n",
    "  16. Regular cost optimization reviews\n",
    "\n",
    "{'='*80}\n",
    "COMPLIANCE NOTES\n",
    "{'='*80}\n",
    "\n",
    "✓ All data stored in Unity Catalog with access controls\n",
    "✓ Complete audit trail maintained for all operations\n",
    "✓ Model lineage fully documented (KB → Model → Responses)\n",
    "✓ Reproducibility guaranteed via MLflow tracking\n",
    "✓ Cost tracking and budget monitoring in place\n",
    "✓ RBAC implemented for data and model access\n",
    "✓ Documentation meets regulatory requirements\n",
    "✓ Ready for compliance review\n",
    "\n",
    "Regulatory Considerations:\n",
    "  • Financial services: Ensure responses comply with regulations\n",
    "  • Data privacy: Implement PII detection and redaction\n",
    "  • Audit requirements: Maintain logs for required retention period\n",
    "  • Model explainability: Document retrieval and generation process\n",
    "\n",
    "{'='*80}\n",
    "LAB COMPLETE - ENTERPRISE RAG GOVERNANCE ACHIEVED\n",
    "{'='*80}\n",
    "\n",
    "Congratulations! You've successfully implemented enterprise-grade governance\n",
    "for a RAG (Retrieval-Augmented Generation) system using MLflow and Unity Catalog.\n",
    "\n",
    "You now have:\n",
    "  ✓ Complete experiment tracking and reproducibility\n",
    "  ✓ Centralized model registry with versioning\n",
    "  ✓ Fine-grained access control and audit logging\n",
    "  ✓ Cost tracking and optimization framework\n",
    "  ✓ Production-ready deployment pipeline\n",
    "  ✓ Compliance and governance controls\n",
    "\n",
    "This foundation enables you to:\n",
    "  • Deploy RAG systems with confidence\n",
    "  • Meet regulatory and compliance requirements\n",
    "  • Track and optimize costs at scale\n",
    "  • Maintain complete audit trails\n",
    "  • Collaborate effectively across teams\n",
    "  • Iterate and improve continuously\n",
    "\n",
    "Next: Deploy to production and start serving customers!\n",
    "\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open('/tmp/rag_lab_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\n✓ Lab summary saved to /tmp/rag_lab_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21ae9f6e-0cc3-4dbd-95f2-80a66b306ae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 10: Hands-On Exercises\n",
    "\n",
    "Now that you've completed the guided lab, try these exercises to reinforce your learning:\n",
    "\n",
    "### Exercise 1: Experiment with Different RAG Configurations\n",
    "- Create a new RAG experiment with top_k=4\n",
    "- Try a different prompt template (modify PROMPT_TEMPLATE)\n",
    "- Log the experiment to MLflow with appropriate tags\n",
    "- Compare cost and token usage to existing configurations\n",
    "\n",
    "### Exercise 2: Enhance the Knowledge Base\n",
    "- Add 5 new documents to the knowledge base\n",
    "- Update the version number\n",
    "- Re-run the evaluation with the updated knowledge base\n",
    "- Compare retrieval quality before and after\n",
    "\n",
    "### Exercise 3: Create Custom Evaluation Questions\n",
    "- Add 10 new evaluation questions to the eval_questions table\n",
    "- Run predictions on the new questions\n",
    "- Analyze which questions are answered well vs. poorly\n",
    "- Identify knowledge gaps in the knowledge base\n",
    "\n",
    "### Exercise 4: Implement Cost Monitoring\n",
    "- Create a function to track cumulative costs across all experiments\n",
    "- Calculate cost per category of question\n",
    "- Identify the most expensive question types\n",
    "- Propose cost optimization strategies\n",
    "\n",
    "### Exercise 5: Promote and Test the Challenger\n",
    "- Load both Champion and Challenger models\n",
    "- Run the same questions through both\n",
    "- Compare responses, costs, and token usage\n",
    "- Make a data-driven decision on which to promote\n",
    "\n",
    "### Exercise 6: Query Audit Logs\n",
    "- Query Unity Catalog audit logs\n",
    "- Find all operations on your RAG model\n",
    "- Create a compliance report showing:\n",
    "  - Who registered the model\n",
    "  - When aliases were set\n",
    "  - All table access operations\n",
    "\n",
    "### Exercise 7: Archive Old Versions\n",
    "- Identify versions that should be archived\n",
    "- Apply archiving tags with proper documentation\n",
    "- Verify archived versions are still loadable (for audit purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67750962-2af5-4135-aac8-643822541318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've completed a comprehensive lab on MLflow and Unity Catalog for enterprise RAG governance.\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **RAG System Architecture**\n",
    "   - Building retrieval-augmented generation pipelines\n",
    "   - Implementing keyword-based retrieval (foundation for vector search)\n",
    "   - Creating mock LLM generators for cost-effective development\n",
    "   - Packaging RAG systems as MLflow PyFunc models\n",
    "\n",
    "2. **MLflow Experiment Tracking for RAG**\n",
    "   - Logging RAG-specific parameters (top_k, prompt templates, LLM settings)\n",
    "   - Tracking cost metrics (token usage, estimated costs)\n",
    "   - Saving artifacts (prompts, sample predictions, cost analysis)\n",
    "   - Comparing different RAG configurations\n",
    "\n",
    "3. **Model Registry with Unity Catalog**\n",
    "   - Registering RAG models with comprehensive documentation\n",
    "   - Managing model lifecycle with aliases (Champion, Challenger)\n",
    "   - Loading models for inference\n",
    "   - Maintaining version history for audit trails\n",
    "\n",
    "4. **Enterprise Governance for RAG**\n",
    "   - RBAC for knowledge bases and models\n",
    "   - Audit logging for compliance\n",
    "   - Data lineage from knowledge base to responses\n",
    "   - Cost governance and budget monitoring\n",
    "\n",
    "5. **RAG-Specific Best Practices**\n",
    "   - Cost tracking and optimization for LLM-based systems\n",
    "   - Prompt template versioning\n",
    "   - Knowledge base versioning and updates\n",
    "   - Evaluation methodology for RAG quality\n",
    "   - Production deployment requirements\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "This RAG governance workflow is used in production environments for:\n",
    "- **Customer Support**: Automated question answering, ticket deflection\n",
    "- **Internal Knowledge Management**: Employee self-service, policy Q&A\n",
    "- **Financial Services**: Regulatory compliance Q&A, product information\n",
    "- **Healthcare**: Medical information retrieval, patient education\n",
    "- **Legal**: Contract analysis, legal research assistance\n",
    "- **E-commerce**: Product recommendations, customer inquiries\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "✅ **Cost Control**: Track every token and dollar spent on LLM inference\n",
    "✅ **Reproducibility**: Every RAG experiment is fully reproducible\n",
    "✅ **Governance**: Complete audit trail from knowledge base to responses\n",
    "✅ **Scalability**: Framework supports production deployment at scale\n",
    "✅ **Compliance**: Meets regulatory requirements for financial services and healthcare\n",
    "✅ **Flexibility**: Easy to swap retrieval methods, LLMs, and configurations\n",
    "\n",
    "### Production Deployment Checklist:\n",
    "\n",
    "Before deploying your RAG system to production:\n",
    "- [ ] Replace mock LLM with production LLM (DBRX, GPT-4, etc.)\n",
    "- [ ] Implement vector-based retrieval (Databricks Vector Search)\n",
    "- [ ] Add content filtering and guardrails\n",
    "- [ ] Implement response caching\n",
    "- [ ] Conduct human evaluation (100+ questions)\n",
    "- [ ] Set up monitoring and alerting\n",
    "- [ ] Obtain compliance approval\n",
    "- [ ] Deploy to Databricks Model Serving\n",
    "- [ ] Establish knowledge base update workflow\n",
    "- [ ] Create incident response runbook\n",
    "\n",
    "### Resources:\n",
    "\n",
    "**MLflow & Unity Catalog:**\n",
    "- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)\n",
    "- [Unity Catalog Documentation](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n",
    "- [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html)\n",
    "\n",
    "**RAG & LLMs:**\n",
    "- [Databricks Foundation Models](https://docs.databricks.com/machine-learning/foundation-models/index.html)\n",
    "- [Databricks Vector Search](https://docs.databricks.com/vector-search/index.html)\n",
    "- [RAG Best Practices](https://docs.databricks.com/generative-ai/retrieval-augmented-generation.html)\n",
    "\n",
    "**Governance & Compliance:**\n",
    "- [Unity Catalog RBAC](https://docs.databricks.com/data-governance/unity-catalog/manage-privileges/index.html)\n",
    "- [Audit Logging](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html)\n",
    "- [Data Lineage](https://docs.databricks.com/data-governance/unity-catalog/data-lineage.html)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Immediate**: Integrate a production LLM and test with real queries\n",
    "2. **Short-term**: Implement vector search for better retrieval quality\n",
    "3. **Medium-term**: Deploy to production and monitor performance\n",
    "4. **Long-term**: Continuously improve based on user feedback and metrics\n",
    "\n",
    "### Thank You!\n",
    "\n",
    "You're now equipped to implement enterprise-grade governance for RAG systems in your organization.\n",
    "\n",
    "**Key Achievement**: You can now build, track, govern, and deploy RAG systems that meet enterprise requirements for cost control, compliance, and quality.\n",
    "\n",
    "**Remember**: The governance framework you've learned applies to any LLM-based system, not just RAG. Use these patterns for:\n",
    "- Fine-tuned models\n",
    "- Prompt engineering experiments\n",
    "- Multi-agent systems\n",
    "- Any GenAI application\n",
    "\n",
    "Good luck with your RAG deployments! \uD83D\uDE80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eef8e380-6b98-48c0-b114-4ea267b91f89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final verification - Display key resources\n",
    "print(\"=\"*80)\n",
    "print(\"RAG LAB RESOURCES - QUICK REFERENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n\uD83D\uDCDA Knowledge Base:\")\n",
    "print(f\"   - Table: {CATALOG_NAME}.{SCHEMA_NAME}.{KNOWLEDGE_BASE_TABLE}\")\n",
    "print(f\"   - Documents: {len(df_knowledge_base):,}\")\n",
    "print(f\"   - Version: {df_knowledge_base['version'].iloc[0]}\")\n",
    "\n",
    "print(f\"\\n❓ Evaluation Questions:\")\n",
    "print(f\"   - Table: {CATALOG_NAME}.{SCHEMA_NAME}.{EVAL_QUESTIONS_TABLE}\")\n",
    "print(f\"   - Questions: {len(df_eval_questions):,}\")\n",
    "\n",
    "print(f\"\\n\uD83E\uDD16 Model Registry:\")\n",
    "print(f\"   - Model Name: {MODEL_NAME}\")\n",
    "print(f\"   - Champion: Version {model_version.version} (top_k=3)\")\n",
    "print(f\"   - Challenger: Version {model_version_v2.version} (top_k=5)\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDD2C MLflow Experiment:\")\n",
    "print(f\"   - Experiment: {experiment_name}\")\n",
    "print(f\"   - Total Runs: 3 (Baseline, Standard, Comprehensive)\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCB0 Cost Summary (Champion):\")\n",
    "print(f\"   - Cost per query: ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD']:.6f}\")\n",
    "print(f\"   - Monthly (1K queries): ${comparison_df.loc[best_config_idx, 'Cost_Per_Query_USD'] * 1000:.2f}\")\n",
    "print(f\"   - Avg tokens/query: {comparison_df.loc[best_config_idx, 'Avg_Tokens_Per_Query']:.1f}\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCC1 Generated Reports:\")\n",
    "print(f\"   - Performance Report: /tmp/rag_performance_report.txt\")\n",
    "print(f\"   - Lab Summary: /tmp/rag_lab_summary.txt\")\n",
    "\n",
    "print(f\"\\n\uD83C\uDFAF Next Steps:\")\n",
    "print(f\"   1. Replace mock LLM with production LLM\")\n",
    "print(f\"   2. Implement vector-based retrieval\")\n",
    "print(f\"   3. Add guardrails and content filtering\")\n",
    "print(f\"   4. Deploy to Model Serving\")\n",
    "\n",
    "print(f\"\\n✅ Lab Status: COMPLETE\")\n",
    "print(f\"✅ RAG Governance Framework: ESTABLISHED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n\uD83C\uDF89 Congratulations! You've successfully built an enterprise-grade RAG system\")\n",
    "print(f\"   with complete MLflow tracking and Unity Catalog governance!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Orielly -Chapter 6-End-to-End Model Management with MLflow and Unity Catalog",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}