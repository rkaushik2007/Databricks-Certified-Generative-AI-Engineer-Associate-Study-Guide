{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c4fa92d-cea1-4c27-907d-d1c768a86395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üß™ Hands-On Lab: End-to-End RAG System Deployment on Databricks\n",
    "\n",
    "## üìå Scenario\n",
    "\n",
    "You are a **machine learning engineer** at a large enterprise responsible for deploying a **Retrieval-Augmented Generation (RAG) application** on Databricks. Business stakeholders want employees to query internal knowledge sources‚Äîsuch as **technical documentation, compliance policies, and internal reports**‚Äîusing natural language.\n",
    "\n",
    "While an initial prototype already exists, leadership now requires a **production-ready solution** that is:\n",
    "- **Secure** and governed according to enterprise standards\n",
    "- **Scalable** to handle enterprise workloads\n",
    "- **Cost-aware** with appropriate resource controls\n",
    "- **Testable and reliable** for production operations\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Your task is **not limited to building a working RAG pipeline**. You must design the system so that it can be:\n",
    "- **Tested** with validation queries\n",
    "- **Deployed** to production endpoints\n",
    "- **Accessed securely** with identity-based controls\n",
    "- **Operated reliably** with monitoring and governance\n",
    "\n",
    "This includes:\n",
    "1. Translating functional requirements into a well-defined **RAG chain**\n",
    "2. Packaging that chain using **MLflow PyFunc**\n",
    "3. Deploying it to a **Databricks Model Serving endpoint**\n",
    "4. Ensuring that **access, performance, and resource usage** are appropriately controlled\n",
    "\n",
    "Throughout the lab, you will work within the boundaries of **Databricks-managed services** rather than implementing custom infrastructure logic.\n",
    "\n",
    "### Real-World Context\n",
    "\n",
    "This lab reflects **real-world enterprise conditions** where deployment decisions, access control, and resource planning are as important as model accuracy. You will implement a complete RAG deployment workflow that aligns with all concepts covered in Chapter 5.\n",
    "\n",
    "Specifically, you will:\n",
    "- Translate a simple business requirement into a deployable RAG chain\n",
    "- Package the chain inside an MLflow PyFunc model with explicit pre-processing and post-processing\n",
    "- Create and query a Vector Search index to support retrieval\n",
    "- Register and deploy the model to a Databricks Model Serving endpoint\n",
    "- Apply identity-based access controls to restrict who can invoke the endpoint\n",
    "- Reason about and observe resource usage across vector search and model serving components\n",
    "\n",
    "Each step demonstrates how RAG systems transition from notebooks to governed production environments.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "By the end of this lab, you will be able to do the following:\n",
    "\n",
    "1. **Design a requirement-driven RAG chain** that performs retrieval and generation in a predictable sequence\n",
    "2. **Package a LangChain-based RAG pipeline** into an MLflow PyFunc model using supported APIs\n",
    "3. **Implement basic pre-processing and post-processing logic** outside of the core chain\n",
    "4. **Track experiments, parameters, and artifacts** using MLflow\n",
    "5. **Register a model and deploy it** to a Databricks Model Serving endpoint\n",
    "6. **Apply identity-based access control** to manage who can invoke or manage the endpoint\n",
    "7. **Build and query a Vector Search index** to support similarity-based retrieval\n",
    "8. **Identify which resource category to adjust** when diagnosing latency, cost, or scaling issues\n",
    "9. **Execute test queries against the deployed endpoint** to validate correctness and behavior\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Technologies Used\n",
    "\n",
    "- **MLflow** ‚Üí Experiment tracking, model packaging, and registration\n",
    "- **Databricks Model Serving** ‚Üí Production deployment and endpoint management\n",
    "- **Databricks Vector Search** ‚Üí Similarity-based document retrieval\n",
    "- **Unity Catalog** ‚Üí Governance, access control, and version management\n",
    "- **LangChain** ‚Üí RAG chain orchestration\n",
    "- **PySpark** ‚Üí Distributed data processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25c7ac60-d468-4fb0-acad-1d29fb6673af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 1: Install Required Libraries\n",
    "\n",
    "In this step, we install the Python packages necessary for building and deploying the **Retrieval-Augmented Generation (RAG) system** on Databricks.\n",
    "\n",
    "- **databricks-vectorsearch** ‚Üí Provides APIs for creating and querying Vector Search indexes.\n",
    "- **mlflow** ‚Üí Used for experiment tracking, packaging the RAG pipeline, and model registration.\n",
    "- **langchain** ‚Üí Simplifies orchestration of retrieval + generation workflows.\n",
    "- **tiktoken** ‚Üí Tokenizer for working with LLM prompts and\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2b3f720-b607-4bcb-b85b-781d7f97eeab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet databricks-vectorsearch mlflow langchain tiktoken requests\n",
    "%pip install --quiet -U databricks-vectorsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88995e4a-30a8-43fb-be10-2a25f5bb76b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 2: Restart Python Kernel\n",
    "\n",
    "After installing new libraries with `%pip install`, we need to restart the Python kernel so that the environment picks up the newly installed or upgraded packages.\n",
    "This ensures that the correct versions of `databricks-vectorsearch`, `mlflow`, `langchain`, and others are available for use in subsequent steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "765dcb9d-e884-4bc9-8ddd-af95f018cba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e953c78-2683-467c-9db0-70037322b43f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 3: Define Configuration Variables\n",
    "\n",
    "In this step, we define **all environment-specific configuration values** required for the lab.\n",
    "These variables make the notebook portable and easier to adapt across environments (development, staging, production).\n",
    "\n",
    "Key sections:\n",
    "\n",
    "- **Databricks Workspace Configuration**\n",
    "  Workspace URL, authentication token, and secret scope setup.\n",
    "  ‚ö†Ô∏è For production use, prefer `dbutils.secrets.get()` instead of hardcoding tokens.\n",
    "\n",
    "- **Model and Endpoint Configuration**\n",
    "  Embedding model endpoint, Vector Search endpoint name, registered model name, and the final serving endpoint.\n",
    "\n",
    "- **Database Configuration**\n",
    "  Catalog, schema, and table names for raw documents, processed chunks, embeddings, and the vector index.\n",
    "\n",
    "- **Processing Configuration**\n",
    "  Chunk sizes, batch sizes, similarity search parameters, and request timeouts.\n",
    "\n",
    "- **Circuit Breaker Configuration**\n",
    "  Settings for failure thresholds and recovery logic to improve production resilience.\n",
    "\n",
    "- **Derived Configuration**\n",
    "  Fully qualified paths and derived variables (do not modify).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16965fa7-0c51-4a16-8f74-c6e5589780ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# üîë How to Create a Databricks Personal Access Token (PAT)\n",
    "\n",
    "A **Personal Access Token (PAT)** is required for programmatic access to Databricks REST APIs and Model Serving.\n",
    "Follow these steps to generate one:\n",
    "\n",
    "1. **Log in** to your Databricks workspace.\n",
    "2. In the top-right corner, click on your **user profile icon** ‚Üí select **User Settings**.\n",
    "3. Go to the **Access tokens** tab.\n",
    "4. Click **Generate new token**.\n",
    "5. Provide a **description** (e.g., \"RAG Lab Token\") and optionally set an **expiry date**.\n",
    "6. Click **Generate**.\n",
    "7. Copy the token immediately ‚Äî it will not be shown again.\n",
    "8. Use this token in your code (preferably via `dbutils.secrets.get()` in production for security).\n",
    "\n",
    "‚ö†Ô∏è **Best Practices:**\n",
    "- Store the token in **Databricks Secret Scope** instead of hardcoding.\n",
    "- Use **short-lived tokens** whenever possible.\n",
    "- Rotate and revoke tokens regularly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0027ad8c-7ee1-4eae-a34b-6269a789d8ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION VARIABLES - MODIFY THESE FOR YOUR ENVIRONMENT\n",
    "# =============================================================================\n",
    "\n",
    "# Databricks Workspace Configuration\n",
    "WORKSPACE_URL = \"Put your workspace URL\"\n",
    "# For example \"https://adb-YOUR-WORKSPACE-ID.azuredatabricks.net\"\n",
    "TOKEN = \"Put personal access token\"\n",
    "# for example \"dapi_YOUR_DATABRICKS_TOKEN_HERE-2\"  # Consider using dbutils.secrets.get() for production\n",
    "SECRET_SCOPE = \"corp_lab\"\n",
    "SECRET_KEY = \"databricks_pat\"\n",
    "\n",
    "# Model and Endpoint Configuration\n",
    "EMBEDDING_ENDPOINT = \"databricks-bge-large-en\"\n",
    "VECTOR_SEARCH_ENDPOINT_NAME = \"orielly-chapter5-endpoint\"\n",
    "MODEL_NAME = \"main.default.rag_pyfunc\"\n",
    "SERVING_ENDPOINT_NAME = \"rag-pyfunc-endpoint-Chapter-5\"\n",
    "\n",
    "# Database Configuration\n",
    "CATALOG_NAME = \"corp_ai\"\n",
    "SCHEMA_NAME = \"rag_lab\"\n",
    "RAW_TABLE = \"docs_raw\"\n",
    "CHUNKS_TABLE = \"docs_chunks\"\n",
    "EMBEDDINGS_TABLE = \"docs_embed\"\n",
    "VECTOR_INDEX_NAME = \"docs_index_sync\"\n",
    "\n",
    "# Processing Configuration\n",
    "CHUNK_SIZE = 350\n",
    "BATCH_SIZE = 32\n",
    "SIMILARITY_SEARCH_RESULTS = 5\n",
    "REQUEST_TIMEOUT = 60\n",
    "\n",
    "# Circuit Breaker Configuration\n",
    "FAILURE_THRESHOLD = 20  # 20% failure rate\n",
    "RECOVERY_TIMEOUT = 60   # 1 minute recovery\n",
    "SUCCESS_THRESHOLD = 3   # 3 successes to close\n",
    "WINDOW_SIZE = 50        # Track last 50 requests\n",
    "MIN_REQUESTS = 10       # Minimum requests before calculating failure rate\n",
    "\n",
    "# Derived Configuration (DO NOT MODIFY)\n",
    "FULL_CATALOG_SCHEMA = f\"{CATALOG_NAME}.{SCHEMA_NAME}\"\n",
    "SOURCE_TABLE_FULLNAME = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{CHUNKS_TABLE}\"\n",
    "VS_INDEX_FULLNAME = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{VECTOR_INDEX_NAME}\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "RETURN_COLUMNS = [\"chunk_id\", \"doc_id\", \"section\", \"product_line\", \"region\", \"chunk\"]\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully\")\n",
    "print(f\"üìç Workspace: {WORKSPACE_URL}\")\n",
    "print(f\"üóÑÔ∏è Database: {FULL_CATALOG_SCHEMA}\")\n",
    "print(f\"ü§ñ Model: {MODEL_NAME}\")\n",
    "print(f\"üîó Serving Endpoint: {SERVING_ENDPOINT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d590f067-2a67-4589-b0d4-9dfec9372203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 4: Import Required Libraries\n",
    "\n",
    "In this step, we import all the Python libraries required for the **RAG system deployment**.\n",
    "\n",
    "- **Core Python libraries** ‚Üí Utilities for file handling, JSON, concurrency, random sampling, and system operations.\n",
    "- **Data Processing (Pandas, NumPy)** ‚Üí Efficient manipulation of tabular data and numerical arrays.\n",
    "- **Requests** ‚Üí For making REST API calls to Databricks endpoints.\n",
    "- **MLflow** ‚Üí Used for model logging, tracking, registration, and signature inference.\n",
    "- **Databricks Vector Search Client** ‚Üí To create, query, and manage Vector Search indexes.\n",
    "- **PySpark** ‚Üí Provides distributed processing and DataFrame APIs for preparing documents, embeddings, and feature engineering.\n",
    "\n",
    "Once this cell runs, you will have all necessary libraries loaded and ready for use in the following steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7812b61-b35f-4155-ac4a-91af28464996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS - ALL REQUIRED LIBRARIES\n",
    "# =============================================================================\n",
    "\n",
    "# Core Python libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import tempfile\n",
    "import threading\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque\n",
    "from enum import Enum\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# HTTP requests\n",
    "import requests\n",
    "\n",
    "# MLflow and Databricks\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b140905e-5be8-48d1-8d39-a4d853e8e5ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 5: Initialize Database Catalog and Schema\n",
    "\n",
    "In this step, we set up the **Databricks Unity Catalog** and a dedicated schema for storing all artifacts of the RAG pipeline.\n",
    "\n",
    "Why this matters:\n",
    "- **Catalogs** provide a top-level namespace in Unity Catalog.\n",
    "- **Schemas** organize related tables and models inside a catalog.\n",
    "- Ensures all tables (raw docs, chunks, embeddings) and models are grouped under a governed namespace.\n",
    "- Promotes **data governance, access control, and reproducibility** across teams.\n",
    "\n",
    "Here, we:\n",
    "1. Create the catalog (if it doesn‚Äôt already exist).\n",
    "2. Create the schema within that catalog.\n",
    "3. Switch the Spark session to use this catalog and schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b721d46-9e53-45b7-859e-9470fc2d0c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create catalog and schema\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {FULL_CATALOG_SCHEMA}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA_NAME}\")\n",
    "\n",
    "print(f\"‚úÖ Database setup complete: {FULL_CATALOG_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54332c52-f6ef-47bd-ba03-653c0edfc65f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 6: Load Sample Enterprise Documents\n",
    "\n",
    "To simulate enterprise knowledge bases (such as compliance manuals, product specifications, and policy handbooks), we create a **sample dataset**.\n",
    "\n",
    "Why this matters:\n",
    "- Provides a controlled **corpus of documents** for testing the RAG system.\n",
    "- Each document includes metadata such as:\n",
    "  - `doc_id` ‚Üí Unique document identifier\n",
    "  - `doc_type` ‚Üí Type of document (manual, spec, handbook)\n",
    "  - `section` ‚Üí Section or chapter reference\n",
    "  - `product_line` ‚Üí Product relevance\n",
    "  - `region` ‚Üí Regional applicability\n",
    "  - `effective_date` ‚Üí Date the policy/spec becomes effective\n",
    "  - `text` ‚Üí The actual document content\n",
    "\n",
    "These documents are written into a Delta table (`docs_raw`) under the configured catalog and schema.\n",
    "This ensures governance and easy retrieval when we process them into chunks and embeddings in later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21768925-3cfa-4cb1-a009-6d851a580e69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample enterprise documents\n",
    "sample_data = [\n",
    "    (\"DOC-001\", \"Compliance Manual\", \"Storage Policy\", \"product-a\", \"us\", \"2024-01-15\",\n",
    "     \"All customer data must be stored in encrypted volumes with AES-256. Backups require weekly integrity checks and must reside in approved regions.\"),\n",
    "    (\"DOC-002\", \"Compliance Manual\", \"Access Control\", \"product-a\", \"eu\", \"2024-03-01\",\n",
    "     \"Access to production data requires MFA and is restricted to on-call engineers. All access events must be logged and retained for 365 days.\"),\n",
    "    (\"DOC-003\", \"Product Spec\", \"Warranty Terms\", \"product-b\", \"us\", \"2023-11-20\",\n",
    "     \"Product-B includes a standard warranty of 12 months covering manufacturing defects. Consumables and accidental damage are excluded.\"),\n",
    "    (\"DOC-004\", \"Product Spec\", \"Maintenance Guide\", \"product-b\", \"apac\", \"2023-10-05\",\n",
    "     \"Maintenance requires quarterly inspections and replacement of filters after 500 hours of operation. Use only certified parts.\"),\n",
    "    (\"DOC-005\", \"Policy Handbook\", \"Data Retention\", \"shared\", \"us\", \"2024-02-10\",\n",
    "     \"Logs must be retained for a minimum of 180 days and a maximum of 730 days depending on classification. High-sensitivity logs require masking.\")\n",
    "]\n",
    "\n",
    "# Define schema for the documents\n",
    "document_schema = T.StructType([\n",
    "    T.StructField(\"doc_id\", T.StringType()),\n",
    "    T.StructField(\"doc_type\", T.StringType()),\n",
    "    T.StructField(\"section\", T.StringType()),\n",
    "    T.StructField(\"product_line\", T.StringType()),\n",
    "    T.StructField(\"region\", T.StringType()),\n",
    "    T.StructField(\"effective_date\", T.StringType()),\n",
    "    T.StructField(\"text\", T.StringType()),\n",
    "])\n",
    "\n",
    "# Create DataFrame and save to table\n",
    "df_raw = spark.createDataFrame(sample_data, document_schema)\n",
    "df_raw = df_raw.withColumn(\"effective_date\", F.to_date(\"effective_date\"))\n",
    "df_raw.write.mode(\"overwrite\").saveAsTable(f\"{FULL_CATALOG_SCHEMA}.{RAW_TABLE}\")\n",
    "\n",
    "print(f\"‚úÖ Sample data created: {len(sample_data)} documents\")\n",
    "display(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9acbe064-559d-46af-a68e-3df39e75ec59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 7: Chunk Documents for Embedding\n",
    "\n",
    "Large documents are difficult to embed and query directly. To make them more manageable and semantically searchable, we split them into **smaller chunks** of text.\n",
    "\n",
    "### Why this matters:\n",
    "- Embeddings models have input size limits (token limits).\n",
    "- Chunking ensures each piece of text is within the embedding model‚Äôs capacity.\n",
    "- Improves retrieval accuracy, since queries can match **specific sections** rather than entire documents.\n",
    "- Each chunk is assigned a unique `chunk_id` for tracking and indexing.\n",
    "\n",
    "### Approach:\n",
    "1. Use a **UDF (User Defined Function)** `simple_chunker`:\n",
    "   - Splits text into sentences.\n",
    "   - Groups sentences into chunks until `CHUNK_SIZE` is reached.\n",
    "   - Produces an array of text chunks.\n",
    "2. Explode the chunks into individual rows.\n",
    "3. Assign unique `chunk_id`s.\n",
    "4. Save results to a governed Delta table (`docs_chunks`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdb0c6c6-5ead-44ec-a6cc-c7a5b86e9adc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Document chunking function\n",
    "@F.udf(\"array<string>\")\n",
    "def simple_chunker(text):\n",
    "    import re\n",
    "    sents = re.split(r\"(?<=[.!?])\\s+\", text.strip())\n",
    "    chunks, cur = [], []\n",
    "    total = 0\n",
    "    for s in sents:\n",
    "        total += len(s)\n",
    "        cur.append(s)\n",
    "        if total > CHUNK_SIZE:\n",
    "            chunks.append(\" \".join(cur))\n",
    "            cur, total = [], 0\n",
    "    if cur:\n",
    "        chunks.append(\" \".join(cur))\n",
    "    return chunks\n",
    "\n",
    "# Process documents into chunks\n",
    "chunks = (spark.table(f\"{FULL_CATALOG_SCHEMA}.{RAW_TABLE}\")\n",
    "    .withColumn(\"chunks\", simple_chunker(F.col(\"text\")))\n",
    "    .withColumn(\"chunk\", F.explode(\"chunks\"))\n",
    "    .withColumn(\"chunk_id\", F.monotonically_increasing_id())\n",
    "    .select(\"chunk_id\", \"doc_id\", \"doc_type\", \"section\", \"product_line\", \"region\", \"effective_date\", \"chunk\")\n",
    ")\n",
    "\n",
    "chunks.write.mode(\"overwrite\").saveAsTable(f\"{FULL_CATALOG_SCHEMA}.{CHUNKS_TABLE}\")\n",
    "print(f\"‚úÖ Document chunking complete\")\n",
    "display(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e581b408-550f-4543-8df4-3a94a0faff2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 8: Test Embedding Endpoint Connectivity\n",
    "\n",
    "Before generating embeddings for all document chunks, we first test the **embedding model endpoint** to ensure it is accessible and returning vectors correctly.\n",
    "\n",
    "### Why this matters:\n",
    "- Confirms that the configured **Databricks embedding endpoint** (`databricks-bge-large-en`) is online and reachable.\n",
    "- Ensures that authentication headers and workspace URLs are correctly set up.\n",
    "- Validates that the output vector has the expected dimensionality (e.g., 1024 dimensions).\n",
    "\n",
    "We send a simple test sentence to the endpoint and check the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a900156-52e8-46ce-b8e2-a7ca2df1b93c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test embedding endpoint connectivity\n",
    "payload_single = {\"input\": \"Databricks simplifies production RAG pipelines.\"}\n",
    "response = requests.post(\n",
    "    f\"{WORKSPACE_URL}/serving-endpoints/{EMBEDDING_ENDPOINT}/invocations\",\n",
    "    headers=HEADERS,\n",
    "    data=json.dumps(payload_single),\n",
    "    timeout=REQUEST_TIMEOUT\n",
    ")\n",
    "response.raise_for_status()\n",
    "embedding = response.json()[\"data\"][0][\"embedding\"]\n",
    "print(f\"‚úÖ Embedding endpoint test successful - Dimension: {len(embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb67bb8e-243c-4a13-b14b-590ad53f584e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 9: Generate Embeddings for Document Chunks\n",
    "\n",
    "Now that we‚Äôve verified the embedding endpoint, we generate embeddings for **all document chunks** and store them in a Delta table for later retrieval.\n",
    "\n",
    "### Why this matters:\n",
    "- Embeddings transform text into high-dimensional vectors that capture semantic meaning.\n",
    "- These embeddings are the foundation for **Vector Search**, enabling semantic similarity queries.\n",
    "- Storing embeddings alongside metadata ensures we can later join search results back to their original documents.\n",
    "\n",
    "### Approach:\n",
    "1. Define a **Pandas UDF** `embed_udf` to call the embedding endpoint in **batches** (efficient API usage).\n",
    "2. Apply the UDF on the `chunk` column from the `docs_chunks` table.\n",
    "3. Store the results in a governed Delta table (`docs_embed`) with all chunk metadata + embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be70afe5-0148-47f6-935d-25a11715a091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Batch embedding generation function\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def embed_udf(texts: pd.Series) -> pd.Series:\n",
    "    out = []\n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        batch = texts.iloc[i:i+BATCH_SIZE].tolist()\n",
    "        response = requests.post(\n",
    "            f\"{WORKSPACE_URL}/serving-endpoints/{EMBEDDING_ENDPOINT}/invocations\",\n",
    "            headers=HEADERS,\n",
    "            data=json.dumps({\"input\": batch}),\n",
    "            timeout=REQUEST_TIMEOUT\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        out.extend([row[\"embedding\"] for row in response.json()[\"data\"]])\n",
    "    return pd.Series(out)\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "chunks_df = spark.table(f\"{FULL_CATALOG_SCHEMA}.{CHUNKS_TABLE}\")\n",
    "df_embeddings = chunks_df.withColumn(\"embedding\", embed_udf(col(\"chunk\")))\n",
    "df_embeddings.write.mode(\"overwrite\").saveAsTable(f\"{FULL_CATALOG_SCHEMA}.{EMBEDDINGS_TABLE}\")\n",
    "\n",
    "print(f\"‚úÖ Embeddings generated and saved\")\n",
    "display(df_embeddings.limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cb4fd70-88cf-470c-8d7a-b2373fb986fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 10: Initialize Vector Search Endpoint\n",
    "\n",
    "Before we can create a **Vector Search Index** to power semantic retrieval, we need a running **Vector Search Endpoint**.\n",
    "\n",
    "### Why this matters:\n",
    "- A **Vector Search Endpoint** is a managed service in Databricks that hosts and serves your indexes.\n",
    "- You can attach one or more indexes to a single endpoint.\n",
    "- Ensuring the endpoint is online is critical before syncing embeddings.\n",
    "\n",
    "### Approach:\n",
    "1. Initialize the `VectorSearchClient`.\n",
    "2. Define helper functions:\n",
    "   - `endpoint_exists()` ‚Üí Check if an endpoint already exists.\n",
    "   - `wait_for_vs_endpoint_to_be_ready()` ‚Üí Poll the endpoint until it becomes `ONLINE`.\n",
    "3. Create the endpoint if it does not already exist.\n",
    "4. Wait until the endpoint is ready before proceeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a8da3ed-9669-45e5-a4c1-42e41fed7d6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Vector Search client and utility functions\n",
    "vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "def endpoint_exists(client, endpoint_name):\n",
    "    \"\"\"Check if vector search endpoint exists\"\"\"\n",
    "    try:\n",
    "        client.get_endpoint(endpoint_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        if \"NOT_FOUND\" in str(e) or \"does not exist\" in str(e):\n",
    "            return False\n",
    "        raise e\n",
    "\n",
    "def wait_for_vs_endpoint_to_be_ready(client, endpoint_name, timeout=700, poll_interval=15):\n",
    "    \"\"\"Wait for vector search endpoint to be ready\"\"\"\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            status = client.get_endpoint(endpoint_name).get(\"endpoint_status\", {}).get(\"state\", \"\")\n",
    "            print(f\"Status: {status}\")\n",
    "            if status == \"ONLINE\":\n",
    "                print(f\"‚úÖ Vector Search endpoint '{endpoint_name}' is ready.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to get endpoint status: {e}\")\n",
    "\n",
    "        if time.time() - start_time > timeout:\n",
    "            raise TimeoutError(f\"‚ùå Timeout: Endpoint '{endpoint_name}' was not ready after {timeout} seconds.\")\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "# Create endpoint if needed\n",
    "if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):\n",
    "    print(f\"üöÄ Creating Vector Search endpoint: {VECTOR_SEARCH_ENDPOINT_NAME}\")\n",
    "    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "    time.sleep(5)\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è Vector Search endpoint '{VECTOR_SEARCH_ENDPOINT_NAME}' already exists.\")\n",
    "\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "081e22fc-5f54-46ee-a11f-b5de5aa50cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 11: Create and Sync Vector Search Index\n",
    "\n",
    "With the Vector Search Endpoint online, the next step is to create a **Delta Sync Index** that keeps the embeddings in sync with the source table.\n",
    "\n",
    "### Why this matters:\n",
    "- The **Vector Search Index** is the structure that allows for **fast similarity search**.\n",
    "- By enabling **Change Data Feed (CDF)** on the source table, the index can stay in sync as new data is added.\n",
    "- Once the index is created, we trigger an initial sync so that all embeddings are available for semantic search.\n",
    "\n",
    "### Approach:\n",
    "1. Define a utility function `index_exists()` to check if the index is already present.\n",
    "2. Enable **Change Data Feed (CDF)** on the embeddings source table.\n",
    "3. If the index doesn‚Äôt exist, create it with:\n",
    "   - Primary key ‚Üí `chunk_id`\n",
    "   - Source column for embeddings ‚Üí `chunk`\n",
    "   - Embedding model ‚Üí `databricks-bge-large-en` (configured earlier).\n",
    "4. Wait until the index is ready, then trigger a sync.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "118b6164-9188-405d-9887-c46576513f4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create delta sync index\n",
    "def index_exists(client, endpoint, index_name):\n",
    "    \"\"\"Check if vector search index exists\"\"\"\n",
    "    try:\n",
    "        client.get_index(endpoint_name=endpoint, index_name=index_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        if \"NOT_FOUND\" in str(e) or \"does not exist\" in str(e):\n",
    "            return False\n",
    "        raise e\n",
    "\n",
    "# Enable Change Data Feed on source table\n",
    "try:\n",
    "    spark.sql(f\"ALTER TABLE {SOURCE_TABLE_FULLNAME} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "    print(f\"[INFO] CDF enabled on {SOURCE_TABLE_FULLNAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Could not enable CDF: {e}\")\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, VS_INDEX_FULLNAME):\n",
    "    print(f\"[INFO] Creating delta-sync index {VS_INDEX_FULLNAME}...\")\n",
    "    vsc.create_delta_sync_index(\n",
    "        endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "        index_name=VS_INDEX_FULLNAME,\n",
    "        source_table_name=SOURCE_TABLE_FULLNAME,\n",
    "        pipeline_type=\"TRIGGERED\",\n",
    "        primary_key=\"chunk_id\",\n",
    "        embedding_source_column=\"chunk\",\n",
    "        embedding_model_endpoint_name=EMBEDDING_ENDPOINT\n",
    "    )\n",
    "else:\n",
    "    print(f\"[INFO] Index {VS_INDEX_FULLNAME} already exists.\")\n",
    "\n",
    "# Wait for index to be ready and sync\n",
    "print(f\"[INFO] Waiting for index {VS_INDEX_FULLNAME} to be ready...\")\n",
    "index_obj = vsc.get_index(endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME, index_name=VS_INDEX_FULLNAME)\n",
    "index_obj.wait_until_ready()\n",
    "index_obj.sync()\n",
    "print(f\"[‚úÖ] Index {VS_INDEX_FULLNAME} ready and synced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a8ba3d2-b657-4f5f-ae61-3f64fa5ed23a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 12: Test Vector Search with a Sample Query\n",
    "\n",
    "Now that the Vector Search Index is created and synced, we can test it by issuing a **semantic query**.\n",
    "\n",
    "### Why this matters:\n",
    "- Confirms that the index is correctly populated with embeddings.\n",
    "- Demonstrates how natural language questions can be matched against document chunks.\n",
    "- Returns the **most relevant sections** of enterprise documents for downstream use in the RAG pipeline.\n",
    "\n",
    "### Approach:\n",
    "1. Define a **test query** ‚Üí \"What is the standard warranty for product-b?\".\n",
    "2. Perform a **similarity search** using the index.\n",
    "3. Retrieve the top results with metadata (`doc_id`, `section`, `chunk`).\n",
    "4. Print results to verify that the right document passages are retrieved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe65ae98-8304-4862-87ac-08490939a67a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test vector search with sample query\n",
    "test_question = \"What is the standard warranty for product-b?\"\n",
    "\n",
    "try:\n",
    "    results = index_obj.similarity_search(\n",
    "        query_text=test_question,\n",
    "        columns=RETURN_COLUMNS,\n",
    "        num_results=SIMILARITY_SEARCH_RESULTS\n",
    "    )\n",
    "\n",
    "    cols = results.get(\"result\", {}).get(\"columns\", RETURN_COLUMNS)\n",
    "    rows = results.get(\"result\", {}).get(\"data_array\", [])\n",
    "\n",
    "    print(f\"üîç Query: {test_question}\")\n",
    "    print(f\"üìÑ Found {len(rows)} results:\")\n",
    "\n",
    "    for i, row in enumerate(rows, start=1):\n",
    "        row_map = dict(zip(cols, row))\n",
    "        print(f\"\\nüìπ Result {i}\")\n",
    "        print(f\"Doc ID: {row_map.get('doc_id')}\")\n",
    "        print(f\"Section: {row_map.get('section')}\")\n",
    "        print(f\"Text: {row_map.get('chunk')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Vector search test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f9129e8-a083-4dda-8c20-99f99c9068c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 13: Implement Circuit Breaker for Production Resilience\n",
    "\n",
    "In enterprise systems, it‚Äôs not enough to just deploy a model ‚Äî we also need **resilience** against failures.\n",
    "A **Circuit Breaker** pattern helps prevent cascading failures by temporarily blocking requests when error rates exceed a threshold.\n",
    "\n",
    "### Why this matters:\n",
    "- Protects downstream systems (e.g., Vector Search, LLM endpoints) from being overwhelmed.\n",
    "- Automatically recovers after a cooldown period.\n",
    "- Provides metrics for observability and monitoring.\n",
    "- Ensures production-grade **fault tolerance**.\n",
    "\n",
    "### Circuit Breaker States:\n",
    "- **CLOSED** ‚Üí All requests are allowed (normal operation).\n",
    "- **OPEN** ‚Üí Requests are blocked after repeated failures.\n",
    "- **HALF_OPEN** ‚Üí Trial requests are allowed after cooldown; if they succeed, the breaker closes again.\n",
    "\n",
    "### Features of `AdvancedCircuitBreaker`:\n",
    "- Configurable thresholds: failure %, recovery timeout, success threshold.\n",
    "- Sliding request window (`deque`) to calculate failure rates.\n",
    "- Thread-safe with locks for concurrent requests.\n",
    "- Metrics collected:\n",
    "  - Total requests\n",
    "  - Successful / failed requests\n",
    "  - Circuit trips (how many times breaker opened)\n",
    "  - Current failure rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d2118cb-00ea-4f66-ae31-7cdde28e41b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Advanced RAG Model with Circuit Breaker\n",
    "class CircuitState(Enum):\n",
    "    CLOSED = \"CLOSED\"\n",
    "    OPEN = \"OPEN\"\n",
    "    HALF_OPEN = \"HALF_OPEN\"\n",
    "\n",
    "class AdvancedCircuitBreaker:\n",
    "    \"\"\"Enterprise-grade circuit breaker for RAG system\"\"\"\n",
    "\n",
    "    def __init__(self, failure_threshold=FAILURE_THRESHOLD, recovery_timeout=RECOVERY_TIMEOUT,\n",
    "                 success_threshold=SUCCESS_THRESHOLD, window_size=WINDOW_SIZE, min_requests=MIN_REQUESTS):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.recovery_timeout = recovery_timeout\n",
    "        self.success_threshold = success_threshold\n",
    "        self.window_size = window_size\n",
    "        self.min_requests = min_requests\n",
    "\n",
    "        self.state = CircuitState.CLOSED\n",
    "        self.failure_count = 0\n",
    "        self.success_count = 0\n",
    "        self.last_failure_time = 0\n",
    "        self.next_attempt_time = 0\n",
    "        self.request_window = deque(maxlen=window_size)\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "        self.metrics = {\n",
    "            \"total_requests\": 0,\n",
    "            \"successful_requests\": 0,\n",
    "            \"failed_requests\": 0,\n",
    "            \"circuit_trips\": 0,\n",
    "            \"current_failure_rate\": 0.0\n",
    "        }\n",
    "\n",
    "    def call(self, func, *args, **kwargs):\n",
    "        \"\"\"Execute function with circuit breaker protection\"\"\"\n",
    "        with self.lock:\n",
    "            self.metrics[\"total_requests\"] += 1\n",
    "\n",
    "            if not self._should_allow_request():\n",
    "                self.metrics[\"failed_requests\"] += 1\n",
    "                raise Exception(f\"Circuit breaker is {self.state.value}\")\n",
    "\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                self._record_success()\n",
    "                self.metrics[\"successful_requests\"] += 1\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                self._record_failure()\n",
    "                self.metrics[\"failed_requests\"] += 1\n",
    "                raise e\n",
    "\n",
    "    def _should_allow_request(self):\n",
    "        current_time = time.time()\n",
    "        if self.state == CircuitState.CLOSED:\n",
    "            return True\n",
    "        elif self.state == CircuitState.OPEN:\n",
    "            if current_time >= self.next_attempt_time:\n",
    "                self.state = CircuitState.HALF_OPEN\n",
    "                self.success_count = 0\n",
    "                return True\n",
    "            return False\n",
    "        elif self.state == CircuitState.HALF_OPEN:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _record_success(self):\n",
    "        self.request_window.append({\"timestamp\": time.time(), \"success\": True})\n",
    "        if self.state == CircuitState.HALF_OPEN:\n",
    "            self.success_count += 1\n",
    "            if self.success_count >= self.success_threshold:\n",
    "                self.state = CircuitState.CLOSED\n",
    "                self.failure_count = 0\n",
    "\n",
    "    def _record_failure(self):\n",
    "        current_time = time.time()\n",
    "        self.request_window.append({\"timestamp\": current_time, \"success\": False})\n",
    "        self.last_failure_time = current_time\n",
    "\n",
    "        if self.state == CircuitState.HALF_OPEN:\n",
    "            self.state = CircuitState.OPEN\n",
    "            self.next_attempt_time = current_time + self.recovery_timeout\n",
    "            self.metrics[\"circuit_trips\"] += 1\n",
    "        elif self.state == CircuitState.CLOSED:\n",
    "            failure_rate = self._calculate_failure_rate()\n",
    "            if failure_rate >= (self.failure_threshold / 100.0) and len(self.request_window) >= self.min_requests:\n",
    "                self.state = CircuitState.OPEN\n",
    "                self.next_attempt_time = current_time + self.recovery_timeout\n",
    "                self.metrics[\"circuit_trips\"] += 1\n",
    "\n",
    "    def _calculate_failure_rate(self):\n",
    "        if not self.request_window:\n",
    "            return 0.0\n",
    "        failures = sum(1 for req in self.request_window if not req[\"success\"])\n",
    "        failure_rate = failures / len(self.request_window)\n",
    "        self.metrics[\"current_failure_rate\"] = failure_rate * 100\n",
    "        return failure_rate\n",
    "\n",
    "print(\"‚úÖ Advanced Circuit Breaker class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cd4c84c-1355-4d84-96e4-1fd86ca233f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 14: Implement Advanced Circuit Breaker\n",
    "\n",
    "In production systems, simply deploying a RAG pipeline is not enough ‚Äî we need to ensure **resilience** against endpoint outages, slow responses, or cascading failures.\n",
    "\n",
    "The **Circuit Breaker pattern** is a fault tolerance mechanism that:\n",
    "- Prevents overwhelming downstream services (e.g., Vector Search or LLM endpoints).\n",
    "- Stops repeated failing requests by \"opening the circuit\".\n",
    "- Automatically retries after a recovery timeout, moving to a **HALF_OPEN** state.\n",
    "- Closes the circuit again if requests succeed consistently.\n",
    "\n",
    "### Circuit Breaker States:\n",
    "- **CLOSED** ‚Üí All requests allowed (normal operation).\n",
    "- **OPEN** ‚Üí Requests blocked due to high failure rate.\n",
    "- **HALF_OPEN** ‚Üí Allows limited test requests to check if the system has recovered.\n",
    "\n",
    "### Features of `AdvancedCircuitBreaker`:\n",
    "- **Failure threshold** (% of failed requests before tripping the circuit).\n",
    "- **Recovery timeout** (time before retrying after a trip).\n",
    "- **Success threshold** (number of successful requests to close circuit again).\n",
    "- **Sliding request window** to calculate real-time failure rates.\n",
    "- **Thread-safe** for concurrent requests.\n",
    "- **Metrics** collected for monitoring:\n",
    "  - Total requests\n",
    "  - Successful / failed requests\n",
    "  - Circuit trips\n",
    "  - Current failure rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "194eb8bf-49ca-4147-8653-5a04b297391b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enterprise RAG Model\n",
    "class EnterpriseRAGModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Production-ready RAG model with advanced features\"\"\"\n",
    "\n",
    "    def load_context(self, context):\n",
    "        with open(context.artifacts[\"config\"], \"r\") as f:\n",
    "            self.config = json.load(f)\n",
    "\n",
    "        # Initialize circuit breaker\n",
    "        self.circuit_breaker = AdvancedCircuitBreaker()\n",
    "\n",
    "        # Initialize vector search client\n",
    "        self.vsc = VectorSearchClient(disable_notice=True)\n",
    "        self.index = self.vsc.get_index(\n",
    "            endpoint_name=self.config[\"vector_search_endpoint\"],\n",
    "            index_name=self.config[\"vector_index_name\"]\n",
    "        )\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        outputs = []\n",
    "        for _, row in model_input.iterrows():\n",
    "            question = row[\"question\"]\n",
    "\n",
    "            try:\n",
    "                # Use circuit breaker for vector search\n",
    "                search_results = self.circuit_breaker.call(\n",
    "                    self._perform_search, question\n",
    "                )\n",
    "\n",
    "                # Generate answer based on retrieved context\n",
    "                answer = self._generate_answer(question, search_results)\n",
    "\n",
    "                outputs.append({\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"retrieved\": search_results,\n",
    "                    \"circuit_breaker_state\": self.circuit_breaker.state.value\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                # Fallback response\n",
    "                outputs.append({\n",
    "                    \"question\": question,\n",
    "                    \"answer\": f\"I apologize, but I'm currently unable to process your request due to technical issues: {str(e)}. Please try again later.\",\n",
    "                    \"retrieved\": [],\n",
    "                    \"circuit_breaker_state\": self.circuit_breaker.state.value,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(outputs)\n",
    "\n",
    "    def _perform_search(self, question):\n",
    "        \"\"\"Perform vector search with error handling\"\"\"\n",
    "        results = self.index.similarity_search(\n",
    "            query_text=question,\n",
    "            columns=self.config[\"return_columns\"],\n",
    "            num_results=self.config[\"num_results\"]\n",
    "        )\n",
    "\n",
    "        cols = results.get(\"result\", {}).get(\"columns\", [])\n",
    "        rows = results.get(\"result\", {}).get(\"data_array\", [])\n",
    "\n",
    "        return [{\"chunk_text\": dict(zip(cols, row)).get(\"chunk\", \"\"),\n",
    "                \"source\": dict(zip(cols, row)).get(\"doc_id\", \"\")} for row in rows]\n",
    "\n",
    "    def _generate_answer(self, question, search_results):\n",
    "        \"\"\"Generate answer based on retrieved context\"\"\"\n",
    "        if not search_results:\n",
    "            return \"I couldn't find relevant information to answer your question.\"\n",
    "\n",
    "        # Simple answer generation based on retrieved context\n",
    "        context = \" \".join([result[\"chunk_text\"] for result in search_results[:3]])\n",
    "\n",
    "        # Basic keyword matching for demo purposes\n",
    "        if \"warranty\" in question.lower():\n",
    "            for result in search_results:\n",
    "                if \"warranty\" in result[\"chunk_text\"].lower():\n",
    "                    return f\"Based on the documentation: {result['chunk_text']}\"\n",
    "\n",
    "        return f\"Based on the available information: {context[:200]}...\"\n",
    "\n",
    "print(\"‚úÖ Enterprise RAG Model class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4961290c-1ba5-476f-af49-05225ef34eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 15: Package and Register the RAG Model\n",
    "\n",
    "Now that we have document embeddings and a working Vector Search index, we need to **package the RAG pipeline as an MLflow PyFunc model**.\n",
    "\n",
    "### Why this matters:\n",
    "- MLflow packaging makes the model reproducible and deployable across environments.\n",
    "- Unity Catalog registration ensures **governance**, **versioning**, and **traceability**.\n",
    "- PyFunc models support flexible APIs (`predict`) for integration with Serving endpoints.\n",
    "\n",
    "### Fixes applied in `SimpleRAGModel`:\n",
    "- **Lazy initialization of VectorSearchClient** inside `_get_vector_search_index()`\n",
    "  ‚Üí avoids serialization errors when logging the model.\n",
    "- **Robust error handling** with fallback responses.\n",
    "- **Keyword-based answer generation** for warranty, retention, access control, and maintenance queries.\n",
    "- **Config artifact** stored in JSON so parameters are externalized (endpoint, index, return columns).\n",
    "\n",
    "### Registration Process:\n",
    "1. Define model configuration and save to `config.json`.\n",
    "2. Create example input/output to define MLflow **signature**.\n",
    "3. Log and register the model in MLflow + Unity Catalog.\n",
    "4. Confirm successful registration with model name and version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "686add12-5f1e-4de1-bcf3-e757eb0d3880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fixed RAG Model that can be serialized\n",
    "class SimpleRAGModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Simplified RAG model that avoids serialization issues\"\"\"\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"Load configuration - don't initialize complex objects here\"\"\"\n",
    "        with open(context.artifacts[\"config\"], \"r\") as f:\n",
    "            self.config = json.load(f)\n",
    "        # Don't initialize VectorSearchClient here - causes serialization issues\n",
    "        self.vsc = None\n",
    "        self.index = None\n",
    "\n",
    "    def _get_vector_search_index(self):\n",
    "        \"\"\"Lazy initialization of vector search client\"\"\"\n",
    "        if self.vsc is None:\n",
    "            from databricks.vector_search.client import VectorSearchClient\n",
    "            self.vsc = VectorSearchClient(disable_notice=True)\n",
    "            self.index = self.vsc.get_index(\n",
    "                endpoint_name=self.config[\"vector_search_endpoint\"],\n",
    "                index_name=self.config[\"vector_index_name\"]\n",
    "            )\n",
    "        return self.index\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"Process questions and return answers\"\"\"\n",
    "        outputs = []\n",
    "\n",
    "        for _, row in model_input.iterrows():\n",
    "            question = row[\"question\"]\n",
    "\n",
    "            try:\n",
    "                # Get vector search index (lazy initialization)\n",
    "                index = self._get_vector_search_index()\n",
    "\n",
    "                # Perform vector search\n",
    "                search_results = self._perform_search(index, question)\n",
    "\n",
    "                # Generate answer\n",
    "                answer = self._generate_answer(question, search_results)\n",
    "\n",
    "                outputs.append({\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"retrieved\": search_results\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                # Fallback response\n",
    "                outputs.append({\n",
    "                    \"question\": question,\n",
    "                    \"answer\": f\"I apologize, but I'm currently unable to process your request: {str(e)}. Please try again later.\",\n",
    "                    \"retrieved\": [],\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(outputs)\n",
    "\n",
    "    def _perform_search(self, index, question):\n",
    "        \"\"\"Perform vector search\"\"\"\n",
    "        try:\n",
    "            results = index.similarity_search(\n",
    "                query_text=question,\n",
    "                columns=self.config[\"return_columns\"],\n",
    "                num_results=self.config[\"num_results\"]\n",
    "            )\n",
    "\n",
    "            cols = results.get(\"result\", {}).get(\"columns\", [])\n",
    "            rows = results.get(\"result\", {}).get(\"data_array\", [])\n",
    "\n",
    "            return [{\n",
    "                \"chunk_text\": dict(zip(cols, row)).get(\"chunk\", \"\"),\n",
    "                \"source\": dict(zip(cols, row)).get(\"doc_id\", \"\")\n",
    "            } for row in rows]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Vector search failed: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _generate_answer(self, question, search_results):\n",
    "        \"\"\"Generate answer based on retrieved context\"\"\"\n",
    "        if not search_results:\n",
    "            return \"I couldn't find relevant information to answer your question.\"\n",
    "\n",
    "        # Enhanced answer generation with keyword matching\n",
    "        question_lower = question.lower()\n",
    "\n",
    "        # Check for warranty questions\n",
    "        if \"warranty\" in question_lower:\n",
    "            for result in search_results:\n",
    "                if \"warranty\" in result[\"chunk_text\"].lower():\n",
    "                    return f\"Based on the documentation: {result['chunk_text']}\"\n",
    "\n",
    "        # Check for data retention questions\n",
    "        if \"retention\" in question_lower or \"data\" in question_lower:\n",
    "            for result in search_results:\n",
    "                if \"retention\" in result[\"chunk_text\"].lower() or \"days\" in result[\"chunk_text\"].lower():\n",
    "                    return f\"Based on the policy: {result['chunk_text']}\"\n",
    "\n",
    "        # Check for access control questions\n",
    "        if \"access\" in question_lower or \"control\" in question_lower:\n",
    "            for result in search_results:\n",
    "                if \"access\" in result[\"chunk_text\"].lower() or \"MFA\" in result[\"chunk_text\"]:\n",
    "                    return f\"Based on the access control policy: {result['chunk_text']}\"\n",
    "\n",
    "        # Check for maintenance questions\n",
    "        if \"maintenance\" in question_lower:\n",
    "            for result in search_results:\n",
    "                if \"maintenance\" in result[\"chunk_text\"].lower() or \"inspection\" in result[\"chunk_text\"].lower():\n",
    "                    return f\"Based on the maintenance guide: {result['chunk_text']}\"\n",
    "\n",
    "        # Default response with context\n",
    "        context = \" \".join([result[\"chunk_text\"] for result in search_results[:2]])\n",
    "        return f\"Based on the available information: {context[:300]}...\"\n",
    "\n",
    "print(\"‚úÖ Fixed RAG Model class defined\")\n",
    "\n",
    "\n",
    "\n",
    "# Fixed model registration\n",
    "config = {\n",
    "    \"vector_search_endpoint\": VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    \"vector_index_name\": VS_INDEX_FULLNAME,\n",
    "    \"return_columns\": RETURN_COLUMNS,\n",
    "    \"num_results\": SIMILARITY_SEARCH_RESULTS\n",
    "}\n",
    "\n",
    "with tempfile.TemporaryDirectory() as td:\n",
    "    cfg_path = os.path.join(td, \"config.json\")\n",
    "    with open(cfg_path, \"w\") as f:\n",
    "        json.dump(config, f)\n",
    "\n",
    "    # Define model signature with proper input example\n",
    "    example_input = pd.DataFrame([{\"question\": \"What are the warranty terms?\"}])\n",
    "    example_output = pd.DataFrame([{\n",
    "        \"question\": \"What are the warranty terms?\",\n",
    "        \"answer\": \"Based on the documentation: Product-B includes a standard warranty of 12 months covering manufacturing defects.\",\n",
    "        \"retrieved\": [{\"chunk_text\": \"Sample context\", \"source\": \"doc_001\"}]\n",
    "    }])\n",
    "\n",
    "    signature = infer_signature(example_input, example_output)\n",
    "\n",
    "    # Log and register model with all required parameters\n",
    "    with mlflow.start_run(run_name=\"fixed_rag_model\") as run:\n",
    "        mlflow.pyfunc.log_model(\n",
    "            name=\"fixed_rag\",\n",
    "            python_model=SimpleRAGModel(),  # Use the fixed model class\n",
    "            artifacts={\"config\": cfg_path},\n",
    "            signature=signature,\n",
    "            input_example=example_input,  # This fixes the warning\n",
    "            registered_model_name=MODEL_NAME,\n",
    "            pip_requirements=[\n",
    "                \"databricks-vectorsearch\",\n",
    "                \"pandas>=1.3.0\",\n",
    "                \"numpy>=1.21.0\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "print(f\"‚úÖ Model registered successfully: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9a2df6d-e588-43df-ab30-d351c377722f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 16: Deploy RAG Model to Databricks Serving\n",
    "\n",
    "With the RAG model registered in MLflow, the next step is to **deploy it as a REST-serving endpoint** in Databricks.\n",
    "\n",
    "### Why this matters:\n",
    "- Exposes the RAG pipeline as a **scalable API** for enterprise applications.\n",
    "- Allows employees and downstream systems to query the model using standard HTTP requests.\n",
    "- Supports **autoscaling and scale-to-zero**, optimizing cost efficiency.\n",
    "- Ensures deployment is governed and version-controlled via Unity Catalog.\n",
    "\n",
    "### Deployment Process:\n",
    "1. **Get latest model version** from Unity Catalog/MLflow Registry.\n",
    "2. Define the **endpoint configuration**:\n",
    "   - Model name and version.\n",
    "   - Workload size (e.g., Small).\n",
    "   - Environment variables (`DATABRICKS_HOST`, `DATABRICKS_TOKEN`) for runtime access.\n",
    "   - 100% traffic routed to this version.\n",
    "3. Check if the serving endpoint already exists:\n",
    "   - If yes ‚Üí update configuration.\n",
    "   - If no ‚Üí create a new endpoint.\n",
    "4. Monitor deployment in the Databricks UI under **Serving**.\n",
    "\n",
    "This process may take several minutes as the model container spins up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60f7526a-622f-4958-a575-e883ed3e3255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Deploy serving endpoint\n",
    "def deploy_serving_endpoint():\n",
    "    \"\"\"Deploy or update serving endpoint\"\"\"\n",
    "\n",
    "    # Get latest model version\n",
    "    client = MlflowClient()\n",
    "    versions = client.search_model_versions(f\"name='{MODEL_NAME}'\")\n",
    "    latest_version = max(versions, key=lambda v: int(v.version)).version\n",
    "\n",
    "    # Endpoint configuration\n",
    "    endpoint_config = {\n",
    "        \"served_models\": [{\n",
    "            \"name\": \"enterprise-rag-model\",\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"model_version\": latest_version,\n",
    "            \"workload_size\": \"Small\",\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"environment_vars\": {\n",
    "                \"DATABRICKS_HOST\": WORKSPACE_URL,\n",
    "                \"DATABRICKS_TOKEN\": TOKEN\n",
    "            }\n",
    "        }],\n",
    "        \"traffic_config\": {\n",
    "            \"routes\": [{\n",
    "                \"served_model_name\": \"enterprise-rag-model\",\n",
    "                \"traffic_percentage\": 100\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Check if endpoint exists\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{WORKSPACE_URL}/api/2.0/serving-endpoints/{SERVING_ENDPOINT_NAME}\",\n",
    "            headers=HEADERS\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Update existing endpoint\n",
    "            print(f\"üîÑ Updating serving endpoint: {SERVING_ENDPOINT_NAME}\")\n",
    "            response = requests.put(\n",
    "                f\"{WORKSPACE_URL}/api/2.0/serving-endpoints/{SERVING_ENDPOINT_NAME}/config\",\n",
    "                headers=HEADERS,\n",
    "                data=json.dumps(endpoint_config)\n",
    "            )\n",
    "        else:\n",
    "            # Create new endpoint\n",
    "            print(f\"üöÄ Creating serving endpoint: {SERVING_ENDPOINT_NAME}\")\n",
    "            endpoint_payload = {\n",
    "                \"name\": SERVING_ENDPOINT_NAME,\n",
    "                \"config\": endpoint_config\n",
    "            }\n",
    "            response = requests.post(\n",
    "                f\"{WORKSPACE_URL}/api/2.0/serving-endpoints\",\n",
    "                headers=HEADERS,\n",
    "                data=json.dumps(endpoint_payload)\n",
    "            )\n",
    "\n",
    "        if response.status_code in [200, 201]:\n",
    "            print(f\"‚úÖ Endpoint deployment initiated successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Deployment failed: {response.status_code} - {response.text}\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during deployment: {e}\")\n",
    "        return False\n",
    "\n",
    "# Deploy the endpoint\n",
    "deployment_success = deploy_serving_endpoint()\n",
    "\n",
    "if deployment_success:\n",
    "    print(f\"\\n‚è≥ Waiting for endpoint to be ready (this may take several minutes)...\")\n",
    "    print(f\"üìç You can monitor the deployment in the Databricks UI under 'Serving'\")\n",
    "else:\n",
    "    print(f\"‚ùå Deployment failed. Please check the configuration and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e2d6914-3ab0-424f-bc94-235d193fa888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 17: Deploy Serving Endpoint\n",
    "\n",
    "With the RAG model registered in MLflow, the next step is to **deploy it as a Databricks Model Serving Endpoint**.\n",
    "\n",
    "### Why this matters:\n",
    "- Makes the RAG pipeline available as a **REST API**.\n",
    "- Allows business users and applications to query the system with natural language questions.\n",
    "- Supports **autoscaling and scale-to-zero** for cost efficiency.\n",
    "- Ensures deployment is managed under **Unity Catalog governance**.\n",
    "\n",
    "### Deployment Process:\n",
    "1. Retrieve the **latest model version** from MLflow.\n",
    "2. Define the **endpoint configuration**:\n",
    "   - Model name and version.\n",
    "   - Workload size (e.g., `Small`).\n",
    "   - Environment variables (`DATABRICKS_HOST`, `DATABRICKS_TOKEN`).\n",
    "   - Traffic policy (100% routed to this version).\n",
    "3. Check if the endpoint already exists:\n",
    "   - If **yes** ‚Üí update the configuration.\n",
    "   - If **no** ‚Üí create a new endpoint.\n",
    "4. Wait for the deployment to complete (can take several minutes).\n",
    "5. Monitor status in the **Databricks UI ‚Üí Serving**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b081f6b-fadb-43dc-b89e-2d3951718265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Comprehensive RAG system testing\n",
    "def test_rag_system():\n",
    "    \"\"\"Test the complete RAG system\"\"\"\n",
    "\n",
    "    test_queries = [\n",
    "        \"What are the warranty terms for product-b?\",\n",
    "        \"What is the data retention policy?\",\n",
    "        \"What are the access control requirements?\",\n",
    "        \"How often should maintenance be performed?\",\n",
    "        \"What are the storage policy requirements?\"\n",
    "    ]\n",
    "\n",
    "    print(\"üß™ Testing RAG System\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nüìù Test Query {i}: {query}\")\n",
    "\n",
    "        try:\n",
    "            # Test endpoint (when ready)\n",
    "            payload = {\"dataframe_records\": [{\"question\": query}]}\n",
    "\n",
    "            start_time = time.time()\n",
    "            response = requests.post(\n",
    "                f\"{WORKSPACE_URL}/serving-endpoints/{SERVING_ENDPOINT_NAME}/invocations\",\n",
    "                headers=HEADERS,\n",
    "                data=json.dumps(payload),\n",
    "                timeout=REQUEST_TIMEOUT\n",
    "            )\n",
    "            response_time = time.time() - start_time\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                answer = result[\"predictions\"][0][\"answer\"]\n",
    "                retrieved_docs = result[\"predictions\"][0][\"retrieved\"]\n",
    "\n",
    "                print(f\"   ‚úÖ SUCCESS! Response time: {response_time:.2f}s\")\n",
    "                print(f\"   üìÑ Answer: {answer[:100]}...\")\n",
    "                print(f\"   üîç Retrieved {len(retrieved_docs)} documents\")\n",
    "\n",
    "                results.append({\n",
    "                    \"query\": query,\n",
    "                    \"status\": \"success\",\n",
    "                    \"response_time\": response_time,\n",
    "                    \"answer\": answer\n",
    "                })\n",
    "            else:\n",
    "                print(f\"   ‚ùå Error: {response.status_code} - {response.text}\")\n",
    "                results.append({\n",
    "                    \"query\": query,\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": response.text\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Exception: {str(e)}\")\n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"status\": \"exception\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "\n",
    "    # Summary\n",
    "    successful_tests = [r for r in results if r[\"status\"] == \"success\"]\n",
    "    print(f\"\\nüìä TEST SUMMARY\")\n",
    "    print(f\"‚úÖ Successful queries: {len(successful_tests)}/{len(test_queries)}\")\n",
    "\n",
    "    if successful_tests:\n",
    "        avg_response_time = np.mean([r[\"response_time\"] for r in successful_tests])\n",
    "        print(f\"‚ö° Average response time: {avg_response_time:.2f}s\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Note: Uncomment the line below to run tests when endpoint is ready\n",
    "# test_results = test_rag_system()\n",
    "\n",
    "print(\"\\nüéâ RAG SYSTEM DEPLOYMENT COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Configuration centralized\")\n",
    "print(\"‚úÖ Database and tables created\")\n",
    "print(\"‚úÖ Document chunking implemented\")\n",
    "print(\"‚úÖ Embeddings generated\")\n",
    "print(\"‚úÖ Vector search index created\")\n",
    "print(\"‚úÖ Advanced RAG model with circuit breaker\")\n",
    "print(\"‚úÖ Model registered in MLflow\")\n",
    "print(\"‚úÖ Serving endpoint deployed\")\n",
    "print(\"‚úÖ Comprehensive testing framework\")\n",
    "print(\"\\nüöÄ Your enterprise RAG system is ready for production!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7693fe7-479c-42c2-9465-20dab725d5d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 18: Apply Identity-Based Access Control to Serving Endpoint\n",
    "\n",
    "In enterprise environments, **not everyone should have access to invoke or manage model serving endpoints**. Databricks provides **identity-based access control** through permissions that can be applied to serving endpoints.\n",
    "\n",
    "### Why this matters:\n",
    "- **Security**: Restrict who can invoke the endpoint to authorized users/groups only\n",
    "- **Governance**: Control who can manage, update, or delete the endpoint\n",
    "- **Compliance**: Meet regulatory requirements for access control and audit trails\n",
    "- **Cost Control**: Prevent unauthorized usage that could incur costs\n",
    "\n",
    "### Permission Levels:\n",
    "1. **CAN_QUERY** ‚Üí Can invoke the endpoint (send requests)\n",
    "2. **CAN_MANAGE** ‚Üí Can update endpoint configuration, view metrics\n",
    "3. **CAN_MANAGE_RUN** ‚Üí Can manage endpoint lifecycle (start, stop, delete)\n",
    "\n",
    "### Access Control Strategies:\n",
    "- **User-based**: Grant permissions to specific users by email\n",
    "- **Group-based**: Grant permissions to groups (e.g., \"data-scientists\", \"ml-engineers\")\n",
    "- **Service Principal**: Grant permissions to automated systems/applications\n",
    "\n",
    "### Implementation:\n",
    "We'll use the Databricks REST API to:\n",
    "1. Get current endpoint permissions\n",
    "2. Add specific users/groups with appropriate permission levels\n",
    "3. Verify the permissions are applied correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5742933-76fa-4afe-802d-fe24598f2290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identity-Based Access Control Functions\n",
    "def get_endpoint_permissions(endpoint_name):\n",
    "    \"\"\"Get current permissions for a serving endpoint\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{WORKSPACE_URL}/api/2.0/permissions/serving-endpoints/{endpoint_name}\",\n",
    "            headers=HEADERS\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            permissions = response.json()\n",
    "            print(f\"‚úÖ Current permissions for endpoint '{endpoint_name}':\")\n",
    "            print(json.dumps(permissions, indent=2))\n",
    "            return permissions\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to get permissions: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def grant_endpoint_access(endpoint_name, user_email=None, group_name=None, permission_level=\"CAN_QUERY\"):\n",
    "    \"\"\"\n",
    "    Grant access to a serving endpoint for a user or group\n",
    "\n",
    "    Args:\n",
    "        endpoint_name: Name of the serving endpoint\n",
    "        user_email: Email of user to grant access (optional)\n",
    "        group_name: Name of group to grant access (optional)\n",
    "        permission_level: One of \"CAN_QUERY\", \"CAN_MANAGE\", \"CAN_MANAGE_RUN\"\n",
    "    \"\"\"\n",
    "    if not user_email and not group_name:\n",
    "        print(\"‚ùå Must specify either user_email or group_name\")\n",
    "        return False\n",
    "\n",
    "    # Build access control list entry\n",
    "    acl_entry = {\n",
    "        \"permission_level\": permission_level\n",
    "    }\n",
    "\n",
    "    if user_email:\n",
    "        acl_entry[\"user_name\"] = user_email\n",
    "    if group_name:\n",
    "        acl_entry[\"group_name\"] = group_name\n",
    "\n",
    "    # Prepare payload\n",
    "    payload = {\n",
    "        \"access_control_list\": [acl_entry]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.patch(\n",
    "            f\"{WORKSPACE_URL}/api/2.0/permissions/serving-endpoints/{endpoint_name}\",\n",
    "            headers=HEADERS,\n",
    "            data=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            target = user_email if user_email else group_name\n",
    "            print(f\"‚úÖ Granted {permission_level} to {target} on endpoint '{endpoint_name}'\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to grant access: {response.status_code} - {response.text}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def revoke_endpoint_access(endpoint_name, user_email=None, group_name=None):\n",
    "    \"\"\"\n",
    "    Revoke access to a serving endpoint for a user or group\n",
    "\n",
    "    Args:\n",
    "        endpoint_name: Name of the serving endpoint\n",
    "        user_email: Email of user to revoke access (optional)\n",
    "        group_name: Name of group to revoke access (optional)\n",
    "    \"\"\"\n",
    "    if not user_email and not group_name:\n",
    "        print(\"‚ùå Must specify either user_email or group_name\")\n",
    "        return False\n",
    "\n",
    "    # Build access control list entry with no permissions\n",
    "    acl_entry = {\n",
    "        \"permission_level\": \"CAN_VIEW\"  # Minimum permission\n",
    "    }\n",
    "\n",
    "    if user_email:\n",
    "        acl_entry[\"user_name\"] = user_email\n",
    "    if group_name:\n",
    "        acl_entry[\"group_name\"] = group_name\n",
    "\n",
    "    # To revoke, we set permission to the lowest level or remove entirely\n",
    "    # For complete removal, use DELETE method\n",
    "    try:\n",
    "        # Get current permissions first\n",
    "        current_perms = get_endpoint_permissions(endpoint_name)\n",
    "        if not current_perms:\n",
    "            return False\n",
    "\n",
    "        # Filter out the user/group we want to remove\n",
    "        new_acl = []\n",
    "        for acl in current_perms.get(\"access_control_list\", []):\n",
    "            if user_email and acl.get(\"user_name\") == user_email:\n",
    "                continue\n",
    "            if group_name and acl.get(\"group_name\") == group_name:\n",
    "                continue\n",
    "            new_acl.append(acl)\n",
    "\n",
    "        # Update with filtered list\n",
    "        payload = {\"access_control_list\": new_acl}\n",
    "\n",
    "        response = requests.put(\n",
    "            f\"{WORKSPACE_URL}/api/2.0/permissions/serving-endpoints/{endpoint_name}\",\n",
    "            headers=HEADERS,\n",
    "            data=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            target = user_email if user_email else group_name\n",
    "            print(f\"‚úÖ Revoked access for {target} on endpoint '{endpoint_name}'\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to revoke access: {response.status_code} - {response.text}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (commented out - replace with actual user/group names):\n",
    "#\n",
    "# # Grant query access to a specific user\n",
    "# grant_endpoint_access(\n",
    "#     endpoint_name=SERVING_ENDPOINT_NAME,\n",
    "#     user_email=\"data.scientist@company.com\",\n",
    "#     permission_level=\"CAN_QUERY\"\n",
    "# )\n",
    "#\n",
    "# # Grant management access to ML engineers group\n",
    "# grant_endpoint_access(\n",
    "#     endpoint_name=SERVING_ENDPOINT_NAME,\n",
    "#     group_name=\"ml-engineers\",\n",
    "#     permission_level=\"CAN_MANAGE\"\n",
    "# )\n",
    "#\n",
    "# # View current permissions\n",
    "# get_endpoint_permissions(SERVING_ENDPOINT_NAME)\n",
    "#\n",
    "# # Revoke access\n",
    "# revoke_endpoint_access(\n",
    "#     endpoint_name=SERVING_ENDPOINT_NAME,\n",
    "#     user_email=\"former.employee@company.com\"\n",
    "# )\n",
    "\n",
    "print(\"‚úÖ Identity-based access control functions defined\")\n",
    "print(\"\\nüìã Available permission levels:\")\n",
    "print(\"   - CAN_QUERY: Can invoke the endpoint\")\n",
    "print(\"   - CAN_MANAGE: Can update configuration and view metrics\")\n",
    "print(\"   - CAN_MANAGE_RUN: Can manage endpoint lifecycle\")\n",
    "print(\"\\nüí° Tip: Use group-based permissions for easier management at scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6075b2f-958f-4741-a63d-63d65e909ff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 19: End-to-End REST API Testing\n",
    "\n",
    "Now that the serving endpoint is deployed, we need to **test it end-to-end** using REST API calls to ensure it's working correctly in production.\n",
    "\n",
    "### Why this matters:\n",
    "- Validates that the entire pipeline (Vector Search ‚Üí Retrieval ‚Üí Answer Generation) works correctly.\n",
    "- Tests the endpoint's **response time** and **accuracy**.\n",
    "- Ensures the API contract matches expectations for downstream consumers.\n",
    "- Provides baseline metrics for monitoring and alerting.\n",
    "\n",
    "### Testing Approach:\n",
    "1. Define a set of **test queries** covering different document types and topics.\n",
    "2. Send HTTP POST requests to the serving endpoint.\n",
    "3. Measure **response times** and validate **answer quality**.\n",
    "4. Collect metrics: success rate, average latency, error types.\n",
    "5. Generate a **test summary report**.\n",
    "\n",
    "### Test Queries:\n",
    "- Warranty questions (Product Spec)\n",
    "- Data retention policies (Policy Handbook)\n",
    "- Access control requirements (Compliance Manual)\n",
    "- Maintenance procedures (Product Spec)\n",
    "- Storage policies (Compliance Manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9afc745-0c78-4156-9dad-83d65b156707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# End-to-End REST API Testing\n",
    "def test_endpoint_rest_api():\n",
    "    \"\"\"Comprehensive REST API testing for the deployed endpoint\"\"\"\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"query\": \"What are the warranty terms for product-b?\",\n",
    "            \"expected_keywords\": [\"warranty\", \"12 months\", \"manufacturing defects\"],\n",
    "            \"category\": \"Product Spec\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"What is the data retention policy?\",\n",
    "            \"expected_keywords\": [\"retention\", \"180 days\", \"730 days\"],\n",
    "            \"category\": \"Policy Handbook\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"What are the access control requirements?\",\n",
    "            \"expected_keywords\": [\"MFA\", \"access\", \"logged\"],\n",
    "            \"category\": \"Compliance Manual\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"How often should maintenance be performed?\",\n",
    "            \"expected_keywords\": [\"quarterly\", \"inspection\", \"filters\"],\n",
    "            \"category\": \"Product Spec\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"What are the storage policy requirements?\",\n",
    "            \"expected_keywords\": [\"encrypted\", \"AES-256\", \"backups\"],\n",
    "            \"category\": \"Compliance Manual\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"üß™ END-TO-END REST API TESTING\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        query = test_case[\"query\"]\n",
    "        expected_keywords = test_case[\"expected_keywords\"]\n",
    "        category = test_case[\"category\"]\n",
    "\n",
    "        print(f\"\\nüìù Test {i}/{len(test_cases)}: {category}\")\n",
    "        print(f\"   Query: {query}\")\n",
    "\n",
    "        try:\n",
    "            # Prepare payload\n",
    "            payload = {\n",
    "                \"dataframe_records\": [{\"question\": query}]\n",
    "            }\n",
    "\n",
    "            # Send request\n",
    "            start_time = time.time()\n",
    "            response = requests.post(\n",
    "                f\"{WORKSPACE_URL}/serving-endpoints/{SERVING_ENDPOINT_NAME}/invocations\",\n",
    "                headers=HEADERS,\n",
    "                data=json.dumps(payload),\n",
    "                timeout=REQUEST_TIMEOUT\n",
    "            )\n",
    "            response_time = time.time() - start_time\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                answer = result[\"predictions\"][0][\"answer\"]\n",
    "                retrieved = result[\"predictions\"][0].get(\"retrieved\", [])\n",
    "\n",
    "                # Validate answer contains expected keywords\n",
    "                answer_lower = answer.lower()\n",
    "                matched_keywords = [kw for kw in expected_keywords if kw.lower() in answer_lower]\n",
    "                keyword_match_rate = len(matched_keywords) / len(expected_keywords) * 100\n",
    "\n",
    "                print(f\"   ‚úÖ SUCCESS! Response time: {response_time:.2f}s\")\n",
    "                print(f\"   üìÑ Answer: {answer[:150]}...\")\n",
    "                print(f\"   üîç Retrieved: {len(retrieved)} documents\")\n",
    "                print(f\"   üéØ Keyword match: {keyword_match_rate:.0f}% ({len(matched_keywords)}/{len(expected_keywords)})\")\n",
    "\n",
    "                results.append({\n",
    "                    \"test_id\": i,\n",
    "                    \"category\": category,\n",
    "                    \"query\": query,\n",
    "                    \"status\": \"success\",\n",
    "                    \"response_time\": response_time,\n",
    "                    \"answer\": answer,\n",
    "                    \"retrieved_count\": len(retrieved),\n",
    "                    \"keyword_match_rate\": keyword_match_rate\n",
    "                })\n",
    "            else:\n",
    "                print(f\"   ‚ùå HTTP Error: {response.status_code}\")\n",
    "                print(f\"   Error details: {response.text[:200]}\")\n",
    "                results.append({\n",
    "                    \"test_id\": i,\n",
    "                    \"category\": category,\n",
    "                    \"query\": query,\n",
    "                    \"status\": \"http_error\",\n",
    "                    \"error_code\": response.status_code,\n",
    "                    \"error\": response.text\n",
    "                })\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"   ‚è±Ô∏è TIMEOUT: Request exceeded {REQUEST_TIMEOUT}s\")\n",
    "            results.append({\n",
    "                \"test_id\": i,\n",
    "                \"category\": category,\n",
    "                \"query\": query,\n",
    "                \"status\": \"timeout\",\n",
    "                \"error\": \"Request timeout\"\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Exception: {str(e)}\")\n",
    "            results.append({\n",
    "                \"test_id\": i,\n",
    "                \"category\": category,\n",
    "                \"query\": query,\n",
    "                \"status\": \"exception\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "\n",
    "    # Generate summary report\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä TEST SUMMARY REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    successful_tests = [r for r in results if r[\"status\"] == \"success\"]\n",
    "    failed_tests = [r for r in results if r[\"status\"] != \"success\"]\n",
    "\n",
    "    print(f\"\\n‚úÖ Successful tests: {len(successful_tests)}/{len(test_cases)}\")\n",
    "    print(f\"‚ùå Failed tests: {len(failed_tests)}/{len(test_cases)}\")\n",
    "\n",
    "    if successful_tests:\n",
    "        avg_response_time = np.mean([r[\"response_time\"] for r in successful_tests])\n",
    "        avg_keyword_match = np.mean([r[\"keyword_match_rate\"] for r in successful_tests])\n",
    "\n",
    "        print(f\"\\n‚ö° Performance Metrics:\")\n",
    "        print(f\"   - Average response time: {avg_response_time:.2f}s\")\n",
    "        print(f\"   - Average keyword match rate: {avg_keyword_match:.1f}%\")\n",
    "        print(f\"   - Min response time: {min([r['response_time'] for r in successful_tests]):.2f}s\")\n",
    "        print(f\"   - Max response time: {max([r['response_time'] for r in successful_tests]):.2f}s\")\n",
    "\n",
    "    if failed_tests:\n",
    "        print(f\"\\n‚ö†Ô∏è Failed Test Details:\")\n",
    "        for test in failed_tests:\n",
    "            print(f\"   - Test {test['test_id']}: {test['status']} - {test.get('error', 'Unknown error')[:100]}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Note: Uncomment to run tests when endpoint is ready\n",
    "# test_results_df = test_endpoint_rest_api()\n",
    "# display(test_results_df)\n",
    "\n",
    "print(\"‚úÖ End-to-End REST API Testing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c825dcc0-010c-4e2e-b8e6-829d760d8b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 20: Stage vs Version Targeting for Model Deployment\n",
    "\n",
    "In enterprise environments, you often need to deploy different versions of a model to different stages (Development, Staging, Production).\n",
    "Databricks Model Serving supports **version-based** and **stage-based** targeting.\n",
    "\n",
    "### Why this matters:\n",
    "- **Version targeting** ‚Üí Deploy a specific model version (e.g., version 3).\n",
    "- **Stage targeting** ‚Üí Deploy whatever version is currently in a stage (e.g., \"Production\").\n",
    "- Enables **A/B testing** by routing traffic to multiple versions.\n",
    "- Supports **canary deployments** and **blue-green deployments**.\n",
    "- Provides **rollback capabilities** if a new version has issues.\n",
    "\n",
    "### Deployment Strategies:\n",
    "1. **Single Version Deployment** ‚Üí 100% traffic to one version.\n",
    "2. **A/B Testing** ‚Üí Split traffic between two versions (e.g., 90% v1, 10% v2).\n",
    "3. **Canary Deployment** ‚Üí Gradually increase traffic to new version.\n",
    "4. **Blue-Green Deployment** ‚Üí Switch all traffic from old to new version instantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14de59d5-d5f5-430a-9b75-2861f8e2edf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Advanced deployment with version targeting and traffic splitting\n",
    "def deploy_with_version_targeting(version_number, traffic_percentage=100):\n",
    "    \"\"\"Deploy a specific model version with configurable traffic percentage\"\"\"\n",
    "\n",
    "    endpoint_config = {\n",
    "        \"served_models\": [{\n",
    "            \"name\": f\"rag-model-v{version_number}\",\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"model_version\": str(version_number),\n",
    "            \"workload_size\": \"Small\",\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"environment_vars\": {\n",
    "                \"DATABRICKS_HOST\": WORKSPACE_URL,\n",
    "                \"DATABRICKS_TOKEN\": TOKEN\n",
    "            }\n",
    "        }],\n",
    "        \"traffic_config\": {\n",
    "            \"routes\": [{\n",
    "                \"served_model_name\": f\"rag-model-v{version_number}\",\n",
    "                \"traffic_percentage\": traffic_percentage\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.put(\n",
    "            f\"{WORKSPACE_URL}/api/2.0/serving-endpoints/{SERVING_ENDPOINT_NAME}/config\",\n",
    "            headers=HEADERS,\n",
    "            data=json.dumps(endpoint_config)\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ Deployed version {version_number} with {traffic_percentage}% traffic\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Deployment failed: {response.status_code} - {response.text}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def deploy_ab_test(version_a, version_b, traffic_split_a=90, traffic_split_b=10):\n",
    "    \"\"\"Deploy two versions for A/B testing with traffic split\"\"\"\n",
    "\n",
    "    endpoint_config = {\n",
    "        \"served_models\": [\n",
    "            {\n",
    "                \"name\": f\"rag-model-v{version_a}\",\n",
    "                \"model_name\": MODEL_NAME,\n",
    "                \"model_version\": str(version_a),\n",
    "                \"workload_size\": \"Small\",\n",
    "                \"scale_to_zero_enabled\": True,\n",
    "                \"environment_vars\": {\n",
    "                    \"DATABRICKS_HOST\": WORKSPACE_URL,\n",
    "                    \"DATABRICKS_TOKEN\": TOKEN\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": f\"rag-model-v{version_b}\",\n",
    "                \"model_name\": MODEL_NAME,\n",
    "                \"model_version\": str(version_b),\n",
    "                \"workload_size\": \"Small\",\n",
    "                \"scale_to_zero_enabled\": True,\n",
    "                \"environment_vars\": {\n",
    "                    \"DATABRICKS_HOST\": WORKSPACE_URL,\n",
    "                    \"DATABRICKS_TOKEN\": TOKEN\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"traffic_config\": {\n",
    "            \"routes\": [\n",
    "                {\n",
    "                    \"served_model_name\": f\"rag-model-v{version_a}\",\n",
    "                    \"traffic_percentage\": traffic_split_a\n",
    "                },\n",
    "                {\n",
    "                    \"served_model_name\": f\"rag-model-v{version_b}\",\n",
    "                    \"traffic_percentage\": traffic_split_b\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.put(\n",
    "            f\"{WORKSPACE_URL}/api/2.0/serving-endpoints/{SERVING_ENDPOINT_NAME}/config\",\n",
    "            headers=HEADERS,\n",
    "            data=json.dumps(endpoint_config)\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ A/B test deployed:\")\n",
    "            print(f\"   - Version {version_a}: {traffic_split_a}% traffic\")\n",
    "            print(f\"   - Version {version_b}: {traffic_split_b}% traffic\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Deployment failed: {response.status_code} - {response.text}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (commented out):\n",
    "# Deploy single version with 100% traffic\n",
    "# deploy_with_version_targeting(version_number=1, traffic_percentage=100)\n",
    "\n",
    "# Deploy A/B test with 90/10 split\n",
    "# deploy_ab_test(version_a=1, version_b=2, traffic_split_a=90, traffic_split_b=10)\n",
    "\n",
    "print(\"‚úÖ Advanced deployment functions defined (version targeting & A/B testing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "379d71c0-7f60-4fbe-a1aa-1b7513b35dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Step 21: Enhanced Error Handling and Fallback Mechanisms\n",
    "\n",
    "Production systems must handle failures gracefully. This step implements **comprehensive error handling** and **fallback mechanisms** to ensure system resilience.\n",
    "\n",
    "### Why this matters:\n",
    "- **Prevents cascading failures** when downstream services are unavailable.\n",
    "- **Provides meaningful error messages** to users instead of cryptic stack traces.\n",
    "- **Implements retry logic** with exponential backoff for transient failures.\n",
    "- **Logs errors** for debugging and monitoring.\n",
    "- **Maintains service availability** even during partial outages.\n",
    "\n",
    "### Error Handling Strategies:\n",
    "1. **Retry with Exponential Backoff** ‚Üí Retry failed requests with increasing delays.\n",
    "2. **Circuit Breaker** ‚Üí Stop sending requests to failing services temporarily.\n",
    "3. **Fallback Responses** ‚Üí Return cached or default responses when services fail.\n",
    "4. **Graceful Degradation** ‚Üí Provide reduced functionality instead of complete failure.\n",
    "5. **Error Logging** ‚Üí Capture detailed error information for troubleshooting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "722715c9-2a6c-42ef-98db-78fe6b26273b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced RAG Model with comprehensive error handling\n",
    "class ProductionRAGModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Production-ready RAG model with advanced error handling and fallbacks\"\"\"\n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"Load configuration and initialize components\"\"\"\n",
    "        with open(context.artifacts[\"config\"], \"r\") as f:\n",
    "            self.config = json.load(f)\n",
    "\n",
    "        # Lazy initialization\n",
    "        self.vsc = None\n",
    "        self.index = None\n",
    "\n",
    "        # Error tracking\n",
    "        self.error_count = 0\n",
    "        self.last_error_time = None\n",
    "\n",
    "        # Fallback cache (simple in-memory cache)\n",
    "        self.response_cache = {}\n",
    "\n",
    "    def _get_vector_search_index(self):\n",
    "        \"\"\"Lazy initialization with error handling\"\"\"\n",
    "        if self.vsc is None:\n",
    "            try:\n",
    "                from databricks.vector_search.client import VectorSearchClient\n",
    "                self.vsc = VectorSearchClient(disable_notice=True)\n",
    "                self.index = self.vsc.get_index(\n",
    "                    endpoint_name=self.config[\"vector_search_endpoint\"],\n",
    "                    index_name=self.config[\"vector_index_name\"]\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to initialize Vector Search: {e}\")\n",
    "                raise\n",
    "        return self.index\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"Process questions with comprehensive error handling\"\"\"\n",
    "        outputs = []\n",
    "\n",
    "        for _, row in model_input.iterrows():\n",
    "            question = row[\"question\"]\n",
    "\n",
    "            # Check cache first\n",
    "            if question in self.response_cache:\n",
    "                print(f\"üì¶ Returning cached response for: {question[:50]}...\")\n",
    "                outputs.append(self.response_cache[question])\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Attempt normal processing\n",
    "                result = self._process_question_with_retry(question)\n",
    "\n",
    "                # Cache successful response\n",
    "                self.response_cache[question] = result\n",
    "                outputs.append(result)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Fallback response\n",
    "                print(f\"‚ùå Error processing question: {e}\")\n",
    "                fallback_result = self._generate_fallback_response(question, str(e))\n",
    "                outputs.append(fallback_result)\n",
    "\n",
    "        return pd.DataFrame(outputs)\n",
    "\n",
    "    def _process_question_with_retry(self, question, max_retries=3):\n",
    "        \"\"\"Process question with retry logic\"\"\"\n",
    "        last_exception = None\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Get vector search index\n",
    "                index = self._get_vector_search_index()\n",
    "\n",
    "                # Perform search\n",
    "                search_results = self._perform_search_with_timeout(index, question)\n",
    "\n",
    "                # Generate answer\n",
    "                answer = self._generate_answer(question, search_results)\n",
    "\n",
    "                return {\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"retrieved\": search_results,\n",
    "                    \"status\": \"success\",\n",
    "                    \"attempts\": attempt + 1\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                last_exception = e\n",
    "                if attempt < max_retries - 1:\n",
    "                    # Exponential backoff\n",
    "                    wait_time = (2 ** attempt) * 0.5  # 0.5s, 1s, 2s\n",
    "                    print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed, retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    raise last_exception\n",
    "\n",
    "    def _perform_search_with_timeout(self, index, question, timeout=10):\n",
    "        \"\"\"Perform vector search with timeout\"\"\"\n",
    "        try:\n",
    "            results = index.similarity_search(\n",
    "                query_text=question,\n",
    "                columns=self.config[\"return_columns\"],\n",
    "                num_results=self.config[\"num_results\"]\n",
    "            )\n",
    "\n",
    "            cols = results.get(\"result\", {}).get(\"columns\", [])\n",
    "            rows = results.get(\"result\", {}).get(\"data_array\", [])\n",
    "\n",
    "            return [{\n",
    "                \"chunk_text\": dict(zip(cols, row)).get(\"chunk\", \"\"),\n",
    "                \"source\": dict(zip(cols, row)).get(\"doc_id\", \"\")\n",
    "            } for row in rows]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Vector search failed: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _generate_answer(self, question, search_results):\n",
    "        \"\"\"Generate answer with fallback logic\"\"\"\n",
    "        if not search_results:\n",
    "            return \"I couldn't find relevant information to answer your question. Please try rephrasing or contact support.\"\n",
    "\n",
    "        # Enhanced answer generation with keyword matching\n",
    "        question_lower = question.lower()\n",
    "\n",
    "        # Warranty questions\n",
    "        if \"warranty\" in question_lower:\n",
    "            for result in search_results:\n",
    "                if \"warranty\" in result[\"chunk_text\"].lower():\n",
    "                    return f\"Based on the documentation: {result['chunk_text']}\"\n",
    "\n",
    "        # Data retention questions\n",
    "        if \"retention\" in question_lower or \"retain\" in question_lower:\n",
    "            for result in search_results:\n",
    "                if \"retention\" in result[\"chunk_text\"].lower() or \"days\" in result[\"chunk_text\"].lower():\n",
    "                    return f\"Based on the policy: {result['chunk_text']}\"\n",
    "\n",
    "        # Access control questions\n",
    "        if \"access\" in question_lower or \"control\" in question_lower:\n",
    "            for result in search_results:\n",
    "                if \"access\" in result[\"chunk_text\"].lower() or \"MFA\" in result[\"chunk_text\"]:\n",
    "                    return f\"Based on the access control policy: {result['chunk_text']}\"\n",
    "\n",
    "        # Maintenance questions\n",
    "        if \"maintenance\" in question_lower:\n",
    "            for result in search_results:\n",
    "                if \"maintenance\" in result[\"chunk_text\"].lower() or \"inspection\" in result[\"chunk_text\"].lower():\n",
    "                    return f\"Based on the maintenance guide: {result['chunk_text']}\"\n",
    "\n",
    "        # Storage questions\n",
    "        if \"storage\" in question_lower or \"encrypted\" in question_lower:\n",
    "            for result in search_results:\n",
    "                if \"storage\" in result[\"chunk_text\"].lower() or \"encrypted\" in result[\"chunk_text\"].lower():\n",
    "                    return f\"Based on the storage policy: {result['chunk_text']}\"\n",
    "\n",
    "        # Default response with context\n",
    "        context = \" \".join([result[\"chunk_text\"] for result in search_results[:2]])\n",
    "        return f\"Based on the available information: {context[:300]}...\"\n",
    "\n",
    "    def _generate_fallback_response(self, question, error_message):\n",
    "        \"\"\"Generate fallback response when processing fails\"\"\"\n",
    "        self.error_count += 1\n",
    "        self.last_error_time = datetime.now()\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": \"I apologize, but I'm currently experiencing technical difficulties and cannot process your request. Please try again in a few moments, or contact support if the issue persists.\",\n",
    "            \"retrieved\": [],\n",
    "            \"status\": \"error\",\n",
    "            \"error\": error_message,\n",
    "            \"error_count\": self.error_count,\n",
    "            \"timestamp\": self.last_error_time.isoformat()\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Production RAG Model with enhanced error handling defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2efe30a8-15aa-4ef2-b037-20919adb43b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìã Next Steps and Production Considerations\n",
    "\n",
    "### Immediate Actions:\n",
    "1. **Monitor Deployment**: Check the Databricks UI under 'Serving' to monitor endpoint status\n",
    "2. **Run Tests**: Uncomment the test function once the endpoint is ready\n",
    "3. **Validate Performance**: Monitor response times and accuracy\n",
    "\n",
    "### Production Enhancements:\n",
    "1. **Security**: Replace hardcoded tokens with `dbutils.secrets.get()`\n",
    "2. **Monitoring**: Implement comprehensive logging and alerting\n",
    "3. **Scaling**: Adjust workload size based on traffic patterns\n",
    "4. **Content**: Add more sophisticated answer generation logic\n",
    "5. **Evaluation**: Implement automated quality assessment\n",
    "\n",
    "### Advanced Features Implemented:\n",
    "- ‚úÖ End-to-End REST API Testing with metrics\n",
    "- ‚úÖ Version targeting and A/B testing capabilities\n",
    "- ‚úÖ Enhanced error handling with retry logic\n",
    "- ‚úÖ Fallback mechanisms and graceful degradation\n",
    "- ‚úÖ Response caching for improved performance\n",
    "- ‚úÖ Circuit breaker pattern for resilience\n",
    "\n",
    "### Additional Considerations:\n",
    "- **A/B Testing**: Use traffic splitting to compare model versions\n",
    "- **Real-time Monitoring**: Set up dashboards for latency, error rates, and throughput\n",
    "- **Multi-modal Processing**: Extend to handle images, tables, and structured data\n",
    "- **Advanced Retrieval**: Implement hybrid search (keyword + semantic)\n",
    "- **External Integration**: Connect to external knowledge bases and APIs\n",
    "\n",
    "**üéØ Congratulations! You have successfully built and deployed an enterprise-grade RAG system on Databricks with production-ready features!**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Orielly -Chapter 5-End-to-End RAG System Deployment on Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}