{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a12634a-91e0-425d-8e54-6bbcaa83726c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hands-On Lab: Building a Retrieval-Augmented GenAI App\n",
    "\n",
    "## Scenario\n",
    "You are a data engineer working for a knowledge-intensive enterprise. Your team has been asked to build a Retrieval-Augmented Generation (RAG) application that allows employees to query internal documents such as compliance manuals, product specifications, and policy handbooks using large language models (LLMs).\n",
    "\n",
    "The challenge is to ensure that responses are not only fluent but also factually accurate and contextually grounded. This lab mirrors a real-world use case where factual accuracy and compliance are critical.\n",
    "\n",
    "To achieve this, you must:\n",
    "- Connect a retriever to query a knowledge base stored in a vector database  \n",
    "- Integrate the retriever with an LLM to generate natural, context-aware answers  \n",
    "- Ensure hallucinations are minimized by grounding responses in the retrieved content  \n",
    "- Provide outputs in a format that aligns with organizational standards and compliance requirements  \n",
    "\n",
    "## Objective\n",
    "By the end of this lab, you will be able to:\n",
    "- Build a LangChain pipeline that combines a retriever with an LLM  \n",
    "- Configure the retriever to pull relevant context from a vector store  \n",
    "- Generate safe, accurate, and contextual answers aligned with enterprise knowledge sources  \n",
    "- Demonstrate how retrieval augmentation solves the problem of hallucinations and improves user trust in AI-driven applications  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "098f9e5d-c4d7-4918-b109-c5455d9f70cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Install Required Libraries\n",
    "We begin by installing the latest versions of LangChain, LangChain Community integrations, LangChain OpenAI, and FAISS. \n",
    "These packages provide the core framework, integrations, and vector search functionality needed for building our Retrieval-Augmented Generation (RAG) application.\n",
    "\n",
    "> **Note:** After installation, we restart the Python runtime to ensure the environment is updated with the new packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d77a584-8fd6-43c1-a695-2dfbaff3ef36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U langchain langchain-community langchain-openai faiss-cpu\n",
    "%restart_python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e109284-a02c-47fb-8d04-3a4e0d28bfe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Step 2: Import Required Libraries and Configure API Key\n",
    "In this step, we import the necessary LangChain modules and supporting libraries to build our Retrieval-Augmented Generation (RAG) application.\n",
    "\n",
    "- **ChatOpenAI**: Provides access to OpenAI’s GPT models for natural language responses.  \n",
    "- **OpenAIEmbeddings**: Converts documents and queries into vector embeddings for semantic search.  \n",
    "- **RetrievalQA**: A LangChain chain that combines a retriever with an LLM to produce grounded answers.  \n",
    "- **FAISS**: A vector database used for efficient similarity search over embedded documents.  \n",
    "- **PromptTemplate**: Defines structured prompts for consistent and safe responses.  \n",
    "\n",
    "Finally, we set the `OPENAI_API_KEY` environment variable so the application can authenticate with OpenAI services.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21721169-4ef2-4c83-b210-1860a8f5855f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#   Importing required libraries (Databricks compatible)\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Use your own Open AI Key\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad54065-c243-48d8-afba-f06efd171b71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Step 3: Prepare and Index Documents with FAISS\n",
    "In this step, we simulate internal enterprise documents such as compliance policies, product specifications, and employee handbooks.  \n",
    "\n",
    "To make these documents searchable, we:  \n",
    "- Convert the text into **embeddings** using OpenAIEmbeddings, which transform each document into a vector representation.  \n",
    "- Store the resulting vectors in a **FAISS vector store**, allowing us to perform semantic search queries later in the pipeline.  \n",
    "\n",
    "This step ensures that when the user asks a question, the system can retrieve the most relevant pieces of information to ground the LLM’s answer.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb27b82b-1376-4a35-b6d6-7b1c6078e235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparing a FAISS vector store\n",
    "documents = [\n",
    "    \"Policy: All employees must complete annual compliance training by June.\",\n",
    "    \"Product spec: The Model X100 device must be tested every 12 months.\",\n",
    "    \"Handbook: Employees are entitled to 15 days of paid leave per year.\"\n",
    "]\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_texts(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ce45ec3-f3f6-4819-88aa-be24d3adebda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Step 4: Configure the Retriever\n",
    "Next, we set up a **retriever**, which acts as the search mechanism over the FAISS vector store.  \n",
    "\n",
    "- The retriever identifies the most relevant documents based on semantic similarity to the user’s query.  \n",
    "- We configure it to return the **top 2 results** (`k=2`) for each query, striking a balance between accuracy and performance.  \n",
    "\n",
    "This ensures that the LLM will have the right context to generate accurate and grounded responses.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592f417c-c0ec-4070-adc2-4c23b555bcdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuring the retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef3cb33-94bf-404e-8b16-3b2453c53013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Step 5: Create a Safety-Focused Prompt\n",
    "To reduce the risk of hallucinations and ensure compliance, we design a **prompt template** with explicit safety instructions.  \n",
    "\n",
    "This template enforces the following rules:  \n",
    "- Clearly defines the model’s **role** as a compliance assistant.  \n",
    "- Instructs the LLM to base its answers strictly on the provided **context**.  \n",
    "- Requires the model to state *“the information is not available”* if the answer cannot be found in the context.  \n",
    "\n",
    "By using this approach, we ensure that the model produces accurate, safe, and policy‑aligned responses.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a98425d7-8cd3-4235-b8d9-a5179ffd36a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Creating a safety-focused prompt\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are a compliance assistant.\n",
    "Use the provided context to answer the question truthfully and clearly.\n",
    "If the answer cannot be found in the context, state that the information is not available.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae2c15ca-4ebe-409c-8813-0edfb8b751e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 6: Build the Retrieval-Augmented Generation (RAG) Pipeline\n",
    "Now we bring all the components together to create a **Retrieval-Augmented Generation (RAG) pipeline**.  \n",
    "\n",
    "- **ChatOpenAI** initializes the LLM (using GPT‑4 in this case) with a deterministic temperature of 0 to reduce randomness.  \n",
    "- **RetrievalQA** builds a chain that connects the retriever with the LLM.  \n",
    "- The **prompt template** ensures the model answers truthfully, stays within the context, and avoids hallucinations.  \n",
    "\n",
    "This pipeline is now capable of retrieving relevant context and generating accurate, safe responses.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a853dce-300f-491c-84d8-b95570e1c300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Building the RAG pipeline\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt_template},\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ef3622-cf60-4dea-ac14-3c25fda410cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Step 7: Run a Test Query\n",
    "Finally, we run a compliance-related query through the RAG pipeline to validate its performance.  \n",
    "\n",
    "In this example, we ask:  \n",
    "**“How many days of paid leave are employees entitled to?”**\n",
    "\n",
    "- The retriever searches the FAISS vector store for the most relevant documents.  \n",
    "- The LLM uses the retrieved context and the safety-focused prompt to generate a grounded, compliant response.  \n",
    "- The output is both factually accurate and aligned with organizational standards.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1311c930-f9e1-4ba1-95ee-004b1d2b7868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Running a query through the RAG pipeline\n",
    "query = \"How many days of paid leave are employees entitled to?\"\n",
    "response = qa_chain.run(query)\n",
    "\n",
    "print(\"User Query:\", query)\n",
    "print(\"RAG Response:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Orielly -Chapter 4: Building a Retrieval-Augmented GenAI App",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}