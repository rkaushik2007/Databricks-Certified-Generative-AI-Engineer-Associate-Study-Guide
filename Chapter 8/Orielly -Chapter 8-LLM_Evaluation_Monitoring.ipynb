{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25cf03e5-93dd-4285-990d-8e855fffede3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hands-On Lab: Evaluating and Monitoring LLM Performance\n",
    "\n",
    "---\n",
    "\n",
    "## Scenario\n",
    "\n",
    "You are an **LLM operations engineer** responsible for managing a production question-answering system used by internal employees across your organization. The system relies on a large language model deployed through **Databricks Model Serving**. In recent weeks, users have reported:\n",
    "\n",
    "- **Inconsistent answer quality**\n",
    "- **Occasional latency spikes**\n",
    "- **Rising usage costs**\n",
    "\n",
    "Leadership has asked you to diagnose these issues, evaluate system performance, and implement improvements using Databricks monitoring and evaluation tools.\n",
    "\n",
    "### Your Workflow in This Lab\n",
    "\n",
    "1. **Running structured evaluations** using MLflow to measure latency, token usage, and groundedness\n",
    "2. **Querying inference tables** to identify patterns in real-world traffic, including token growth and slow prompts\n",
    "3. **Using monitoring dashboards and agent traces** to investigate multi-step workflows\n",
    "4. **Detecting anomalies and configuring alert conditions** for early issue detection\n",
    "5. **Applying cost-optimization techniques** such as prompt refinement and context control\n",
    "6. **Validating improvements** with repeatable evaluation runs\n",
    "\n",
    "This lab mirrors real-world LLM observability and optimization challenges, where reliable performance and controlled operational cost are essential to business continuity. It also applies the best practices introduced throughout Chapter 8, including evaluation-dataset versioning, baseline-driven dashboard analysis, calibrated alerting strategies, and recurring review cycles that help maintain long-term system health. By completing this lab, you will integrate evaluation, monitoring, and optimization workflows to achieve end-to-end operational excellence.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. âœ… **Design an end-to-end evaluation and monitoring workflow** for a production LLM system on Databricks\n",
    "2. âœ… **Compare multiple model or prompt configurations** using MLflow evaluation metrics and justify a deployment choice\n",
    "3. âœ… **Analyze inference tables** to diagnose latency, token usage, and RAG grounding issues\n",
    "4. âœ… **Distinguish retrieval failures from model or prompt failures** using inference logs\n",
    "5. âœ… **Use agent monitoring** to analyze multi-step workflows and identify inefficiencies\n",
    "6. âœ… **Apply Databricks-native cost controls** and validate cost-saving changes without degrading output quality\n",
    "7. âœ… **Select appropriate evaluation judges** and determine when ground truth is required\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Duration\n",
    "\n",
    "**Estimated Time:** 90-120 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12ac24b3-4312-45c4-98af-2b4f5e3ec492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Part 1: Prerequisites and Environment Setup\n",
    "\n",
    "In this section, we will:\n",
    "1. Install required libraries\n",
    "2. Configure connection to Databricks workspace\n",
    "3. Set up MLflow tracking\n",
    "4. Generate synthetic Q&A data for our evaluation scenarios\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56f95945-79d3-45ba-a2e8-f876323126f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1.1: Install Required Libraries\n",
    "\n",
    "We need to install the necessary Python packages for working with Databricks, MLflow, and LLM evaluation. These packages provide the foundation for our monitoring and evaluation workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "133581ec-f805-425e-8829-c2fdb2490dc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages for the lab\n",
    "# - databricks-sdk: Official Databricks SDK for Python\n",
    "# - mlflow[databricks]: MLflow with Databricks integration (includes GenAI evaluation)\n",
    "# - openai: OpenAI client for model serving interactions\n",
    "#\n",
    "# Note: As of MLflow 3, the mlflow.genai namespace replaces databricks-agents\n",
    "# for evaluation functionality. We install mlflow[databricks] for full integration.\n",
    "\n",
    "%pip install databricks-sdk \"mlflow[databricks]>=3.1.0\" openai pandas numpy matplotlib seaborn --quiet\n",
    "\n",
    "# Restart Python to pick up new packages (required in Databricks notebooks)\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39388b2a-1219-4d1d-86a0-27838451466e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1.2: Configure Databricks Connection\n",
    "\n",
    "We'll set up the connection to our Databricks workspace using the SDK. This configuration enables us to:\n",
    "- Create and manage Model Serving endpoints\n",
    "- Access inference tables\n",
    "- Configure monitoring and alerts\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ“‹ How to Find Your Workspace URL and ID\n",
    "\n",
    "1. **Workspace URL**: Look at your browser's address bar when logged into Databricks. It follows this format:\n",
    "   ```\n",
    "   https://adb-<workspace-id>.<region>.azuredatabricks.net\n",
    "   ```\n",
    "   For example: `https://adb-YOUR-WORKSPACE-ID.azuredatabricks.net`\n",
    "\n",
    "2. **Workspace ID**: The numeric value after `adb-` in your URL (e.g., `3141834805281315`)\n",
    "\n",
    "3. **Alternative Method**:\n",
    "   - Click on your **username** in the top-right corner\n",
    "   - Select **User Settings**\n",
    "   - The Workspace ID is displayed under **Workspace Info**\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”‘ How to Generate a Personal Access Token (PAT)\n",
    "\n",
    "1. **Navigate to User Settings**:\n",
    "   - Click on your **username** in the top-right corner of the Databricks workspace\n",
    "   - Select **User Settings** from the dropdown menu\n",
    "\n",
    "2. **Access Developer Settings**:\n",
    "   - In the left sidebar, click on **Developer**\n",
    "   - You'll see the **Access tokens** section\n",
    "\n",
    "3. **Generate New Token**:\n",
    "   - Click the **Manage** button next to Access tokens\n",
    "   - Click **Generate new token**\n",
    "   - Enter a **Comment** (e.g., \"Chapter 8 Lab Token\")\n",
    "   - Set **Lifetime (days)** - leave blank for no expiration, or set a value like `90`\n",
    "   - Click **Generate**\n",
    "\n",
    "4. **Copy and Save the Token**:\n",
    "   - âš ï¸ **IMPORTANT**: Copy the token immediately! It will only be shown once.\n",
    "   - The token starts with `dapi` (e.g., `dapixxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-2`)\n",
    "   - Store it securely - you'll need it for the configuration below\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ”’ Security Note:** In a production environment, store credentials securely using Databricks Secrets instead of hardcoding tokens in notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "890d8bea-7e54-4046-8efa-f8b7d820f1ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries for Databricks connection\n",
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedEntityInput\n",
    "\n",
    "# =============================================================================\n",
    "# DATABRICKS WORKSPACE CONFIGURATION\n",
    "# =============================================================================\n",
    "# These credentials connect us to the Azure Databricks workspace\n",
    "# In production, use Databricks Secrets instead of hardcoding tokens\n",
    "\n",
    "DATABRICKS_HOST = \"https://adb-YOUR-WORKSPACE-ID.azuredatabricks.net\"\n",
    "DATABRICKS_TOKEN = \"dapi_YOUR_DATABRICKS_TOKEN_HERE-2\"\n",
    "CLUSTER_ID = \"1130-045701-3mvg1aql\"\n",
    "\n",
    "# Set environment variables for SDK and MLflow\n",
    "os.environ[\"DATABRICKS_HOST\"] = DATABRICKS_HOST\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = DATABRICKS_TOKEN\n",
    "\n",
    "# Initialize the Workspace Client\n",
    "# This client provides access to all Databricks APIs\n",
    "w = WorkspaceClient(\n",
    "    host=DATABRICKS_HOST,\n",
    "    token=DATABRICKS_TOKEN\n",
    ")\n",
    "\n",
    "# Verify connection by listing current user\n",
    "current_user = w.current_user.me()\n",
    "print(f\"âœ… Connected to Databricks as: {current_user.user_name}\")\n",
    "print(f\"ðŸ“ Workspace: {DATABRICKS_HOST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b812a830-df9c-43b1-b105-1bf5a26dd625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1.3: Configure MLflow Tracking\n",
    "\n",
    "MLflow is our primary tool for tracking experiments, logging metrics, and evaluating LLM performance. We'll configure it to use the Databricks-hosted MLflow tracking server, which provides:\n",
    "\n",
    "- **Centralized experiment tracking** across team members\n",
    "- **Model versioning** through the Model Registry\n",
    "- **Built-in LLM evaluation metrics** for quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88f52a91-0b19-4add-abc4-2dece1eb6949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# =============================================================================\n",
    "# MLFLOW CONFIGURATION\n",
    "# =============================================================================\n",
    "# Configure MLflow to use Databricks as the tracking server\n",
    "# This enables centralized experiment management and model registry\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "\n",
    "# Create an experiment for our LLM evaluation work\n",
    "# Experiments group related runs together for easy comparison\n",
    "EXPERIMENT_NAME = \"/Users/\" + current_user.user_name + \"/llm_evaluation_monitoring_lab\"\n",
    "\n",
    "# Create or get the experiment\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "    print(f\"âœ… Created new experiment: {EXPERIMENT_NAME}\")\n",
    "except mlflow.exceptions.MlflowException:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    experiment_id = experiment.experiment_id\n",
    "    print(f\"âœ… Using existing experiment: {EXPERIMENT_NAME}\")\n",
    "\n",
    "# Set the active experiment\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "print(f\"ðŸ“Š Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69b74fb0-6d47-4287-a687-87b7aa3a55a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1.4: Generate Synthetic Q&A Dataset\n",
    "\n",
    "To simulate our production question-answering system, we'll create a synthetic dataset that includes:\n",
    "\n",
    "- **Questions** of varying complexity (simple, moderate, complex)\n",
    "- **Ground truth answers** for evaluation\n",
    "- **Context documents** to test groundedness\n",
    "- **Metadata** for categorization and analysis\n",
    "\n",
    "This dataset will be used throughout the lab to:\n",
    "1. Test our Model Serving endpoint\n",
    "2. Evaluate response quality\n",
    "3. Measure latency and token usage\n",
    "4. Simulate production traffic patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5904118-c03d-4576-ba8c-f6407e91f83e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import json\n",
    "\n",
    "# =============================================================================\n",
    "# SYNTHETIC DATA GENERATION\n",
    "# =============================================================================\n",
    "# We create realistic Q&A data that mimics an internal knowledge base system\n",
    "# This includes various question types, complexities, and domains\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Define question categories and their characteristics\n",
    "CATEGORIES = {\n",
    "    \"hr_policy\": {\n",
    "        \"questions\": [\n",
    "            \"What is the company's remote work policy?\",\n",
    "            \"How many vacation days do employees get per year?\",\n",
    "            \"What is the process for requesting parental leave?\",\n",
    "            \"How do I submit an expense report?\",\n",
    "            \"What are the health insurance options available?\"\n",
    "        ],\n",
    "        \"contexts\": [\n",
    "            \"The company allows employees to work remotely up to 3 days per week. Remote work requests must be approved by the direct manager. Employees must maintain core hours of 10 AM to 3 PM in their local timezone.\",\n",
    "            \"Full-time employees receive 20 vacation days per year, accrued monthly. Unused vacation days can be carried over up to a maximum of 5 days. New employees are eligible for vacation after 90 days.\",\n",
    "            \"Parental leave policy provides 16 weeks of paid leave for primary caregivers and 8 weeks for secondary caregivers. Leave must be requested at least 30 days in advance through the HR portal.\",\n",
    "            \"Expense reports must be submitted within 30 days of the expense date. Use the Concur system to upload receipts and categorize expenses. Manager approval is required for expenses over $100.\",\n",
    "            \"The company offers three health insurance tiers: Basic, Standard, and Premium. All plans include dental and vision coverage. Open enrollment occurs annually in November.\"\n",
    "        ],\n",
    "        \"complexity\": \"simple\"\n",
    "    },\n",
    "    \"technical_docs\": {\n",
    "        \"questions\": [\n",
    "            \"How do I configure the API authentication for our microservices?\",\n",
    "            \"What is the recommended approach for database connection pooling?\",\n",
    "            \"How should I handle error logging in production applications?\",\n",
    "            \"What are the security requirements for storing customer data?\",\n",
    "            \"How do I set up CI/CD pipelines for new projects?\"\n",
    "        ],\n",
    "        \"contexts\": [\n",
    "            \"API authentication uses OAuth 2.0 with JWT tokens. Services must register with the central auth server and obtain client credentials. Token expiration is set to 1 hour with refresh token support.\",\n",
    "            \"Database connection pooling should use HikariCP with a minimum pool size of 5 and maximum of 20 connections. Connection timeout is set to 30 seconds. Enable connection validation on borrow.\",\n",
    "            \"Production logging must use structured JSON format with correlation IDs. Log levels: ERROR for exceptions, WARN for recoverable issues, INFO for business events. Use ELK stack for aggregation.\",\n",
    "            \"Customer data must be encrypted at rest using AES-256 and in transit using TLS 1.3. PII requires additional masking in logs. Data retention policy is 7 years for financial data.\",\n",
    "            \"CI/CD pipelines use GitHub Actions with standardized templates. All projects must include unit tests (80% coverage), security scanning, and automated deployment to staging before production.\"\n",
    "        ],\n",
    "        \"complexity\": \"moderate\"\n",
    "    },\n",
    "    \"strategic_planning\": {\n",
    "        \"questions\": [\n",
    "            \"What is our company's five-year growth strategy and how does it align with market trends?\",\n",
    "            \"How should we approach the integration of AI capabilities across all product lines?\",\n",
    "            \"What are the key risk factors in our current market expansion plan?\",\n",
    "            \"How do we balance innovation investment with maintaining core business profitability?\",\n",
    "            \"What organizational changes are needed to support our digital transformation initiative?\"\n",
    "        ],\n",
    "        \"contexts\": [\n",
    "            \"The five-year strategy focuses on three pillars: market expansion into APAC region, product diversification through AI integration, and operational efficiency through automation. Target growth is 25% CAGR.\",\n",
    "            \"AI integration roadmap includes: Phase 1 - customer service automation, Phase 2 - predictive analytics for sales, Phase 3 - AI-powered product features. Budget allocation is $50M over 3 years.\",\n",
    "            \"Key risks include: regulatory changes in target markets, currency fluctuation exposure, talent acquisition challenges, and competitive pressure from well-funded startups. Mitigation strategies are documented.\",\n",
    "            \"Innovation budget is set at 15% of revenue with quarterly review cycles. Core business must maintain 20% profit margin. New ventures have 18-month runway to demonstrate product-market fit.\",\n",
    "            \"Digital transformation requires: flattening organizational hierarchy, creating cross-functional pods, investing in employee upskilling, and establishing a dedicated transformation office with C-suite sponsorship.\"\n",
    "        ],\n",
    "        \"complexity\": \"complex\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… Question categories defined\")\n",
    "print(f\"ðŸ“‹ Categories: {list(CATEGORIES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0df3003-ea7d-45e6-a89e-3266a153ec3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Creating the Evaluation Dataset\n",
    "\n",
    "**What we're doing:** Building a structured evaluation dataset that pairs each question with its \"ground truth\" (correct) answer. This is essential for measuring how well the LLM performs.\n",
    "\n",
    "**How it works:**\n",
    "1. **`generate_ground_truth_answer()` function**: Extracts the expected answer from the context based on complexity:\n",
    "   - *Simple questions*: First sentence of context is the answer\n",
    "   - *Moderate questions*: First two sentences combined\n",
    "   - *Complex questions*: Full context needed for complete answer\n",
    "\n",
    "2. **Dataset structure**: Each record contains:\n",
    "   - `id`: Unique identifier (e.g., \"product_info_1\")\n",
    "   - `question`: The user's question\n",
    "   - `context`: Background information the LLM should use\n",
    "   - `ground_truth`: The correct answer we expect\n",
    "   - `category` and `complexity`: For stratified analysis\n",
    "   - `expected_tokens`: Estimated token count for cost prediction\n",
    "\n",
    "**Why this matters:** Without ground truth answers, we can't objectively measure if the LLM is giving correct responses. This dataset becomes our \"answer key\" for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b95e83c-09a5-401c-88be-8b848853f702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE EVALUATION DATASET\n",
    "# =============================================================================\n",
    "# Build a comprehensive dataset for LLM evaluation with ground truth answers\n",
    "\n",
    "def generate_ground_truth_answer(question, context, complexity):\n",
    "    \"\"\"\n",
    "    Generate a ground truth answer based on the context.\n",
    "    In a real scenario, these would be human-curated answers.\n",
    "    \"\"\"\n",
    "    # Extract key information from context as the ground truth\n",
    "    sentences = context.split(\". \")\n",
    "    if complexity == \"simple\":\n",
    "        return sentences[0] + \".\" if sentences else context\n",
    "    elif complexity == \"moderate\":\n",
    "        return \". \".join(sentences[:2]) + \".\" if len(sentences) >= 2 else context\n",
    "    else:  # complex\n",
    "        return context\n",
    "\n",
    "# Build the evaluation dataset\n",
    "eval_data = []\n",
    "\n",
    "for category, data in CATEGORIES.items():\n",
    "    for i, (question, context) in enumerate(zip(data[\"questions\"], data[\"contexts\"])):\n",
    "        ground_truth = generate_ground_truth_answer(question, context, data[\"complexity\"])\n",
    "\n",
    "        eval_data.append({\n",
    "            \"id\": f\"{category}_{i+1}\",\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"category\": category,\n",
    "            \"complexity\": data[\"complexity\"],\n",
    "            \"expected_tokens\": len(context.split()) * 2  # Rough estimate\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "\n",
    "print(f\"âœ… Created evaluation dataset with {len(eval_df)} samples\")\n",
    "print(f\"\\nðŸ“Š Dataset Summary:\")\n",
    "print(eval_df.groupby([\"category\", \"complexity\"]).size().unstack(fill_value=0))\n",
    "display(eval_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c5fa05-dc34-4903-9b00-b490d864642d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Generating Simulated Production Traffic\n",
    "\n",
    "**What we're doing:** Creating synthetic inference logs that mimic what a real production LLM endpoint would generate over 7 days of operation.\n",
    "\n",
    "**How the `generate_traffic_data()` function works:**\n",
    "\n",
    "1. **Time distribution**: Generates timestamps over 7 days with realistic patterns:\n",
    "   - More requests during business hours (9 AM - 6 PM)\n",
    "   - Fewer requests on weekends and nights\n",
    "\n",
    "2. **Latency simulation** using normal distribution:\n",
    "   ```python\n",
    "   base_latency = {\"simple\": 200, \"moderate\": 400, \"complex\": 800}\n",
    "   latency = np.random.normal(base_latency, std_dev)\n",
    "   ```\n",
    "   - Simple queries: ~200ms average\n",
    "   - Complex queries: ~800ms average\n",
    "   - 5% of requests get artificial \"spikes\" (3-5x normal latency)\n",
    "\n",
    "3. **Token usage**: Calculated based on complexity with random variation\n",
    "\n",
    "4. **Error injection**: 2% of requests marked as \"error\" or \"timeout\" to simulate real failures\n",
    "\n",
    "**Why this matters:** Real production data takes weeks to accumulate. Synthetic data lets us immediately practice monitoring, anomaly detection, and alerting without waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f01d8dac-4e54-447f-b89a-6ed7dbd1836a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE SIMULATED PRODUCTION TRAFFIC DATA\n",
    "# =============================================================================\n",
    "# Create synthetic inference logs to simulate production traffic patterns\n",
    "# This data will be used for monitoring and anomaly detection exercises\n",
    "\n",
    "def generate_traffic_data(num_records=500):\n",
    "    \"\"\"\n",
    "    Generate synthetic production traffic data with realistic patterns:\n",
    "    - Normal latency with occasional spikes\n",
    "    - Varying token usage based on question complexity\n",
    "    - Time-based patterns (higher traffic during business hours)\n",
    "    \"\"\"\n",
    "    traffic_data = []\n",
    "    base_time = datetime.now() - timedelta(days=7)\n",
    "\n",
    "    for i in range(num_records):\n",
    "        # Select random question from our dataset\n",
    "        sample = random.choice(eval_data)\n",
    "\n",
    "        # Generate timestamp with business hour bias\n",
    "        hour_offset = random.gauss(14, 4)  # Peak around 2 PM\n",
    "        hour_offset = max(8, min(20, hour_offset))  # Clamp to business hours\n",
    "        timestamp = base_time + timedelta(\n",
    "            days=random.randint(0, 6),\n",
    "            hours=hour_offset,\n",
    "            minutes=random.randint(0, 59)\n",
    "        )\n",
    "\n",
    "        # Generate latency based on complexity with occasional spikes\n",
    "        base_latency = {\"simple\": 200, \"moderate\": 400, \"complex\": 800}\n",
    "        latency = base_latency[sample[\"complexity\"]]\n",
    "        latency += random.gauss(0, latency * 0.2)  # Add noise\n",
    "\n",
    "        # Inject latency spikes (5% of requests)\n",
    "        if random.random() < 0.05:\n",
    "            latency *= random.uniform(3, 8)  # Spike multiplier\n",
    "\n",
    "        # Generate token counts\n",
    "        input_tokens = len(sample[\"question\"].split()) + len(sample[\"context\"].split())\n",
    "        output_tokens = int(sample[\"expected_tokens\"] * random.uniform(0.8, 1.2))\n",
    "\n",
    "        # Inject token anomalies (3% of requests)\n",
    "        if random.random() < 0.03:\n",
    "            output_tokens *= random.randint(3, 5)  # Unexpectedly long responses\n",
    "\n",
    "        # Generate quality score (simulated)\n",
    "        base_quality = {\"simple\": 0.9, \"moderate\": 0.8, \"complex\": 0.7}\n",
    "        quality_score = base_quality[sample[\"complexity\"]] + random.gauss(0, 0.1)\n",
    "        quality_score = max(0, min(1, quality_score))  # Clamp to [0, 1]\n",
    "\n",
    "        traffic_data.append({\n",
    "            \"request_id\": f\"req_{i:06d}\",\n",
    "            \"timestamp\": timestamp,\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"category\": sample[\"category\"],\n",
    "            \"complexity\": sample[\"complexity\"],\n",
    "            \"latency_ms\": round(latency, 2),\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"total_tokens\": input_tokens + output_tokens,\n",
    "            \"quality_score\": round(quality_score, 3),\n",
    "            \"status\": \"success\" if random.random() > 0.02 else \"error\"\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(traffic_data)\n",
    "\n",
    "# Generate traffic data\n",
    "traffic_df = generate_traffic_data(500)\n",
    "\n",
    "print(f\"âœ… Generated {len(traffic_df)} synthetic traffic records\")\n",
    "print(f\"\\nðŸ“Š Traffic Summary:\")\n",
    "print(f\"   Date Range: {traffic_df['timestamp'].min()} to {traffic_df['timestamp'].max()}\")\n",
    "print(f\"   Avg Latency: {traffic_df['latency_ms'].mean():.2f} ms\")\n",
    "print(f\"   Avg Tokens: {traffic_df['total_tokens'].mean():.1f}\")\n",
    "print(f\"   Error Rate: {(traffic_df['status'] == 'error').mean()*100:.1f}%\")\n",
    "display(traffic_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11b4cd45-458f-4a20-8be4-7c5ff76f1777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Saving Datasets to Delta Tables\n",
    "\n",
    "**What we're doing:** Persisting our synthetic datasets to Delta Lake tables in the Unity Catalog so they survive cluster restarts and can be queried by other notebooks.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **Create schema**: `spark.sql(\"CREATE SCHEMA IF NOT EXISTS main.llm_monitoring_lab\")`\n",
    "   - Creates a namespace to organize our tables\n",
    "\n",
    "2. **Convert and save**:\n",
    "   ```python\n",
    "   spark.createDataFrame(pandas_df)  # Convert pandas â†’ Spark DataFrame\n",
    "   .write.mode(\"overwrite\")          # Replace if exists\n",
    "   .saveAsTable(\"catalog.schema.table\")  # Save as managed Delta table\n",
    "   ```\n",
    "\n",
    "3. **Tables created**:\n",
    "   - `evaluation_dataset`: Our Q&A pairs with ground truth answers\n",
    "   - `traffic_data`: Synthetic production traffic logs\n",
    "\n",
    "**Why Delta Lake?**\n",
    "- **ACID transactions**: Safe concurrent reads/writes\n",
    "- **Time travel**: Query previous versions with `VERSION AS OF`\n",
    "- **Schema enforcement**: Prevents bad data from corrupting tables\n",
    "- **Optimized queries**: Auto-compaction and Z-ordering for fast reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c58581-451e-43ba-b122-4268e7c76af3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE DATASETS TO DATABRICKS\n",
    "# =============================================================================\n",
    "# Store our synthetic data in Delta tables for use throughout the lab\n",
    "\n",
    "# Define catalog and schema (using default catalog)\n",
    "CATALOG = \"main\"\n",
    "SCHEMA = \"llm_monitoring_lab\"\n",
    "\n",
    "# Create schema if it doesn't exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "# Convert pandas DataFrames to Spark DataFrames and save as Delta tables\n",
    "eval_spark_df = spark.createDataFrame(eval_df)\n",
    "eval_spark_df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SCHEMA}.evaluation_dataset\")\n",
    "\n",
    "traffic_spark_df = spark.createDataFrame(traffic_df)\n",
    "traffic_spark_df.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{SCHEMA}.traffic_data\")\n",
    "\n",
    "print(f\"âœ… Saved evaluation dataset to {CATALOG}.{SCHEMA}.evaluation_dataset\")\n",
    "print(f\"âœ… Saved traffic data to {CATALOG}.{SCHEMA}.traffic_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "258315a3-8466-4274-b6db-b117308e6cea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 2: Creating and Deploying the Model Serving Endpoint\n",
    "\n",
    "In this section, we will:\n",
    "1. Register a foundation model for serving\n",
    "2. Create a Model Serving endpoint\n",
    "3. Configure inference tables for logging\n",
    "4. Test the endpoint with sample queries\n",
    "\n",
    "This endpoint will serve as our production Q&A system that we'll monitor and optimize throughout the lab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ade37a80-4c38-477e-a259-a18fc42db823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2.1: Configure Foundation Model API Access\n",
    "\n",
    "Databricks provides **Foundation Model APIs** - pre-deployed, pay-per-token endpoints for popular LLMs. These are:\n",
    "\n",
    "- **Immediately available** - no deployment required\n",
    "- **Auto-scaling** - handles any traffic volume\n",
    "- **Pay-per-token** - cost-effective for variable workloads\n",
    "- **OpenAI-compatible** - easy integration with existing code\n",
    "\n",
    "We'll use the **Meta Llama 3.1 8B Instruct** model (`databricks-meta-llama-3-1-8b-instruct`), which is well-suited for Q&A tasks.\n",
    "\n",
    "For inference table logging, we'll create a **custom proxy endpoint** that wraps the foundation model and enables request/response logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaca870f-7495-4e8d-be4b-6e4c51ef2cf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SERVING ENDPOINT CONFIGURATION\n",
    "# =============================================================================\n",
    "# Databricks Foundation Model APIs are pre-deployed and ready to use\n",
    "# No custom endpoint creation needed - we query them directly\n",
    "\n",
    "FOUNDATION_MODEL = \"databricks-meta-llama-3-1-8b-instruct\"\n",
    "\n",
    "# First, let's verify the Foundation Model is available\n",
    "print(\"ðŸ” Checking available Foundation Model APIs...\")\n",
    "\n",
    "# List foundation model endpoints\n",
    "try:\n",
    "    all_endpoints = list(w.serving_endpoints.list())\n",
    "    foundation_endpoints = [e for e in all_endpoints if e.name.startswith(\"databricks-\")]\n",
    "    print(f\"   Found {len(foundation_endpoints)} Foundation Model endpoints\")\n",
    "\n",
    "    # Check if our model is available\n",
    "    available_models = [e.name for e in foundation_endpoints]\n",
    "    if FOUNDATION_MODEL in available_models:\n",
    "        print(f\"   âœ… {FOUNDATION_MODEL} is available\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ {FOUNDATION_MODEL} not found in list. Available models:\")\n",
    "        for model in available_models[:5]:\n",
    "            print(f\"      - {model}\")\n",
    "        print(f\"   Note: The model may still work - Foundation Models are always available\")\n",
    "except Exception as e:\n",
    "    print(f\"   Note: Could not list endpoints ({e})\")\n",
    "    print(f\"   Proceeding with Foundation Model - these are always available\")\n",
    "\n",
    "# For this lab, we'll use the Foundation Model directly\n",
    "# The endpoint name for queries will be the foundation model name\n",
    "ENDPOINT_NAME = FOUNDATION_MODEL\n",
    "\n",
    "print(f\"\\nâœ… Using Foundation Model: {ENDPOINT_NAME}\")\n",
    "print(f\"   Endpoint URL: {DATABRICKS_HOST}/serving-endpoints/{ENDPOINT_NAME}/invocations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86a1e9a2-c74d-4da6-b1f7-a792b814902a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Verifying the Foundation Model Endpoint\n",
    "\n",
    "**What we're doing:** Confirming that the Databricks Foundation Model API is accessible before we start sending queries.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **SDK endpoint check**: `w.serving_endpoints.get(name=ENDPOINT_NAME)`\n",
    "   - Retrieves metadata about the endpoint\n",
    "   - Confirms the endpoint exists and is in \"READY\" state\n",
    "\n",
    "2. **What we verify**:\n",
    "   - Endpoint name matches our configuration\n",
    "   - State is \"READY\" (not \"PENDING\" or \"FAILED\")\n",
    "   - URL is correctly formed for API calls\n",
    "\n",
    "**Foundation Models vs Custom Endpoints:**\n",
    "\n",
    "| Aspect | Foundation Model | Custom Endpoint |\n",
    "|--------|-----------------|-----------------|\n",
    "| Provisioning | Always ready | Minutes to hours |\n",
    "| Scaling | Automatic | Manual configuration |\n",
    "| Billing | Pay-per-token | Pay for compute time |\n",
    "| Customization | None | Fine-tuning possible |\n",
    "\n",
    "**Why this matters:** Verifying the endpoint upfront prevents confusing errors later. If the endpoint isn't accessible, we want to know immediately rather than after running expensive evaluation jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fda13f1-1e47-4de8-8351-6b0466f89272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VERIFY FOUNDATION MODEL ENDPOINT\n",
    "# =============================================================================\n",
    "# Foundation Model APIs are always ready - no provisioning needed\n",
    "# Let's verify the endpoint is accessible\n",
    "\n",
    "try:\n",
    "    endpoint_info = w.serving_endpoints.get(name=ENDPOINT_NAME)\n",
    "    print(f\"âœ… Foundation Model Endpoint Verified\")\n",
    "    print(f\"\\nðŸ“Š Endpoint Details:\")\n",
    "    print(f\"   Name: {endpoint_info.name}\")\n",
    "    print(f\"   State: {endpoint_info.state.ready}\")\n",
    "    print(f\"   URL: {DATABRICKS_HOST}/serving-endpoints/{ENDPOINT_NAME}/invocations\")\n",
    "\n",
    "    # Check endpoint configuration\n",
    "    if endpoint_info.config and endpoint_info.config.served_entities:\n",
    "        for entity in endpoint_info.config.served_entities:\n",
    "            print(f\"\\n   Served Entity:\")\n",
    "            print(f\"      Name: {entity.entity_name if hasattr(entity, 'entity_name') else 'N/A'}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not get endpoint details: {str(e)}\")\n",
    "    print(f\"   This is expected for Foundation Model APIs - they work without explicit configuration\")\n",
    "    print(f\"\\nâœ… Proceeding with endpoint: {ENDPOINT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a60988f2-1d00-4e1f-b633-6698d692d736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2.2: Test the Model Serving Endpoint\n",
    "\n",
    "Now we'll test our endpoint with sample queries to verify it's working correctly. We'll use the OpenAI-compatible API that Databricks provides for foundation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "021d16a9-1b2e-41cf-b83c-2b5a20ecec08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# =============================================================================\n",
    "# TEST MODEL SERVING ENDPOINT\n",
    "# =============================================================================\n",
    "# Use OpenAI-compatible client to query our endpoint\n",
    "\n",
    "# Disable MLflow's automatic tracing for OpenAI calls to avoid duplicate trace ID warnings\n",
    "# This happens because MLflow tries to auto-trace LLM calls, but re-running cells\n",
    "# can cause trace ID conflicts\n",
    "try:\n",
    "    mlflow.openai.autolog(disable=True)\n",
    "except AttributeError:\n",
    "    # Older MLflow versions may not have this - that's OK\n",
    "    pass\n",
    "\n",
    "# Initialize OpenAI client pointing to Databricks\n",
    "client = OpenAI(\n",
    "    api_key=DATABRICKS_TOKEN,\n",
    "    base_url=f\"{DATABRICKS_HOST}/serving-endpoints\"\n",
    ")\n",
    "\n",
    "def query_qa_endpoint(question, context, max_tokens=500):\n",
    "    \"\"\"\n",
    "    Query the Q&A endpoint with a question and context.\n",
    "    Returns the response and timing information.\n",
    "    \"\"\"\n",
    "    # Construct the prompt for Q&A\n",
    "    system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "    Be concise and accurate. If the answer is not in the context, say so.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=ENDPOINT_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.1  # Low temperature for factual responses\n",
    "    )\n",
    "\n",
    "    latency_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.choices[0].message.content,\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"input_tokens\": response.usage.prompt_tokens,\n",
    "        \"output_tokens\": response.usage.completion_tokens,\n",
    "        \"total_tokens\": response.usage.total_tokens\n",
    "    }\n",
    "\n",
    "# Test with a sample question\n",
    "test_sample = eval_data[0]\n",
    "print(f\"ðŸ“ Testing with question: {test_sample['question']}\")\n",
    "print(f\"ðŸ“„ Context: {test_sample['context'][:100]}...\")\n",
    "\n",
    "result = query_qa_endpoint(test_sample[\"question\"], test_sample[\"context\"])\n",
    "\n",
    "print(f\"\\nâœ… Response received:\")\n",
    "print(f\"   Answer: {result['answer']}\")\n",
    "print(f\"   Latency: {result['latency_ms']:.2f} ms\")\n",
    "print(f\"   Tokens: {result['total_tokens']} (input: {result['input_tokens']}, output: {result['output_tokens']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f24c481-8690-4cb5-9923-e27696612045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Establishing Baseline Performance Metrics\n",
    "\n",
    "**What we're doing:** Running multiple queries to collect baseline performance data that will inform our monitoring thresholds.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **Batch execution loop**:\n",
    "   ```python\n",
    "   for sample in eval_data[:10]:\n",
    "       result = query_qa_endpoint(sample[\"question\"], sample[\"context\"])\n",
    "       batch_results.append(result)\n",
    "       time.sleep(1)  # Rate limiting to avoid throttling\n",
    "   ```\n",
    "\n",
    "2. **Metrics collected per query**:\n",
    "   - `latency_ms`: End-to-end response time\n",
    "   - `input_tokens`: Tokens in prompt (question + context)\n",
    "   - `output_tokens`: Tokens in generated answer\n",
    "   - `category` and `complexity`: For stratified analysis\n",
    "\n",
    "3. **Baseline statistics calculated**:\n",
    "   - Mean, P50 (median), P95, P99 latency\n",
    "   - Average token usage by complexity\n",
    "   - Success rate\n",
    "\n",
    "**Why baselines matter (Chapter 8 best practice):**\n",
    "- **Threshold setting**: \"Alert when latency > P95 baseline Ã— 1.5\"\n",
    "- **Drift detection**: Compare current metrics to baseline\n",
    "- **Capacity planning**: Predict costs based on token patterns\n",
    "- **SLA definition**: Set realistic performance guarantees\n",
    "\n",
    "Without baselines, you're guessing at what \"normal\" looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5c44367-b5b6-4bdf-b2ae-99e886bdae57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BATCH TEST FOR BASELINE METRICS\n",
    "# =============================================================================\n",
    "# Run multiple queries to establish baseline performance metrics\n",
    "\n",
    "print(\"ðŸ”„ Running batch test to establish baseline metrics...\")\n",
    "print(\"   This will take a few minutes...\\n\")\n",
    "\n",
    "batch_results = []\n",
    "\n",
    "# Test with a subset of our evaluation data\n",
    "for i, sample in enumerate(eval_data[:10]):\n",
    "    try:\n",
    "        result = query_qa_endpoint(sample[\"question\"], sample[\"context\"])\n",
    "        result[\"id\"] = sample[\"id\"]\n",
    "        result[\"category\"] = sample[\"category\"]\n",
    "        result[\"complexity\"] = sample[\"complexity\"]\n",
    "        result[\"ground_truth\"] = sample[\"ground_truth\"]\n",
    "        batch_results.append(result)\n",
    "        print(f\"   âœ“ Completed {i+1}/10: {sample['id']} ({result['latency_ms']:.0f}ms)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Failed {i+1}/10: {sample['id']} - {str(e)}\")\n",
    "\n",
    "    # Small delay to avoid rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "# Create results DataFrame\n",
    "batch_df = pd.DataFrame(batch_results)\n",
    "\n",
    "print(f\"\\nðŸ“Š Baseline Performance Summary:\")\n",
    "print(f\"   Average Latency: {batch_df['latency_ms'].mean():.2f} ms\")\n",
    "print(f\"   P95 Latency: {batch_df['latency_ms'].quantile(0.95):.2f} ms\")\n",
    "print(f\"   Average Tokens: {batch_df['total_tokens'].mean():.1f}\")\n",
    "print(f\"\\n   By Complexity:\")\n",
    "print(batch_df.groupby('complexity')[['latency_ms', 'total_tokens']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aae802df-b3e3-427c-a8a2-a9a42c63f16f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 3: Structured Evaluation with MLflow\n",
    "\n",
    "In this section, we will:\n",
    "1. Set up MLflow evaluation for LLM responses\n",
    "2. Measure quality metrics (groundedness, relevance, coherence)\n",
    "3. Track latency and token usage metrics\n",
    "4. Compare baseline vs. optimized performance\n",
    "\n",
    "MLflow provides built-in LLM evaluation capabilities that help us systematically assess model quality.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f13234d7-f7c2-4538-8ef3-d9809217b573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3.1: Configure MLflow LLM Evaluation\n",
    "\n",
    "**What we're doing:** Setting up custom evaluation functions using the LLM-as-Judge pattern.\n",
    "\n",
    "**How our evaluation works:**\n",
    "\n",
    "1. **Custom evaluation functions**: We create functions that call an LLM to judge responses\n",
    "   ```python\n",
    "   def evaluate_groundedness(question: str, context: str, answer: str) -> int:\n",
    "       # Use LLM-as-judge to score the output\n",
    "       return score  # 1-5\n",
    "   ```\n",
    "\n",
    "2. **Function parameters**: Each function receives explicit parameters:\n",
    "   - `question`: The user's question\n",
    "   - `context`: The retrieved context (for groundedness)\n",
    "   - `answer`: The model's generated response\n",
    "   - `ground_truth`: Expected answer (for correctness)\n",
    "\n",
    "3. **LLM-as-Judge pattern**: We use Llama 8B to evaluate responses\n",
    "\n",
    "4. **MLflow 3 compatibility**: We also provide wrapper functions (`groundedness_scorer`, `correctness_scorer`) that follow the MLflow 3 `@scorer` signature for use with `mlflow.genai.evaluate()` if available.\n",
    "\n",
    "**Why this approach?**\n",
    "- Clear, explicit function signatures\n",
    "- Easy to understand and debug\n",
    "- Compatible with both manual evaluation and MLflow 3 GenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d124e3db-e6d8-4ab5-b43b-8fd7b3193ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MLFLOW GENAI EVALUATION SETUP (New API - MLflow 3+)\n",
    "# =============================================================================\n",
    "# Configure evaluation scorers for our Q&A system\n",
    "#\n",
    "# MLflow 3 introduces the @scorer decorator with standardized signature:\n",
    "#   def my_scorer(inputs, outputs, expectations=None, traces=None)\n",
    "#\n",
    "# For compatibility, we'll create simple scoring functions that can be used\n",
    "# both with the new API and called directly for manual evaluation.\n",
    "\n",
    "# Use the same foundation model for judging (8B model available in workspace)\n",
    "JUDGE_MODEL = FOUNDATION_MODEL  # databricks-meta-llama-3-1-8b-instruct\n",
    "\n",
    "def evaluate_groundedness(question: str, context: str, answer: str) -> int:\n",
    "    \"\"\"\n",
    "    Evaluate if the answer is grounded in the provided context.\n",
    "    Returns a score from 1-5.\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "\n",
    "    judge_client = OpenAI(\n",
    "        api_key=DATABRICKS_TOKEN,\n",
    "        base_url=f\"{DATABRICKS_HOST}/serving-endpoints\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"You are evaluating whether an answer is grounded in the provided context.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Score the groundedness from 1-5:\n",
    "1 = Answer contains claims not supported by context\n",
    "2 = Answer partially supported but has unsupported claims\n",
    "3 = Answer mostly supported with minor gaps\n",
    "4 = Answer well supported by context\n",
    "5 = Answer fully grounded in context\n",
    "\n",
    "Respond with ONLY a single number (1-5).\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = judge_client.chat.completions.create(\n",
    "            model=JUDGE_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=10,\n",
    "            temperature=0\n",
    "        )\n",
    "        score = int(response.choices[0].message.content.strip()[0])\n",
    "        return max(1, min(5, score))  # Ensure score is between 1-5\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning: Groundedness evaluation failed: {e}\")\n",
    "        return 3  # Default to middle score if evaluation fails\n",
    "\n",
    "def evaluate_correctness(question: str, ground_truth: str, answer: str) -> int:\n",
    "    \"\"\"\n",
    "    Evaluate if the answer correctly addresses the question based on ground truth.\n",
    "    Returns a score from 1-5.\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "\n",
    "    judge_client = OpenAI(\n",
    "        api_key=DATABRICKS_TOKEN,\n",
    "        base_url=f\"{DATABRICKS_HOST}/serving-endpoints\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"Compare the generated answer with the ground truth answer.\n",
    "\n",
    "Question: {question}\n",
    "Ground Truth: {ground_truth}\n",
    "Generated Answer: {answer}\n",
    "\n",
    "Score from 1-5:\n",
    "1 = Completely incorrect or contradicts ground truth\n",
    "2 = Partially correct but missing key information\n",
    "3 = Mostly correct with some inaccuracies\n",
    "4 = Correct with minor differences in wording\n",
    "5 = Fully correct and complete\n",
    "\n",
    "Respond with ONLY a single number (1-5).\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = judge_client.chat.completions.create(\n",
    "            model=JUDGE_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=10,\n",
    "            temperature=0\n",
    "        )\n",
    "        score = int(response.choices[0].message.content.strip()[0])\n",
    "        return max(1, min(5, score))  # Ensure score is between 1-5\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning: Correctness evaluation failed: {e}\")\n",
    "        return 3  # Default to middle score if evaluation fails\n",
    "\n",
    "# Wrapper functions compatible with MLflow 3 @scorer signature\n",
    "# These can be used with mlflow.genai.evaluate() if available\n",
    "def groundedness_scorer(inputs, outputs, expectations=None, traces=None):\n",
    "    \"\"\"MLflow 3 compatible scorer wrapper for groundedness.\"\"\"\n",
    "    question = inputs.get('question', '') if isinstance(inputs, dict) else str(inputs)\n",
    "    context = inputs.get('context', '') if isinstance(inputs, dict) else ''\n",
    "    answer = outputs.get('response', outputs) if isinstance(outputs, dict) else str(outputs)\n",
    "    return evaluate_groundedness(question, context, answer)\n",
    "\n",
    "def correctness_scorer(inputs, outputs, expectations=None, traces=None):\n",
    "    \"\"\"MLflow 3 compatible scorer wrapper for correctness.\"\"\"\n",
    "    question = inputs.get('question', '') if isinstance(inputs, dict) else str(inputs)\n",
    "    ground_truth = expectations.get('ground_truth', '') if expectations else ''\n",
    "    answer = outputs.get('response', outputs) if isinstance(outputs, dict) else str(outputs)\n",
    "    return evaluate_correctness(question, ground_truth, answer)\n",
    "\n",
    "print(\"âœ… Custom evaluation scorers configured:\")\n",
    "print(f\"   - Judge model: {JUDGE_MODEL}\")\n",
    "print(\"   - evaluate_groundedness(): Measures factual support from context\")\n",
    "print(\"   - evaluate_correctness(): Compares with ground truth answers\")\n",
    "print(\"   - MLflow 3 compatible wrappers: groundedness_scorer, correctness_scorer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6b75687-de1e-4a7c-8bee-debde6189acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3.2: Run Comprehensive Evaluation\n",
    "\n",
    "Now we'll run a full evaluation using MLflow. This will:\n",
    "1. Query our endpoint for each evaluation sample\n",
    "2. Calculate quality metrics using LLM-as-judge\n",
    "3. Log all results to MLflow for tracking\n",
    "4. Generate a comprehensive evaluation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eb002af-05c1-4425-ae37-f8513995b8c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN MLFLOW EVALUATION\n",
    "# =============================================================================\n",
    "# Execute comprehensive evaluation and log results\n",
    "\n",
    "# Prepare evaluation data with model outputs\n",
    "eval_results = []\n",
    "\n",
    "print(\"ðŸ”„ Running comprehensive evaluation...\")\n",
    "print(\"   Querying endpoint and collecting responses...\\n\")\n",
    "\n",
    "for i, sample in enumerate(eval_data):\n",
    "    try:\n",
    "        # Query the endpoint\n",
    "        result = query_qa_endpoint(sample[\"question\"], sample[\"context\"])\n",
    "\n",
    "        eval_results.append({\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"context\": sample[\"context\"],\n",
    "            \"ground_truth\": sample[\"ground_truth\"],\n",
    "            \"output\": result[\"answer\"],  # MLflow expects 'output' column\n",
    "            \"category\": sample[\"category\"],\n",
    "            \"complexity\": sample[\"complexity\"],\n",
    "            \"latency_ms\": result[\"latency_ms\"],\n",
    "            \"input_tokens\": result[\"input_tokens\"],\n",
    "            \"output_tokens\": result[\"output_tokens\"],\n",
    "            \"total_tokens\": result[\"total_tokens\"]\n",
    "        })\n",
    "\n",
    "        print(f\"   âœ“ {i+1}/{len(eval_data)}: {sample['id']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— {i+1}/{len(eval_data)}: {sample['id']} - {str(e)}\")\n",
    "\n",
    "    time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "# Create evaluation DataFrame\n",
    "eval_results_df = pd.DataFrame(eval_results)\n",
    "print(f\"\\nâœ… Collected {len(eval_results_df)} evaluation samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1bf9a08-1a25-4712-a436-44aca530f427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Logging Evaluation Results to MLflow\n",
    "\n",
    "**What we're doing:** Running our custom scorers on each sample and logging results to MLflow.\n",
    "\n",
    "**How the new GenAI API works:**\n",
    "\n",
    "1. **Prepare data in new format**:\n",
    "   ```python\n",
    "   eval_data_for_genai.append({\n",
    "       \"inputs\": {\"question\": ..., \"context\": ...},\n",
    "       \"outputs\": row[\"output\"],\n",
    "       \"expectations\": {\"ground_truth\": ...}\n",
    "   })\n",
    "   ```\n",
    "\n",
    "2. **Call evaluation functions directly**:\n",
    "   ```python\n",
    "   g_score = evaluate_groundedness(question=..., context=..., answer=...)\n",
    "   c_score = evaluate_correctness(question=..., ground_truth=..., answer=...)\n",
    "   ```\n",
    "   - Each function calls the judge LLM (Llama 8B)\n",
    "   - Returns a score from 1-5\n",
    "\n",
    "3. **Log aggregate metrics**:\n",
    "   ```python\n",
    "   mlflow.log_metric(\"groundedness_mean\", avg_groundedness)\n",
    "   mlflow.log_metric(\"answer_correctness_mean\", avg_correctness)\n",
    "   ```\n",
    "\n",
    "**Why this approach?** We use explicit function calls for clarity and control. The MLflow 3 compatible wrappers (`groundedness_scorer`, `correctness_scorer`) can be used with `mlflow.genai.evaluate()` if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cdc8a76-97a8-4540-884a-c0f059f8797c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOG EVALUATION TO MLFLOW\n",
    "# =============================================================================\n",
    "# Run MLflow evaluation with our custom scorers (New GenAI API)\n",
    "\n",
    "with mlflow.start_run(run_name=\"baseline_evaluation\") as run:\n",
    "\n",
    "    # Log evaluation parameters\n",
    "    mlflow.log_param(\"endpoint_name\", ENDPOINT_NAME)\n",
    "    mlflow.log_param(\"model_name\", FOUNDATION_MODEL)\n",
    "    mlflow.log_param(\"num_samples\", len(eval_results_df))\n",
    "    mlflow.log_param(\"evaluation_date\", datetime.now().isoformat())\n",
    "\n",
    "    # Prepare data in the new format for mlflow.genai.evaluate()\n",
    "    # The new API expects: inputs (dict), outputs (str), expectations (dict)\n",
    "    eval_data_for_genai = []\n",
    "    for _, row in eval_results_df.iterrows():\n",
    "        eval_data_for_genai.append({\n",
    "            \"inputs\": {\n",
    "                \"question\": row[\"question\"],\n",
    "                \"context\": row[\"context\"]\n",
    "            },\n",
    "            \"outputs\": row[\"output\"],\n",
    "            \"expectations\": {\n",
    "                \"ground_truth\": row[\"ground_truth\"]\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Run evaluation with custom scorers\n",
    "    print(\"ðŸ”„ Running MLflow evaluation with LLM-as-judge scorers...\")\n",
    "    print(\"   (Using new MLflow 3.4+ GenAI API)\")\n",
    "\n",
    "    # Calculate scores manually and log them\n",
    "    groundedness_scores = []\n",
    "    correctness_scores = []\n",
    "\n",
    "    for i, item in enumerate(eval_data_for_genai):\n",
    "        try:\n",
    "            # Use the evaluation functions with explicit parameters\n",
    "            g_score = evaluate_groundedness(\n",
    "                question=item[\"inputs\"][\"question\"],\n",
    "                context=item[\"inputs\"][\"context\"],\n",
    "                answer=item[\"outputs\"]\n",
    "            )\n",
    "            c_score = evaluate_correctness(\n",
    "                question=item[\"inputs\"][\"question\"],\n",
    "                ground_truth=item[\"expectations\"][\"ground_truth\"],\n",
    "                answer=item[\"outputs\"]\n",
    "            )\n",
    "            groundedness_scores.append(g_score)\n",
    "            correctness_scores.append(c_score)\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"   Evaluated {i + 1}/{len(eval_data_for_genai)} samples...\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Warning: Error evaluating sample {i}: {e}\")\n",
    "            groundedness_scores.append(3)\n",
    "            correctness_scores.append(3)\n",
    "        time.sleep(0.5)  # Rate limiting for judge model\n",
    "\n",
    "    # Calculate aggregate metrics\n",
    "    avg_groundedness = sum(groundedness_scores) / len(groundedness_scores)\n",
    "    avg_correctness = sum(correctness_scores) / len(correctness_scores)\n",
    "\n",
    "    # Log LLM-as-judge metrics\n",
    "    mlflow.log_metric(\"groundedness_mean\", avg_groundedness)\n",
    "    mlflow.log_metric(\"answer_correctness_mean\", avg_correctness)\n",
    "\n",
    "    # Log additional performance metrics\n",
    "    mlflow.log_metric(\"avg_latency_ms\", eval_results_df[\"latency_ms\"].mean())\n",
    "    mlflow.log_metric(\"p95_latency_ms\", eval_results_df[\"latency_ms\"].quantile(0.95))\n",
    "    mlflow.log_metric(\"avg_total_tokens\", eval_results_df[\"total_tokens\"].mean())\n",
    "    mlflow.log_metric(\"total_input_tokens\", eval_results_df[\"input_tokens\"].sum())\n",
    "    mlflow.log_metric(\"total_output_tokens\", eval_results_df[\"output_tokens\"].sum())\n",
    "\n",
    "    # Add scores to dataframe and save\n",
    "    eval_results_df[\"groundedness_score\"] = groundedness_scores\n",
    "    eval_results_df[\"correctness_score\"] = correctness_scores\n",
    "\n",
    "    # Log the evaluation dataset as artifact\n",
    "    eval_results_df.to_csv(\"/tmp/evaluation_results.csv\", index=False)\n",
    "    mlflow.log_artifact(\"/tmp/evaluation_results.csv\")\n",
    "\n",
    "    print(f\"\\nâœ… Evaluation logged to MLflow\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")\n",
    "    print(f\"   Experiment: {EXPERIMENT_NAME}\")\n",
    "\n",
    "    # Display metrics summary\n",
    "    print(f\"\\nðŸ“Š Evaluation Metrics Summary:\")\n",
    "    print(f\"   groundedness_mean: {avg_groundedness:.2f}\")\n",
    "    print(f\"   answer_correctness_mean: {avg_correctness:.2f}\")\n",
    "    print(f\"   avg_latency_ms: {eval_results_df['latency_ms'].mean():.2f}\")\n",
    "    print(f\"   p95_latency_ms: {eval_results_df['latency_ms'].quantile(0.95):.2f}\")\n",
    "    print(f\"   avg_total_tokens: {eval_results_df['total_tokens'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bec4a2b3-7bf9-485d-b958-ca805cc09d37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3.3: Selecting Evaluation Judges and Ground Truth Requirements\n",
    "\n",
    "**Learning Objective 7:** Select appropriate evaluation judges and determine when ground truth is required.\n",
    "\n",
    "When evaluating LLM outputs, you must choose the right evaluation approach. This decision depends on what you're measuring and what reference data you have available.\n",
    "\n",
    "---\n",
    "\n",
    "#### When to Use Different Evaluation Approaches\n",
    "\n",
    "| Evaluation Type | When to Use | Ground Truth Required? | Example Metrics |\n",
    "|-----------------|-------------|------------------------|-----------------|\n",
    "| **LLM-as-Judge** | Subjective quality (coherence, helpfulness, tone) | No | Groundedness, Relevance, Fluency |\n",
    "| **Ground Truth Comparison** | Factual correctness, exact answers | Yes | Answer Correctness, F1 Score |\n",
    "| **Heuristic/Rule-Based** | Format validation, length checks | No | Token count, Response format |\n",
    "| **Human Evaluation** | High-stakes decisions, edge cases | Optional | Expert ratings, A/B preferences |\n",
    "\n",
    "---\n",
    "\n",
    "#### LLM-as-Judge: Pros and Cons\n",
    "\n",
    "**Advantages:**\n",
    "- Scales to thousands of evaluations automatically\n",
    "- Can assess subjective qualities (tone, helpfulness)\n",
    "- No need to maintain ground truth datasets\n",
    "\n",
    "**Limitations:**\n",
    "- Judge model may have biases (prefers verbose answers)\n",
    "- Smaller models (8B) less reliable than larger ones (70B+)\n",
    "- Cannot verify factual accuracy without context\n",
    "\n",
    "**Best Practice:** Use a larger model as judge than the model being evaluated when possible.\n",
    "\n",
    "---\n",
    "\n",
    "#### When Ground Truth is Required\n",
    "\n",
    "Ground truth is **essential** when:\n",
    "1. **Factual accuracy matters** - Medical, legal, financial domains\n",
    "2. **Exact answers expected** - Math problems, code generation\n",
    "3. **Compliance requirements** - Auditable evaluation trails\n",
    "4. **Regression testing** - Comparing model versions on known cases\n",
    "\n",
    "Ground truth is **optional** when:\n",
    "1. **Subjective quality** - \"Is this response helpful?\"\n",
    "2. **Style/tone evaluation** - \"Is this professional?\"\n",
    "3. **Exploratory analysis** - Understanding failure modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e8435b1-c4a9-4ca7-88f4-a6f2ee9a0122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEMONSTRATION: CHOOSING EVALUATION APPROACH\n",
    "# =============================================================================\n",
    "# This cell demonstrates how to select the right evaluation approach\n",
    "\n",
    "def select_evaluation_approach(use_case: str) -> dict:\n",
    "    \"\"\"\n",
    "    Helper function to recommend evaluation approach based on use case.\n",
    "\n",
    "    Returns recommended approach with rationale.\n",
    "    \"\"\"\n",
    "    approaches = {\n",
    "        \"factual_qa\": {\n",
    "            \"approach\": \"Ground Truth + LLM-as-Judge\",\n",
    "            \"ground_truth_required\": True,\n",
    "            \"recommended_metrics\": [\"answer_correctness\", \"groundedness\"],\n",
    "            \"rationale\": \"Factual Q&A requires verifiable answers. Use ground truth for correctness, LLM-as-judge for groundedness.\"\n",
    "        },\n",
    "        \"creative_writing\": {\n",
    "            \"approach\": \"LLM-as-Judge Only\",\n",
    "            \"ground_truth_required\": False,\n",
    "            \"recommended_metrics\": [\"coherence\", \"creativity\", \"tone\"],\n",
    "            \"rationale\": \"Creative tasks have no single correct answer. LLM judges can assess quality dimensions.\"\n",
    "        },\n",
    "        \"code_generation\": {\n",
    "            \"approach\": \"Execution-Based + Ground Truth\",\n",
    "            \"ground_truth_required\": True,\n",
    "            \"recommended_metrics\": [\"test_pass_rate\", \"syntax_validity\"],\n",
    "            \"rationale\": \"Code can be objectively tested. Run tests and compare outputs.\"\n",
    "        },\n",
    "        \"summarization\": {\n",
    "            \"approach\": \"LLM-as-Judge + Optional Ground Truth\",\n",
    "            \"ground_truth_required\": False,\n",
    "            \"recommended_metrics\": [\"faithfulness\", \"conciseness\", \"coverage\"],\n",
    "            \"rationale\": \"Summaries can vary. LLM judges assess quality; ground truth optional for key points.\"\n",
    "        },\n",
    "        \"rag_qa\": {\n",
    "            \"approach\": \"Ground Truth + Groundedness Check\",\n",
    "            \"ground_truth_required\": True,\n",
    "            \"recommended_metrics\": [\"answer_correctness\", \"groundedness\", \"context_relevance\"],\n",
    "            \"rationale\": \"RAG systems need both correct answers AND grounding in retrieved context.\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return approaches.get(use_case, {\n",
    "        \"approach\": \"LLM-as-Judge\",\n",
    "        \"ground_truth_required\": False,\n",
    "        \"recommended_metrics\": [\"relevance\", \"coherence\"],\n",
    "        \"rationale\": \"Default to LLM-as-judge for general evaluation.\"\n",
    "    })\n",
    "\n",
    "# Demonstrate for our Q&A use case\n",
    "our_use_case = select_evaluation_approach(\"rag_qa\")\n",
    "\n",
    "print(\"ðŸ“‹ Evaluation Approach for Our RAG Q&A System:\")\n",
    "print(f\"   Approach: {our_use_case['approach']}\")\n",
    "print(f\"   Ground Truth Required: {our_use_case['ground_truth_required']}\")\n",
    "print(f\"   Recommended Metrics: {', '.join(our_use_case['recommended_metrics'])}\")\n",
    "print(f\"   Rationale: {our_use_case['rationale']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"In this lab, we use:\")\n",
    "print(\"   âœ“ Ground truth answers (from our evaluation dataset)\")\n",
    "print(\"   âœ“ LLM-as-judge (groundedness scorer)\")\n",
    "print(\"   âœ“ Answer correctness (compares to ground truth)\")\n",
    "print(\"This combination is ideal for RAG-based Q&A systems.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f12125a-981b-45d3-be3b-c7abe0d7d704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 4: Analyzing Inference Tables\n",
    "\n",
    "In this section, we will:\n",
    "1. Query inference tables to analyze production traffic\n",
    "2. Identify latency patterns and anomalies\n",
    "3. Analyze token usage trends\n",
    "4. Detect problematic query patterns\n",
    "\n",
    "Inference tables automatically capture all requests to Model Serving endpoints, providing rich data for monitoring and debugging.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62fd264c-e330-444c-86e4-4e4c01fe4274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4.1: Query Inference Tables\n",
    "\n",
    "Databricks automatically logs all inference requests to Delta tables when auto-capture is enabled. These tables contain:\n",
    "\n",
    "- **Request payloads**: Input prompts and parameters\n",
    "- **Response data**: Model outputs and metadata\n",
    "- **Timing information**: Latency measurements\n",
    "- **Token counts**: Usage metrics for cost analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d18f0e4-16d6-4f44-b683-80c55c208cc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# QUERY INFERENCE TABLES\n",
    "# =============================================================================\n",
    "# Analyze production traffic patterns from inference logs\n",
    "\n",
    "# For this lab, we'll use our synthetic traffic data\n",
    "# In production, you would query the actual inference tables:\n",
    "# inference_table = f\"{CATALOG}.{SCHEMA}.qa_endpoint_payload\"\n",
    "\n",
    "# Load our synthetic traffic data\n",
    "traffic_analysis_df = spark.table(f\"{CATALOG}.{SCHEMA}.traffic_data\").toPandas()\n",
    "\n",
    "print(f\"ðŸ“Š Analyzing {len(traffic_analysis_df)} inference records\")\n",
    "print(f\"   Time Range: {traffic_analysis_df['timestamp'].min()} to {traffic_analysis_df['timestamp'].max()}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nðŸ“ˆ Traffic Statistics:\")\n",
    "print(f\"   Total Requests: {len(traffic_analysis_df)}\")\n",
    "print(f\"   Success Rate: {(traffic_analysis_df['status'] == 'success').mean()*100:.1f}%\")\n",
    "print(f\"   Avg Latency: {traffic_analysis_df['latency_ms'].mean():.2f} ms\")\n",
    "print(f\"   P50 Latency: {traffic_analysis_df['latency_ms'].median():.2f} ms\")\n",
    "print(f\"   P95 Latency: {traffic_analysis_df['latency_ms'].quantile(0.95):.2f} ms\")\n",
    "print(f\"   P99 Latency: {traffic_analysis_df['latency_ms'].quantile(0.99):.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3cc6210-0356-4fb0-b78d-682203cd27f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4.2: Identify Latency Patterns\n",
    "\n",
    "We'll analyze latency patterns to identify:\n",
    "- Time-based variations (peak hours vs. off-peak)\n",
    "- Complexity-based differences\n",
    "- Anomalous slow requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "333070a0-737f-4cef-936e-815697f21245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# =============================================================================\n",
    "# LATENCY PATTERN ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Latency distribution by complexity\n",
    "ax1 = axes[0, 0]\n",
    "for complexity in ['simple', 'moderate', 'complex']:\n",
    "    data = traffic_analysis_df[traffic_analysis_df['complexity'] == complexity]['latency_ms']\n",
    "    ax1.hist(data, bins=30, alpha=0.5, label=complexity)\n",
    "ax1.set_xlabel('Latency (ms)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Latency Distribution by Complexity')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Latency over time\n",
    "ax2 = axes[0, 1]\n",
    "traffic_analysis_df['hour'] = pd.to_datetime(traffic_analysis_df['timestamp']).dt.hour\n",
    "hourly_latency = traffic_analysis_df.groupby('hour')['latency_ms'].mean()\n",
    "ax2.plot(hourly_latency.index, hourly_latency.values, marker='o')\n",
    "ax2.set_xlabel('Hour of Day')\n",
    "ax2.set_ylabel('Average Latency (ms)')\n",
    "ax2.set_title('Average Latency by Hour')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Token usage vs latency\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(traffic_analysis_df['total_tokens'], traffic_analysis_df['latency_ms'],\n",
    "            alpha=0.5, c=traffic_analysis_df['complexity'].map({'simple': 0, 'moderate': 1, 'complex': 2}))\n",
    "ax3.set_xlabel('Total Tokens')\n",
    "ax3.set_ylabel('Latency (ms)')\n",
    "ax3.set_title('Token Usage vs Latency')\n",
    "\n",
    "# 4. Latency percentiles by category\n",
    "ax4 = axes[1, 1]\n",
    "category_stats = traffic_analysis_df.groupby('category')['latency_ms'].agg(['mean', 'median', lambda x: x.quantile(0.95)])\n",
    "category_stats.columns = ['Mean', 'Median', 'P95']\n",
    "category_stats.plot(kind='bar', ax=ax4)\n",
    "ax4.set_xlabel('Category')\n",
    "ax4.set_ylabel('Latency (ms)')\n",
    "ax4.set_title('Latency Statistics by Category')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/latency_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Latency analysis charts generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7454ac4-c36d-4b11-ae49-42f5e7ac9561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4.3: Identify Slow and Anomalous Queries\n",
    "\n",
    "We'll identify queries that exceed our latency thresholds and investigate their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec46ec5d-dddb-4e7a-bfa9-1991c5838a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANOMALY DETECTION IN LATENCY\n",
    "# =============================================================================\n",
    "\n",
    "# Define latency thresholds based on complexity\n",
    "LATENCY_THRESHOLDS = {\n",
    "    \"simple\": 500,      # 500ms for simple queries\n",
    "    \"moderate\": 1000,   # 1s for moderate queries\n",
    "    \"complex\": 2000     # 2s for complex queries\n",
    "}\n",
    "\n",
    "# Identify slow queries\n",
    "def is_slow_query(row):\n",
    "    threshold = LATENCY_THRESHOLDS.get(row['complexity'], 1000)\n",
    "    return row['latency_ms'] > threshold\n",
    "\n",
    "traffic_analysis_df['is_slow'] = traffic_analysis_df.apply(is_slow_query, axis=1)\n",
    "\n",
    "# Analyze slow queries\n",
    "slow_queries = traffic_analysis_df[traffic_analysis_df['is_slow']]\n",
    "\n",
    "print(f\"ðŸ¢ Slow Query Analysis:\")\n",
    "print(f\"   Total Slow Queries: {len(slow_queries)} ({len(slow_queries)/len(traffic_analysis_df)*100:.1f}%)\")\n",
    "print(f\"\\n   By Complexity:\")\n",
    "print(slow_queries.groupby('complexity').size())\n",
    "print(f\"\\n   By Category:\")\n",
    "print(slow_queries.groupby('category').size())\n",
    "\n",
    "# Identify extreme outliers (> 3 standard deviations)\n",
    "mean_latency = traffic_analysis_df['latency_ms'].mean()\n",
    "std_latency = traffic_analysis_df['latency_ms'].std()\n",
    "outlier_threshold = mean_latency + 3 * std_latency\n",
    "\n",
    "outliers = traffic_analysis_df[traffic_analysis_df['latency_ms'] > outlier_threshold]\n",
    "print(f\"\\nâš ï¸ Extreme Outliers (>{outlier_threshold:.0f}ms): {len(outliers)}\")\n",
    "\n",
    "if len(outliers) > 0:\n",
    "    print(\"\\n   Sample outlier queries:\")\n",
    "    for _, row in outliers.head(3).iterrows():\n",
    "        print(f\"   - {row['request_id']}: {row['latency_ms']:.0f}ms ({row['complexity']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a02cd425-5ff4-4a3a-93cb-2d81c1a340ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4.4: RAG Grounding Analysis\n",
    "\n",
    "**Learning Objective 3:** Analyze inference tables to diagnose latency, token usage, and **RAG grounding issues**.\n",
    "\n",
    "In RAG (Retrieval-Augmented Generation) systems, grounding issues occur when:\n",
    "- The model generates answers not supported by the retrieved context\n",
    "- Retrieved context is irrelevant to the question\n",
    "- The model ignores relevant context and hallucinates\n",
    "\n",
    "We'll analyze our inference data to identify potential grounding problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06d5aa10-d61b-41b8-9247-e0c90b3b7101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RAG GROUNDING ANALYSIS\n",
    "# =============================================================================\n",
    "# Analyze the relationship between context quality and answer quality\n",
    "\n",
    "# Simulate grounding analysis on our evaluation data\n",
    "# In production, this would come from inference tables with context logged\n",
    "\n",
    "def analyze_grounding_signals(question: str, context: str, answer: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze signals that indicate potential grounding issues.\n",
    "\n",
    "    Returns dict with grounding quality indicators.\n",
    "    \"\"\"\n",
    "    signals = {}\n",
    "\n",
    "    # Signal 1: Context length - very short context may lack information\n",
    "    context_words = len(context.split())\n",
    "    signals['context_word_count'] = context_words\n",
    "    signals['context_too_short'] = context_words < 20\n",
    "\n",
    "    # Signal 2: Answer length vs context length ratio\n",
    "    answer_words = len(answer.split())\n",
    "    signals['answer_word_count'] = answer_words\n",
    "    signals['answer_context_ratio'] = answer_words / max(context_words, 1)\n",
    "    signals['answer_longer_than_context'] = answer_words > context_words  # Red flag\n",
    "\n",
    "    # Signal 3: Key term overlap (simple heuristic)\n",
    "    question_terms = set(question.lower().split())\n",
    "    context_terms = set(context.lower().split())\n",
    "    answer_terms = set(answer.lower().split())\n",
    "\n",
    "    # Remove common stop words\n",
    "    stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'what', 'how', 'why', 'when', 'where', 'who'}\n",
    "    question_terms -= stop_words\n",
    "    context_terms -= stop_words\n",
    "    answer_terms -= stop_words\n",
    "\n",
    "    # Check if answer terms appear in context\n",
    "    answer_in_context = len(answer_terms & context_terms) / max(len(answer_terms), 1)\n",
    "    signals['answer_grounded_ratio'] = answer_in_context\n",
    "    signals['potential_hallucination'] = answer_in_context < 0.3  # Less than 30% overlap\n",
    "\n",
    "    # Signal 4: Question-context relevance\n",
    "    question_in_context = len(question_terms & context_terms) / max(len(question_terms), 1)\n",
    "    signals['context_relevance'] = question_in_context\n",
    "    signals['irrelevant_context'] = question_in_context < 0.2\n",
    "\n",
    "    return signals\n",
    "\n",
    "# Analyze grounding for our evaluation samples\n",
    "grounding_analysis = []\n",
    "\n",
    "for _, row in eval_results_df.iterrows():\n",
    "    signals = analyze_grounding_signals(\n",
    "        row['question'],\n",
    "        row['context'],\n",
    "        row['output']\n",
    "    )\n",
    "    signals['question'] = row['question'][:50] + \"...\"\n",
    "    signals['groundedness_score'] = row.get('groundedness_score', 3)\n",
    "    grounding_analysis.append(signals)\n",
    "\n",
    "grounding_df = pd.DataFrame(grounding_analysis)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"ðŸ“Š RAG Grounding Analysis Summary:\")\n",
    "print(f\"   Total samples analyzed: {len(grounding_df)}\")\n",
    "print(f\"\\n   Potential Issues Detected:\")\n",
    "print(f\"   - Context too short (<20 words): {grounding_df['context_too_short'].sum()}\")\n",
    "print(f\"   - Answer longer than context: {grounding_df['answer_longer_than_context'].sum()}\")\n",
    "print(f\"   - Potential hallucinations: {grounding_df['potential_hallucination'].sum()}\")\n",
    "print(f\"   - Irrelevant context retrieved: {grounding_df['irrelevant_context'].sum()}\")\n",
    "\n",
    "# Correlation with groundedness scores\n",
    "if 'groundedness_score' in grounding_df.columns:\n",
    "    print(f\"\\n   Average Groundedness by Issue Type:\")\n",
    "    print(f\"   - With potential hallucination: {grounding_df[grounding_df['potential_hallucination']]['groundedness_score'].mean():.2f}\")\n",
    "    print(f\"   - Without potential hallucination: {grounding_df[~grounding_df['potential_hallucination']]['groundedness_score'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1edc7fe2-6dbf-4ba6-a5ca-177b28ce2329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4.5: Distinguishing Retrieval Failures from Model/Prompt Failures\n",
    "\n",
    "**Learning Objective 4:** Distinguish retrieval failures from model or prompt failures using inference logs.\n",
    "\n",
    "When a RAG system produces poor answers, the root cause can be:\n",
    "\n",
    "| Failure Type | Symptoms | Root Cause | Fix |\n",
    "|--------------|----------|------------|-----|\n",
    "| **Retrieval Failure** | Low context relevance, missing key info | Vector search issues, poor embeddings | Improve retrieval, re-index |\n",
    "| **Model Failure** | Ignores good context, hallucinates | Model limitations, context too long | Try different model, truncate context |\n",
    "| **Prompt Failure** | Misunderstands task, wrong format | Unclear instructions | Refine system prompt |\n",
    "\n",
    "We'll implement a diagnostic framework to classify failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "196b0e05-583d-458b-a620-5ddd2145ab5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FAILURE CLASSIFICATION FRAMEWORK\n",
    "# =============================================================================\n",
    "# Classify failures as retrieval, model, or prompt issues\n",
    "\n",
    "def classify_failure(\n",
    "    question: str,\n",
    "    context: str,\n",
    "    answer: str,\n",
    "    ground_truth: str,\n",
    "    groundedness_score: int,\n",
    "    correctness_score: int\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Classify the type of failure based on evaluation signals.\n",
    "\n",
    "    Returns classification with confidence and recommended action.\n",
    "    \"\"\"\n",
    "    # Calculate diagnostic signals\n",
    "    signals = analyze_grounding_signals(question, context, answer)\n",
    "\n",
    "    classification = {\n",
    "        \"failure_type\": None,\n",
    "        \"confidence\": 0.0,\n",
    "        \"signals\": [],\n",
    "        \"recommended_action\": None\n",
    "    }\n",
    "\n",
    "    # Decision tree for classification\n",
    "\n",
    "    # Case 1: Retrieval Failure\n",
    "    # - Context is irrelevant to question\n",
    "    # - Groundedness is low because context doesn't help\n",
    "    if signals['irrelevant_context']:\n",
    "        classification[\"failure_type\"] = \"RETRIEVAL_FAILURE\"\n",
    "        classification[\"confidence\"] = 0.8\n",
    "        classification[\"signals\"].append(\"Context has low relevance to question\")\n",
    "        classification[\"recommended_action\"] = \"Improve retrieval: check embeddings, expand search, verify index\"\n",
    "\n",
    "    # Case 2: Model Failure (Hallucination)\n",
    "    # - Context is relevant but answer doesn't use it\n",
    "    # - Low groundedness despite good context\n",
    "    elif signals['potential_hallucination'] and not signals['irrelevant_context']:\n",
    "        classification[\"failure_type\"] = \"MODEL_FAILURE\"\n",
    "        classification[\"confidence\"] = 0.7\n",
    "        classification[\"signals\"].append(\"Answer not grounded despite relevant context\")\n",
    "        classification[\"recommended_action\"] = \"Model issue: try larger model, reduce context length, add grounding instructions\"\n",
    "\n",
    "    # Case 3: Prompt Failure\n",
    "    # - Answer is grounded but incorrect\n",
    "    # - Model understood context but misunderstood task\n",
    "    elif groundedness_score >= 4 and correctness_score <= 2:\n",
    "        classification[\"failure_type\"] = \"PROMPT_FAILURE\"\n",
    "        classification[\"confidence\"] = 0.6\n",
    "        classification[\"signals\"].append(\"Answer is grounded but incorrect - task misunderstanding\")\n",
    "        classification[\"recommended_action\"] = \"Refine prompt: clarify instructions, add examples, specify format\"\n",
    "\n",
    "    # Case 4: No clear failure\n",
    "    else:\n",
    "        classification[\"failure_type\"] = \"NO_CLEAR_FAILURE\"\n",
    "        classification[\"confidence\"] = 0.5\n",
    "        classification[\"signals\"].append(\"No obvious failure pattern detected\")\n",
    "        classification[\"recommended_action\"] = \"Review manually or collect more data\"\n",
    "\n",
    "    return classification\n",
    "\n",
    "# Classify failures in our evaluation data\n",
    "failure_classifications = []\n",
    "\n",
    "for _, row in eval_results_df.iterrows():\n",
    "    classification = classify_failure(\n",
    "        question=row['question'],\n",
    "        context=row['context'],\n",
    "        answer=row['output'],\n",
    "        ground_truth=row['ground_truth'],\n",
    "        groundedness_score=row.get('groundedness_score', 3),\n",
    "        correctness_score=row.get('correctness_score', 3)\n",
    "    )\n",
    "    classification['question'] = row['question'][:40] + \"...\"\n",
    "    failure_classifications.append(classification)\n",
    "\n",
    "failure_df = pd.DataFrame(failure_classifications)\n",
    "\n",
    "# Summary\n",
    "print(\"ðŸ” Failure Classification Summary:\")\n",
    "print(f\"\\n   Failure Type Distribution:\")\n",
    "for failure_type in failure_df['failure_type'].unique():\n",
    "    count = len(failure_df[failure_df['failure_type'] == failure_type])\n",
    "    pct = count / len(failure_df) * 100\n",
    "    print(f\"   - {failure_type}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Show examples of each failure type\n",
    "print(\"\\nðŸ“‹ Example Failures by Type:\")\n",
    "for failure_type in ['RETRIEVAL_FAILURE', 'MODEL_FAILURE', 'PROMPT_FAILURE']:\n",
    "    examples = failure_df[failure_df['failure_type'] == failure_type].head(1)\n",
    "    if len(examples) > 0:\n",
    "        ex = examples.iloc[0]\n",
    "        print(f\"\\n   {failure_type}:\")\n",
    "        print(f\"   Question: {ex['question']}\")\n",
    "        print(f\"   Signals: {', '.join(ex['signals'])}\")\n",
    "        print(f\"   Action: {ex['recommended_action']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efc8c40a-9328-42fc-820f-c2cf833f2f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 5: Monitoring Dashboards and Agent Traces\n",
    "\n",
    "In this section, we will:\n",
    "1. Create monitoring dashboards for key metrics\n",
    "2. Implement agent tracing for multi-step workflows\n",
    "3. Analyze trace data to identify bottlenecks\n",
    "4. Set up baseline-aware monitoring\n",
    "\n",
    "Effective monitoring requires both real-time dashboards and detailed traces for debugging.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f377dfe-d7d8-4943-b60f-ab6fc9220583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5.1: Create Monitoring Dashboard Metrics\n",
    "\n",
    "We'll define key metrics for our monitoring dashboard following Chapter 8 best practices:\n",
    "\n",
    "- **Latency metrics**: P50, P95, P99 response times\n",
    "- **Quality metrics**: Groundedness, correctness scores\n",
    "- **Cost metrics**: Token usage, request volume\n",
    "- **Error metrics**: Error rates, timeout rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e2022dc-193c-487d-9a36-0d9ca4c969c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MONITORING DASHBOARD METRICS\n",
    "# =============================================================================\n",
    "# Define and calculate key monitoring metrics\n",
    "\n",
    "def calculate_monitoring_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive monitoring metrics from traffic data.\n",
    "    These metrics form the basis of our monitoring dashboard.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        # Latency Metrics\n",
    "        \"latency_p50\": df['latency_ms'].median(),\n",
    "        \"latency_p95\": df['latency_ms'].quantile(0.95),\n",
    "        \"latency_p99\": df['latency_ms'].quantile(0.99),\n",
    "        \"latency_mean\": df['latency_ms'].mean(),\n",
    "        \"latency_std\": df['latency_ms'].std(),\n",
    "\n",
    "        # Volume Metrics\n",
    "        \"total_requests\": len(df),\n",
    "        \"requests_per_hour\": len(df) / 168,  # 7 days * 24 hours\n",
    "        \"success_rate\": (df['status'] == 'success').mean(),\n",
    "        \"error_rate\": (df['status'] == 'error').mean(),\n",
    "\n",
    "        # Token Metrics\n",
    "        \"avg_input_tokens\": df['input_tokens'].mean(),\n",
    "        \"avg_output_tokens\": df['output_tokens'].mean(),\n",
    "        \"avg_total_tokens\": df['total_tokens'].mean(),\n",
    "        \"total_tokens_consumed\": df['total_tokens'].sum(),\n",
    "\n",
    "        # Quality Metrics (from our synthetic data)\n",
    "        \"avg_quality_score\": df['quality_score'].mean(),\n",
    "        \"quality_below_threshold\": (df['quality_score'] < 0.7).mean(),\n",
    "\n",
    "        # Complexity Distribution\n",
    "        \"pct_simple\": (df['complexity'] == 'simple').mean(),\n",
    "        \"pct_moderate\": (df['complexity'] == 'moderate').mean(),\n",
    "        \"pct_complex\": (df['complexity'] == 'complex').mean(),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Calculate current metrics\n",
    "current_metrics = calculate_monitoring_metrics(traffic_analysis_df)\n",
    "\n",
    "print(\"ðŸ“Š Current Monitoring Metrics:\")\n",
    "print(\"\\n   Latency Metrics:\")\n",
    "print(f\"      P50: {current_metrics['latency_p50']:.2f} ms\")\n",
    "print(f\"      P95: {current_metrics['latency_p95']:.2f} ms\")\n",
    "print(f\"      P99: {current_metrics['latency_p99']:.2f} ms\")\n",
    "\n",
    "print(\"\\n   Volume Metrics:\")\n",
    "print(f\"      Total Requests: {current_metrics['total_requests']}\")\n",
    "print(f\"      Success Rate: {current_metrics['success_rate']*100:.1f}%\")\n",
    "\n",
    "print(\"\\n   Token Metrics:\")\n",
    "print(f\"      Avg Total Tokens: {current_metrics['avg_total_tokens']:.1f}\")\n",
    "print(f\"      Total Consumed: {current_metrics['total_tokens_consumed']:,}\")\n",
    "\n",
    "print(\"\\n   Quality Metrics:\")\n",
    "print(f\"      Avg Quality Score: {current_metrics['avg_quality_score']:.3f}\")\n",
    "print(f\"      Below Threshold: {current_metrics['quality_below_threshold']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66c92284-ce5c-4e1d-bb3d-93056d10e700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5.2: Implement Agent Tracing\n",
    "\n",
    "For multi-step agent workflows, we need detailed tracing to understand:\n",
    "- Which steps take the longest\n",
    "- Where errors occur\n",
    "- How context flows between steps\n",
    "\n",
    "We'll use MLflow's tracing capabilities to instrument our Q&A workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1929b180-b50a-428a-8f3a-3aba2cfd2d8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# =============================================================================\n",
    "# AGENT TRACING IMPLEMENTATION\n",
    "# =============================================================================\n",
    "# Implement manual tracing for multi-step Q&A workflow\n",
    "# We'll track timing and metrics for each step without relying on decorators\n",
    "\n",
    "class TracedQAAgent:\n",
    "    \"\"\"\n",
    "    A traced Q&A agent that logs detailed execution information.\n",
    "    This simulates a multi-step agent workflow with:\n",
    "    1. Query preprocessing\n",
    "    2. Context retrieval\n",
    "    3. LLM inference\n",
    "    4. Response post-processing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, endpoint_name, client):\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.client = client\n",
    "        self.trace_log = []\n",
    "\n",
    "    def preprocess_query(self, question):\n",
    "        \"\"\"Step 1: Preprocess the user query\"\"\"\n",
    "        start = time.time()\n",
    "\n",
    "        # Simulate preprocessing (normalization, spell check, etc.)\n",
    "        processed = question.strip().lower()\n",
    "        processed = ' '.join(processed.split())  # Normalize whitespace\n",
    "\n",
    "        latency = (time.time() - start) * 1000\n",
    "        return {\n",
    "            \"original\": question,\n",
    "            \"processed\": processed,\n",
    "            \"latency_ms\": latency\n",
    "        }\n",
    "\n",
    "    def retrieve_context(self, query, context):\n",
    "        \"\"\"Step 2: Retrieve relevant context\"\"\"\n",
    "        start = time.time()\n",
    "\n",
    "        # In production, this would query a vector database\n",
    "        # For this lab, we use the provided context\n",
    "        relevant_context = context\n",
    "\n",
    "        latency = (time.time() - start) * 1000\n",
    "        return {\n",
    "            \"context\": relevant_context,\n",
    "            \"context_tokens\": len(relevant_context.split()),\n",
    "            \"latency_ms\": latency\n",
    "        }\n",
    "\n",
    "    def generate_response(self, question, context):\n",
    "        \"\"\"Step 3: Generate response using LLM\"\"\"\n",
    "        start = time.time()\n",
    "\n",
    "        system_prompt = \"\"\"You are a helpful assistant. Answer based on the context provided.\"\"\"\n",
    "        user_prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.endpoint_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "            temperature=0.1\n",
    "        )\n",
    "\n",
    "        latency = (time.time() - start) * 1000\n",
    "        return {\n",
    "            \"response\": response.choices[0].message.content,\n",
    "            \"tokens\": response.usage.total_tokens,\n",
    "            \"latency_ms\": latency\n",
    "        }\n",
    "\n",
    "    def postprocess_response(self, response):\n",
    "        \"\"\"Step 4: Post-process the response\"\"\"\n",
    "        start = time.time()\n",
    "\n",
    "        # Simulate post-processing (formatting, safety checks, etc.)\n",
    "        processed = response.strip()\n",
    "\n",
    "        latency = (time.time() - start) * 1000\n",
    "        return {\n",
    "            \"final_response\": processed,\n",
    "            \"latency_ms\": latency\n",
    "        }\n",
    "\n",
    "    def run(self, question, context):\n",
    "        \"\"\"Execute the full Q&A pipeline with manual tracing\"\"\"\n",
    "        trace_id = str(uuid.uuid4())[:8]\n",
    "        trace_record = {\"trace_id\": trace_id, \"steps\": []}\n",
    "\n",
    "        # Step 1: Preprocess\n",
    "        preprocessed = self.preprocess_query(question)\n",
    "        trace_record[\"steps\"].append({\"step\": \"preprocess\", \"latency_ms\": preprocessed[\"latency_ms\"]})\n",
    "\n",
    "        # Step 2: Retrieve context\n",
    "        retrieved = self.retrieve_context(preprocessed[\"processed\"], context)\n",
    "        trace_record[\"steps\"].append({\"step\": \"retrieve\", \"latency_ms\": retrieved[\"latency_ms\"]})\n",
    "\n",
    "        # Step 3: Generate response\n",
    "        generated = self.generate_response(question, retrieved[\"context\"])\n",
    "        trace_record[\"steps\"].append({\"step\": \"generate\", \"latency_ms\": generated[\"latency_ms\"]})\n",
    "\n",
    "        # Step 4: Post-process\n",
    "        final = self.postprocess_response(generated[\"response\"])\n",
    "        trace_record[\"steps\"].append({\"step\": \"postprocess\", \"latency_ms\": final[\"latency_ms\"]})\n",
    "\n",
    "        # Calculate total latency\n",
    "        total_latency = sum(step[\"latency_ms\"] for step in trace_record[\"steps\"])\n",
    "        trace_record[\"total_latency_ms\"] = total_latency\n",
    "\n",
    "        # Store trace\n",
    "        self.trace_log.append(trace_record)\n",
    "\n",
    "        return {\n",
    "            \"trace_id\": trace_id,\n",
    "            \"answer\": final[\"final_response\"],\n",
    "            \"total_latency_ms\": total_latency,\n",
    "            \"step_latencies\": {\n",
    "                \"preprocess\": preprocessed[\"latency_ms\"],\n",
    "                \"retrieve\": retrieved[\"latency_ms\"],\n",
    "                \"generate\": generated[\"latency_ms\"],\n",
    "                \"postprocess\": final[\"latency_ms\"]\n",
    "            },\n",
    "            \"tokens\": generated[\"tokens\"]\n",
    "        }\n",
    "\n",
    "# Initialize traced agent\n",
    "traced_agent = TracedQAAgent(ENDPOINT_NAME, client)\n",
    "print(\"âœ… Traced Q&A Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d6160c1-6b73-45d3-a0d3-e06e5599aabb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Running Traced Queries\n",
    "\n",
    "**What we're doing:** Executing queries through our `TracedQAAgent` to collect detailed timing data for each step of the pipeline.\n",
    "\n",
    "**How the tracing works:**\n",
    "\n",
    "1. **Trace ID generation**: Each query gets a unique ID for correlation:\n",
    "   ```python\n",
    "   trace_id = str(uuid.uuid4())[:8]  # e.g., \"a1b2c3d4\"\n",
    "   ```\n",
    "\n",
    "2. **Step-by-step timing**: Each pipeline step is timed:\n",
    "   ```python\n",
    "   start = time.time()\n",
    "   result = self.preprocess_query(question)\n",
    "   latency_ms = (time.time() - start) * 1000\n",
    "   trace_record[\"steps\"].append({\"step\": \"preprocess\", \"latency_ms\": latency_ms})\n",
    "   ```\n",
    "\n",
    "3. **Pipeline steps traced**:\n",
    "   | Step | What it does | Typical time |\n",
    "   |------|-------------|--------------|\n",
    "   | preprocess | Clean/normalize question | 1-5ms |\n",
    "   | retrieve | Find relevant context | 5-20ms |\n",
    "   | generate | LLM API call | 200-2000ms |\n",
    "   | postprocess | Format response | 1-5ms |\n",
    "\n",
    "4. **Trace log storage**: All traces stored in `traced_agent.trace_log` for analysis\n",
    "\n",
    "**Why trace?** The LLM call typically dominates latency (>95%), but tracing helps identify when other steps become bottlenecks (e.g., slow retrieval from a vector database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daa08919-f98a-4662-8301-5149a95a6f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN TRACED QUERIES\n",
    "# =============================================================================\n",
    "# Execute queries with manual tracing\n",
    "\n",
    "print(\"ðŸ”„ Running traced queries...\")\n",
    "\n",
    "traced_results = []\n",
    "\n",
    "# Run a few traced queries\n",
    "for sample in eval_data[:5]:\n",
    "    try:\n",
    "        result = traced_agent.run(sample[\"question\"], sample[\"context\"])\n",
    "\n",
    "        traced_results.append({\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"complexity\": sample[\"complexity\"],\n",
    "            \"trace_id\": result[\"trace_id\"],\n",
    "            **result[\"step_latencies\"],\n",
    "            \"total_latency\": result[\"total_latency_ms\"],\n",
    "            \"tokens\": result[\"tokens\"]\n",
    "        })\n",
    "\n",
    "        print(f\"   âœ“ {sample['id']} [trace:{result['trace_id']}]: {result['total_latency_ms']:.0f}ms total\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— {sample['id']}: {str(e)}\")\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "# Analyze step latencies\n",
    "traced_df = pd.DataFrame(traced_results)\n",
    "\n",
    "print(f\"\\nðŸ“Š Step Latency Analysis:\")\n",
    "print(f\"   Preprocess:  {traced_df['preprocess'].mean():.2f}ms avg\")\n",
    "print(f\"   Retrieve:    {traced_df['retrieve'].mean():.2f}ms avg\")\n",
    "print(f\"   Generate:    {traced_df['generate'].mean():.2f}ms avg\")\n",
    "print(f\"   Postprocess: {traced_df['postprocess'].mean():.2f}ms avg\")\n",
    "print(f\"\\n   LLM generation accounts for {traced_df['generate'].mean()/traced_df['total_latency'].mean()*100:.1f}% of total latency\")\n",
    "\n",
    "# Display trace log from agent\n",
    "print(f\"\\nðŸ“‹ Trace Log ({len(traced_agent.trace_log)} traces recorded):\")\n",
    "for trace in traced_agent.trace_log[:3]:\n",
    "    print(f\"   Trace {trace['trace_id']}: {trace['total_latency_ms']:.0f}ms\")\n",
    "    for step in trace['steps']:\n",
    "        print(f\"      â””â”€ {step['step']}: {step['latency_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeb0f9c0-8807-4db5-a12a-92b1c196b65a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 6: Anomaly Detection and Alerting\n",
    "\n",
    "In this section, we will:\n",
    "1. Implement statistical anomaly detection\n",
    "2. Configure alert thresholds based on baselines\n",
    "3. Create alerting rules for production monitoring\n",
    "4. Apply calibrated alerting strategies from Chapter 8\n",
    "\n",
    "Effective alerting requires careful calibration to avoid alert fatigue while catching real issues.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "962ee5c5-a97b-45d2-820f-7a86cc5d15c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 6.1: Statistical Anomaly Detection\n",
    "\n",
    "We'll implement anomaly detection using statistical methods:\n",
    "- **Z-score detection**: Identify values beyond N standard deviations\n",
    "- **IQR method**: Detect outliers using interquartile range\n",
    "- **Rolling window analysis**: Detect trend changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "629b2ae7-fdcd-4488-bc75-3059cfb56fd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANOMALY DETECTION IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "class AnomalyDetector:\n",
    "    \"\"\"\n",
    "    Statistical anomaly detection for LLM monitoring metrics.\n",
    "    Implements multiple detection methods for robust alerting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, baseline_df):\n",
    "        \"\"\"Initialize with baseline data for threshold calculation.\"\"\"\n",
    "        self.baseline = baseline_df\n",
    "        self.thresholds = self._calculate_thresholds()\n",
    "\n",
    "    def _calculate_thresholds(self):\n",
    "        \"\"\"Calculate detection thresholds from baseline data.\"\"\"\n",
    "        latency = self.baseline['latency_ms']\n",
    "        tokens = self.baseline['total_tokens']\n",
    "\n",
    "        return {\n",
    "            'latency': {\n",
    "                'mean': latency.mean(),\n",
    "                'std': latency.std(),\n",
    "                'p95': latency.quantile(0.95),\n",
    "                'p99': latency.quantile(0.99),\n",
    "                'iqr_low': latency.quantile(0.25) - 1.5 * (latency.quantile(0.75) - latency.quantile(0.25)),\n",
    "                'iqr_high': latency.quantile(0.75) + 1.5 * (latency.quantile(0.75) - latency.quantile(0.25))\n",
    "            },\n",
    "            'tokens': {\n",
    "                'mean': tokens.mean(),\n",
    "                'std': tokens.std(),\n",
    "                'p95': tokens.quantile(0.95),\n",
    "                'p99': tokens.quantile(0.99)\n",
    "            },\n",
    "            'error_rate': {\n",
    "                'baseline': (self.baseline['status'] == 'error').mean(),\n",
    "                'threshold': max(0.05, (self.baseline['status'] == 'error').mean() * 2)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def detect_latency_anomaly(self, value, method='zscore', threshold=3):\n",
    "        \"\"\"Detect if a latency value is anomalous.\"\"\"\n",
    "        if method == 'zscore':\n",
    "            z = (value - self.thresholds['latency']['mean']) / self.thresholds['latency']['std']\n",
    "            return abs(z) > threshold, z\n",
    "        elif method == 'percentile':\n",
    "            return value > self.thresholds['latency']['p99'], value / self.thresholds['latency']['p99']\n",
    "        elif method == 'iqr':\n",
    "            return value > self.thresholds['latency']['iqr_high'], value / self.thresholds['latency']['iqr_high']\n",
    "\n",
    "    def detect_token_anomaly(self, value, threshold=2):\n",
    "        \"\"\"Detect if token usage is anomalous.\"\"\"\n",
    "        z = (value - self.thresholds['tokens']['mean']) / self.thresholds['tokens']['std']\n",
    "        return abs(z) > threshold, z\n",
    "\n",
    "    def detect_error_rate_anomaly(self, current_error_rate):\n",
    "        \"\"\"Detect if error rate exceeds threshold.\"\"\"\n",
    "        return current_error_rate > self.thresholds['error_rate']['threshold'], current_error_rate\n",
    "\n",
    "    def analyze_batch(self, df):\n",
    "        \"\"\"Analyze a batch of data for anomalies.\"\"\"\n",
    "        anomalies = {\n",
    "            'latency_anomalies': [],\n",
    "            'token_anomalies': [],\n",
    "            'error_rate_anomaly': False\n",
    "        }\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            is_latency_anomaly, score = self.detect_latency_anomaly(row['latency_ms'])\n",
    "            if is_latency_anomaly:\n",
    "                anomalies['latency_anomalies'].append({\n",
    "                    'request_id': row['request_id'],\n",
    "                    'latency_ms': row['latency_ms'],\n",
    "                    'z_score': score\n",
    "                })\n",
    "\n",
    "            is_token_anomaly, score = self.detect_token_anomaly(row['total_tokens'])\n",
    "            if is_token_anomaly:\n",
    "                anomalies['token_anomalies'].append({\n",
    "                    'request_id': row['request_id'],\n",
    "                    'total_tokens': row['total_tokens'],\n",
    "                    'z_score': score\n",
    "                })\n",
    "\n",
    "        # Check overall error rate\n",
    "        current_error_rate = (df['status'] == 'error').mean()\n",
    "        anomalies['error_rate_anomaly'], _ = self.detect_error_rate_anomaly(current_error_rate)\n",
    "        anomalies['current_error_rate'] = current_error_rate\n",
    "\n",
    "        return anomalies\n",
    "\n",
    "# Initialize detector with baseline data\n",
    "detector = AnomalyDetector(traffic_analysis_df)\n",
    "\n",
    "print(\"âœ… Anomaly Detector initialized with baseline thresholds:\")\n",
    "print(f\"\\n   Latency Thresholds:\")\n",
    "print(f\"      Mean: {detector.thresholds['latency']['mean']:.2f} ms\")\n",
    "print(f\"      Std: {detector.thresholds['latency']['std']:.2f} ms\")\n",
    "print(f\"      P95: {detector.thresholds['latency']['p95']:.2f} ms\")\n",
    "print(f\"      P99: {detector.thresholds['latency']['p99']:.2f} ms\")\n",
    "print(f\"\\n   Token Thresholds:\")\n",
    "print(f\"      Mean: {detector.thresholds['tokens']['mean']:.1f}\")\n",
    "print(f\"      P95: {detector.thresholds['tokens']['p95']:.1f}\")\n",
    "print(f\"\\n   Error Rate Threshold: {detector.thresholds['error_rate']['threshold']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f738d564-737b-4aac-b7f8-dd715a9510f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Running Anomaly Detection\n",
    "\n",
    "**What we're doing:** Scanning our traffic data to find requests that deviate significantly from normal patterns.\n",
    "\n",
    "**How the `analyze_batch()` method works:**\n",
    "\n",
    "1. **Iterate through each request**:\n",
    "   ```python\n",
    "   for idx, row in df.iterrows():\n",
    "       is_anomaly, z_score = self.detect_latency_anomaly(row['latency_ms'])\n",
    "   ```\n",
    "\n",
    "2. **Z-score calculation** for each metric:\n",
    "   ```python\n",
    "   z_score = (value - mean) / std_dev\n",
    "   # If z_score > 3, the value is 3+ standard deviations from mean\n",
    "   ```\n",
    "\n",
    "3. **Anomaly types detected**:\n",
    "\n",
    "   | Type | Detection Method | Threshold |\n",
    "   |------|-----------------|-----------|\n",
    "   | Latency | Z-score > 3 | ~99.7% of normal data is below this |\n",
    "   | Tokens | Z-score > 2 | ~95% of normal data is below this |\n",
    "   | Error Rate | Rate > 5% | Fixed threshold based on SLA |\n",
    "\n",
    "4. **Output structure**:\n",
    "   ```python\n",
    "   {\n",
    "       'latency_anomalies': [{'request_id': 'x', 'latency_ms': 5000, 'z_score': 4.2}],\n",
    "       'token_anomalies': [...],\n",
    "       'error_rate_anomaly': True/False,\n",
    "       'current_error_rate': 0.03\n",
    "   }\n",
    "   ```\n",
    "\n",
    "**Why Z-scores?** They're distribution-agnostic and automatically adapt to your baseline. A Z-score of 3 means \"this is extremely unusual\" regardless of whether your mean latency is 100ms or 1000ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee50522-ea95-4d8c-9464-1bd1eadd4283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN ANOMALY DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "# Analyze our traffic data for anomalies\n",
    "anomalies = detector.analyze_batch(traffic_analysis_df)\n",
    "\n",
    "print(\"ðŸ” Anomaly Detection Results:\")\n",
    "print(f\"\\n   Latency Anomalies: {len(anomalies['latency_anomalies'])}\")\n",
    "if anomalies['latency_anomalies']:\n",
    "    print(\"   Top 5 latency anomalies:\")\n",
    "    for a in sorted(anomalies['latency_anomalies'], key=lambda x: x['z_score'], reverse=True)[:5]:\n",
    "        print(f\"      {a['request_id']}: {a['latency_ms']:.0f}ms (z={a['z_score']:.2f})\")\n",
    "\n",
    "print(f\"\\n   Token Anomalies: {len(anomalies['token_anomalies'])}\")\n",
    "if anomalies['token_anomalies']:\n",
    "    print(\"   Top 5 token anomalies:\")\n",
    "    for a in sorted(anomalies['token_anomalies'], key=lambda x: x['z_score'], reverse=True)[:5]:\n",
    "        print(f\"      {a['request_id']}: {a['total_tokens']} tokens (z={a['z_score']:.2f})\")\n",
    "\n",
    "print(f\"\\n   Error Rate Anomaly: {'âš ï¸ YES' if anomalies['error_rate_anomaly'] else 'âœ… NO'}\")\n",
    "print(f\"   Current Error Rate: {anomalies['current_error_rate']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9de6a77c-35d1-4639-bb72-ae2b471ea354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 6.2: Configure Alert Rules\n",
    "\n",
    "Following Chapter 8 best practices for calibrated alerting:\n",
    "- **Severity levels**: Critical, Warning, Info\n",
    "- **Cooldown periods**: Prevent alert storms\n",
    "- **Aggregation windows**: Reduce noise from transient spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b9fe09d-38d4-4e4c-9f69-7f91a52b3108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ALERT CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class AlertManager:\n",
    "    \"\"\"\n",
    "    Alert management system with calibrated thresholds and cooldowns.\n",
    "    Implements Chapter 8 best practices for production alerting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, detector):\n",
    "        self.detector = detector\n",
    "        self.alert_history = []\n",
    "        self.cooldown_periods = {\n",
    "            'critical': 300,   # 5 minutes\n",
    "            'warning': 900,    # 15 minutes\n",
    "            'info': 3600       # 1 hour\n",
    "        }\n",
    "\n",
    "        # Define alert rules\n",
    "        self.rules = {\n",
    "            'latency_critical': {\n",
    "                'condition': lambda m: m['latency_p99'] > detector.thresholds['latency']['p99'] * 2,\n",
    "                'severity': 'critical',\n",
    "                'message': 'P99 latency exceeds 2x baseline'\n",
    "            },\n",
    "            'latency_warning': {\n",
    "                'condition': lambda m: m['latency_p95'] > detector.thresholds['latency']['p95'] * 1.5,\n",
    "                'severity': 'warning',\n",
    "                'message': 'P95 latency exceeds 1.5x baseline'\n",
    "            },\n",
    "            'error_rate_critical': {\n",
    "                'condition': lambda m: m['error_rate'] > 0.10,\n",
    "                'severity': 'critical',\n",
    "                'message': 'Error rate exceeds 10%'\n",
    "            },\n",
    "            'error_rate_warning': {\n",
    "                'condition': lambda m: m['error_rate'] > 0.05,\n",
    "                'severity': 'warning',\n",
    "                'message': 'Error rate exceeds 5%'\n",
    "            },\n",
    "            'token_spike': {\n",
    "                'condition': lambda m: m['avg_total_tokens'] > detector.thresholds['tokens']['p95'],\n",
    "                'severity': 'warning',\n",
    "                'message': 'Average token usage exceeds P95 baseline'\n",
    "            },\n",
    "            'quality_degradation': {\n",
    "                'condition': lambda m: m.get('avg_quality_score', 1) < 0.7,\n",
    "                'severity': 'warning',\n",
    "                'message': 'Average quality score below threshold'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def evaluate_rules(self, metrics):\n",
    "        \"\"\"Evaluate all alert rules against current metrics.\"\"\"\n",
    "        triggered_alerts = []\n",
    "\n",
    "        for rule_name, rule in self.rules.items():\n",
    "            try:\n",
    "                if rule['condition'](metrics):\n",
    "                    triggered_alerts.append({\n",
    "                        'rule': rule_name,\n",
    "                        'severity': rule['severity'],\n",
    "                        'message': rule['message'],\n",
    "                        'timestamp': datetime.now()\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating rule {rule_name}: {e}\")\n",
    "\n",
    "        return triggered_alerts\n",
    "\n",
    "    def should_alert(self, alert):\n",
    "        \"\"\"Check if alert should fire based on cooldown.\"\"\"\n",
    "        cooldown = self.cooldown_periods[alert['severity']]\n",
    "\n",
    "        # Check recent alerts of same type\n",
    "        for hist in self.alert_history:\n",
    "            if hist['rule'] == alert['rule']:\n",
    "                time_diff = (alert['timestamp'] - hist['timestamp']).total_seconds()\n",
    "                if time_diff < cooldown:\n",
    "                    return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def process_alerts(self, metrics):\n",
    "        \"\"\"Process metrics and generate alerts.\"\"\"\n",
    "        triggered = self.evaluate_rules(metrics)\n",
    "        fired_alerts = []\n",
    "\n",
    "        for alert in triggered:\n",
    "            if self.should_alert(alert):\n",
    "                fired_alerts.append(alert)\n",
    "                self.alert_history.append(alert)\n",
    "\n",
    "        return fired_alerts\n",
    "\n",
    "# Initialize alert manager\n",
    "alert_manager = AlertManager(detector)\n",
    "\n",
    "# Evaluate current metrics\n",
    "alerts = alert_manager.process_alerts(current_metrics)\n",
    "\n",
    "print(\"ðŸš¨ Alert Evaluation Results:\")\n",
    "if alerts:\n",
    "    for alert in alerts:\n",
    "        severity_icon = {'critical': 'ðŸ”´', 'warning': 'ðŸŸ¡', 'info': 'ðŸ”µ'}\n",
    "        print(f\"   {severity_icon[alert['severity']]} [{alert['severity'].upper()}] {alert['message']}\")\n",
    "else:\n",
    "    print(\"   âœ… No alerts triggered - all metrics within normal range\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Configured Alert Rules:\")\n",
    "for rule_name, rule in alert_manager.rules.items():\n",
    "    print(f\"   - {rule_name} ({rule['severity']}): {rule['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d82f1382-2e0f-4ad7-9fac-8c53dc86c4e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 7: Cost Optimization Strategies\n",
    "\n",
    "In this section, we will:\n",
    "1. Analyze token usage patterns for cost drivers\n",
    "2. Implement prompt optimization techniques\n",
    "3. Apply context control strategies\n",
    "4. Validate improvements with evaluation runs\n",
    "\n",
    "Cost optimization is critical for sustainable LLM operations at scale.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05d37d12-4a21-4650-96bc-231f7b7a4ff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 7.1: Analyze Cost Drivers\n",
    "\n",
    "We'll identify the main contributors to token usage and cost:\n",
    "- Long context documents\n",
    "- Verbose prompts\n",
    "- Unnecessarily detailed responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d44aa10-b41f-4fa3-a863-4c43c8a86c3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COST ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Analyze token usage patterns\n",
    "print(\"ðŸ’° Cost Analysis:\")\n",
    "\n",
    "# Token usage by complexity\n",
    "token_by_complexity = traffic_analysis_df.groupby('complexity').agg({\n",
    "    'input_tokens': 'mean',\n",
    "    'output_tokens': 'mean',\n",
    "    'total_tokens': ['mean', 'sum']\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\n   Token Usage by Complexity:\")\n",
    "print(token_by_complexity)\n",
    "\n",
    "# Token usage by category\n",
    "token_by_category = traffic_analysis_df.groupby('category').agg({\n",
    "    'total_tokens': ['mean', 'sum', 'count']\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\n   Token Usage by Category:\")\n",
    "print(token_by_category)\n",
    "\n",
    "# Identify high-cost queries\n",
    "high_cost_threshold = traffic_analysis_df['total_tokens'].quantile(0.9)\n",
    "high_cost_queries = traffic_analysis_df[traffic_analysis_df['total_tokens'] > high_cost_threshold]\n",
    "\n",
    "print(f\"\\n   High-Cost Queries (>{high_cost_threshold:.0f} tokens): {len(high_cost_queries)}\")\n",
    "print(f\"   These represent {len(high_cost_queries)/len(traffic_analysis_df)*100:.1f}% of requests\")\n",
    "print(f\"   But consume {high_cost_queries['total_tokens'].sum()/traffic_analysis_df['total_tokens'].sum()*100:.1f}% of tokens\")\n",
    "\n",
    "# Estimate costs (using approximate pricing)\n",
    "COST_PER_1K_INPUT_TOKENS = 0.0015  # Example pricing\n",
    "COST_PER_1K_OUTPUT_TOKENS = 0.002\n",
    "\n",
    "total_input_tokens = traffic_analysis_df['input_tokens'].sum()\n",
    "total_output_tokens = traffic_analysis_df['output_tokens'].sum()\n",
    "\n",
    "estimated_cost = (total_input_tokens / 1000 * COST_PER_1K_INPUT_TOKENS +\n",
    "                  total_output_tokens / 1000 * COST_PER_1K_OUTPUT_TOKENS)\n",
    "\n",
    "print(f\"\\n   Estimated Cost for {len(traffic_analysis_df)} requests: ${estimated_cost:.2f}\")\n",
    "print(f\"   Average Cost per Request: ${estimated_cost/len(traffic_analysis_df)*1000:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0de906a-bd8e-44a6-9cd4-d1302a0dd902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 7.2: Implement Prompt Optimization\n",
    "\n",
    "We'll implement several prompt optimization techniques:\n",
    "1. **Context truncation**: Limit context to relevant portions\n",
    "2. **Prompt compression**: Remove redundant instructions\n",
    "3. **Response length control**: Set appropriate max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc636d4e-2934-4262-a988-e548027054a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PROMPT OPTIMIZATION TECHNIQUES\n",
    "# =============================================================================\n",
    "\n",
    "class PromptOptimizer:\n",
    "    \"\"\"\n",
    "    Implements prompt optimization strategies for cost reduction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_context_tokens=200, max_response_tokens=150):\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "        self.max_response_tokens = max_response_tokens\n",
    "\n",
    "    def truncate_context(self, context, max_tokens=None):\n",
    "        \"\"\"Truncate context to maximum token limit.\"\"\"\n",
    "        max_tokens = max_tokens or self.max_context_tokens\n",
    "        words = context.split()\n",
    "\n",
    "        # Approximate: 1 token â‰ˆ 0.75 words\n",
    "        max_words = int(max_tokens * 0.75)\n",
    "\n",
    "        if len(words) <= max_words:\n",
    "            return context, False\n",
    "\n",
    "        truncated = ' '.join(words[:max_words]) + '...'\n",
    "        return truncated, True\n",
    "\n",
    "    def compress_prompt(self, system_prompt, user_prompt):\n",
    "        \"\"\"Compress prompts by removing redundancy.\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        system_prompt = ' '.join(system_prompt.split())\n",
    "        user_prompt = ' '.join(user_prompt.split())\n",
    "\n",
    "        # Use shorter system prompt\n",
    "        compressed_system = \"Answer questions based on the context. Be concise.\"\n",
    "\n",
    "        return compressed_system, user_prompt\n",
    "\n",
    "    def optimize_query(self, question, context):\n",
    "        \"\"\"Apply all optimization techniques.\"\"\"\n",
    "        # Truncate context\n",
    "        truncated_context, was_truncated = self.truncate_context(context)\n",
    "\n",
    "        # Create optimized prompts\n",
    "        system_prompt = \"Answer based on context. Be concise and accurate.\"\n",
    "        user_prompt = f\"Context: {truncated_context}\\n\\nQ: {question}\\n\\nA:\"\n",
    "\n",
    "        return {\n",
    "            'system_prompt': system_prompt,\n",
    "            'user_prompt': user_prompt,\n",
    "            'context_truncated': was_truncated,\n",
    "            'max_tokens': self.max_response_tokens\n",
    "        }\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = PromptOptimizer(max_context_tokens=150, max_response_tokens=100)\n",
    "\n",
    "# Test optimization on a sample\n",
    "sample = eval_data[0]\n",
    "original_context_tokens = len(sample['context'].split())\n",
    "optimized = optimizer.optimize_query(sample['question'], sample['context'])\n",
    "\n",
    "print(\"ðŸ”§ Prompt Optimization Example:\")\n",
    "print(f\"\\n   Original context tokens: ~{original_context_tokens}\")\n",
    "print(f\"   Optimized context tokens: ~{len(optimized['user_prompt'].split())}\")\n",
    "print(f\"   Context truncated: {optimized['context_truncated']}\")\n",
    "print(f\"   Max response tokens: {optimized['max_tokens']}\")\n",
    "print(f\"\\n   Optimized system prompt: '{optimized['system_prompt']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32372448-1b9a-4d17-8086-05d117eda8a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Comparing Optimized vs Baseline Performance\n",
    "\n",
    "**What we're doing:** Running the same queries with optimized prompts and comparing token usage, latency, and answer quality against our baseline.\n",
    "\n",
    "**How the comparison works:**\n",
    "\n",
    "1. **Run both versions** on the same questions:\n",
    "   ```python\n",
    "   baseline_result = query_qa_endpoint(question, context)  # Full prompts\n",
    "   optimized_result = query_optimized(question, context, optimizer)  # Compressed\n",
    "   ```\n",
    "\n",
    "2. **Metrics compared**:\n",
    "   | Metric | Baseline | Optimized | Goal |\n",
    "   |--------|----------|-----------|------|\n",
    "   | Input tokens | ~300 | ~150 | 50% reduction |\n",
    "   | Output tokens | ~200 | ~100 | 50% reduction |\n",
    "   | Latency | ~500ms | ~350ms | Lower is better |\n",
    "   | Quality | 0.85 | 0.80+ | Minimal degradation |\n",
    "\n",
    "3. **Cost calculation**:\n",
    "   ```python\n",
    "   # Typical pricing: $0.002 per 1K tokens\n",
    "   baseline_cost = baseline_tokens * 0.002 / 1000\n",
    "   optimized_cost = optimized_tokens * 0.002 / 1000\n",
    "   savings = (baseline_cost - optimized_cost) / baseline_cost * 100\n",
    "   ```\n",
    "\n",
    "**The trade-off:** Aggressive optimization saves money but may reduce answer quality. The goal is finding the sweet spot where you save 30-50% on tokens while maintaining >95% of baseline quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50234ee0-0cbf-4e02-aeaa-28ed66dc45f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPARE OPTIMIZED VS BASELINE PERFORMANCE\n",
    "# =============================================================================\n",
    "\n",
    "def query_optimized(question, context, optimizer):\n",
    "    \"\"\"Query endpoint with optimized prompts.\"\"\"\n",
    "    opt = optimizer.optimize_query(question, context)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=ENDPOINT_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": opt['system_prompt']},\n",
    "            {\"role\": \"user\", \"content\": opt['user_prompt']}\n",
    "        ],\n",
    "        max_tokens=opt['max_tokens'],\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    latency_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.choices[0].message.content,\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"input_tokens\": response.usage.prompt_tokens,\n",
    "        \"output_tokens\": response.usage.completion_tokens,\n",
    "        \"total_tokens\": response.usage.total_tokens\n",
    "    }\n",
    "\n",
    "print(\"ðŸ”„ Comparing baseline vs optimized performance...\")\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for sample in eval_data[:5]:\n",
    "    try:\n",
    "        # Baseline query\n",
    "        baseline = query_qa_endpoint(sample[\"question\"], sample[\"context\"])\n",
    "\n",
    "        # Optimized query\n",
    "        optimized_result = query_optimized(sample[\"question\"], sample[\"context\"], optimizer)\n",
    "\n",
    "        comparison_results.append({\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"baseline_tokens\": baseline[\"total_tokens\"],\n",
    "            \"optimized_tokens\": optimized_result[\"total_tokens\"],\n",
    "            \"token_reduction\": baseline[\"total_tokens\"] - optimized_result[\"total_tokens\"],\n",
    "            \"baseline_latency\": baseline[\"latency_ms\"],\n",
    "            \"optimized_latency\": optimized_result[\"latency_ms\"]\n",
    "        })\n",
    "\n",
    "        print(f\"   âœ“ {sample['id']}: {baseline['total_tokens']} â†’ {optimized_result['total_tokens']} tokens\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— {sample['id']}: {str(e)}\")\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "# Analyze results\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(f\"\\nðŸ“Š Optimization Results:\")\n",
    "print(f\"   Average Token Reduction: {comparison_df['token_reduction'].mean():.1f} tokens ({comparison_df['token_reduction'].mean()/comparison_df['baseline_tokens'].mean()*100:.1f}%)\")\n",
    "print(f\"   Average Latency Change: {(comparison_df['optimized_latency'].mean() - comparison_df['baseline_latency'].mean()):.1f}ms\")\n",
    "\n",
    "# Estimate cost savings\n",
    "baseline_cost = comparison_df['baseline_tokens'].sum() / 1000 * 0.002\n",
    "optimized_cost = comparison_df['optimized_tokens'].sum() / 1000 * 0.002\n",
    "savings = baseline_cost - optimized_cost\n",
    "\n",
    "print(f\"\\n   Estimated Cost Savings: ${savings:.4f} for {len(comparison_df)} queries\")\n",
    "print(f\"   Projected Monthly Savings (10K queries/day): ${savings/len(comparison_df)*10000*30:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ba65989-0d10-424b-bcfa-27863406f6f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Part 8: Validation and Periodic Review Cycles\n",
    "\n",
    "In this section, we will:\n",
    "1. Validate optimizations with repeatable evaluation runs\n",
    "2. Establish baseline metrics for ongoing comparison\n",
    "3. Implement periodic review workflows\n",
    "4. Create evaluation versioning for reproducibility\n",
    "\n",
    "Following Chapter 8 best practices, we'll ensure our improvements are measurable and sustainable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96b54d5b-a631-4b89-9641-8269ce48045d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 8.1: Validate Optimizations\n",
    "\n",
    "We'll run a comprehensive evaluation to validate that our optimizations maintain quality while reducing costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2400aa5-59e5-44d4-be0c-6b70e4fe592a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VALIDATION EVALUATION RUN\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸ”„ Running validation evaluation...\")\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "for sample in eval_data:\n",
    "    try:\n",
    "        # Run optimized query\n",
    "        result = query_optimized(sample[\"question\"], sample[\"context\"], optimizer)\n",
    "\n",
    "        validation_results.append({\n",
    "            \"id\": sample[\"id\"],\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"context\": sample[\"context\"],\n",
    "            \"ground_truth\": sample[\"ground_truth\"],\n",
    "            \"output\": result[\"answer\"],\n",
    "            \"category\": sample[\"category\"],\n",
    "            \"complexity\": sample[\"complexity\"],\n",
    "            \"latency_ms\": result[\"latency_ms\"],\n",
    "            \"input_tokens\": result[\"input_tokens\"],\n",
    "            \"output_tokens\": result[\"output_tokens\"],\n",
    "            \"total_tokens\": result[\"total_tokens\"]\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— {sample['id']}: {str(e)}\")\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "\n",
    "print(f\"\\nâœ… Validation complete: {len(validation_df)} samples evaluated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d7e23c8-b31d-47bf-97c3-31d119ce84d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Logging Validation Results to MLflow\n",
    "\n",
    "**What we're doing:** Creating a new MLflow run for our optimized model to enable side-by-side comparison with the baseline.\n",
    "\n",
    "**How the comparison works in MLflow:**\n",
    "\n",
    "1. **Separate runs for each configuration**:\n",
    "   - `baseline_evaluation`: Original prompts, no optimization\n",
    "   - `optimized_validation`: Compressed prompts, token limits\n",
    "\n",
    "2. **Parameters logged** (what we changed):\n",
    "   ```python\n",
    "   mlflow.log_param(\"optimization_type\", \"prompt_compression\")\n",
    "   mlflow.log_param(\"max_context_tokens\", 150)  # Was unlimited\n",
    "   mlflow.log_param(\"max_response_tokens\", 100)  # Was unlimited\n",
    "   ```\n",
    "\n",
    "3. **Metrics logged using new GenAI API**:\n",
    "   ```python\n",
    "   # Run evaluation functions on each sample\n",
    "   g_score = evaluate_groundedness(question=..., context=..., answer=...)\n",
    "   c_score = evaluate_correctness(question=..., ground_truth=..., answer=...)\n",
    "\n",
    "   # Log aggregate metrics\n",
    "   mlflow.log_metric(\"groundedness_mean\", avg_groundedness)\n",
    "   mlflow.log_metric(\"answer_correctness_mean\", avg_correctness)\n",
    "   ```\n",
    "\n",
    "4. **MLflow UI comparison**:\n",
    "   - Navigate to Experiments â†’ Select both runs â†’ Compare\n",
    "   - See metrics side-by-side in charts\n",
    "   - Identify if quality dropped with optimization\n",
    "\n",
    "**Decision framework:**\n",
    "- If quality drops <5% and tokens drop >30% â†’ Accept optimization\n",
    "- If quality drops >10% â†’ Reject, try less aggressive settings\n",
    "- If quality unchanged â†’ Great! Deploy optimized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec834fe5-8143-4fe4-a0d5-833dc0919deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOG VALIDATION RUN TO MLFLOW\n",
    "# =============================================================================\n",
    "\n",
    "with mlflow.start_run(run_name=\"optimized_validation\") as run:\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"endpoint_name\", ENDPOINT_NAME)\n",
    "    mlflow.log_param(\"optimization_type\", \"prompt_compression\")\n",
    "    mlflow.log_param(\"max_context_tokens\", optimizer.max_context_tokens)\n",
    "    mlflow.log_param(\"max_response_tokens\", optimizer.max_response_tokens)\n",
    "    mlflow.log_param(\"num_samples\", len(validation_df))\n",
    "\n",
    "    # Run evaluation using new GenAI API scorers\n",
    "    print(\"ðŸ”„ Running MLflow evaluation on optimized results...\")\n",
    "    print(\"   (Using new MLflow 3.4+ GenAI API)\")\n",
    "\n",
    "    # Prepare data and run scorers\n",
    "    val_groundedness_scores = []\n",
    "    val_correctness_scores = []\n",
    "\n",
    "    for i, row in validation_df.iterrows():\n",
    "        try:\n",
    "            # Use the evaluation functions with explicit parameters\n",
    "            g_score = evaluate_groundedness(\n",
    "                question=row[\"question\"],\n",
    "                context=row[\"context\"],\n",
    "                answer=row[\"output\"]\n",
    "            )\n",
    "            c_score = evaluate_correctness(\n",
    "                question=row[\"question\"],\n",
    "                ground_truth=row[\"ground_truth\"],\n",
    "                answer=row[\"output\"]\n",
    "            )\n",
    "\n",
    "            val_groundedness_scores.append(g_score)\n",
    "            val_correctness_scores.append(c_score)\n",
    "        except Exception as e:\n",
    "            print(f\"   Warning: Error evaluating sample {i}: {e}\")\n",
    "            val_groundedness_scores.append(3)\n",
    "            val_correctness_scores.append(3)\n",
    "        time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "    # Calculate and log aggregate metrics\n",
    "    avg_groundedness = sum(val_groundedness_scores) / len(val_groundedness_scores)\n",
    "    avg_correctness = sum(val_correctness_scores) / len(val_correctness_scores)\n",
    "\n",
    "    mlflow.log_metric(\"groundedness_mean\", avg_groundedness)\n",
    "    mlflow.log_metric(\"answer_correctness_mean\", avg_correctness)\n",
    "\n",
    "    # Log performance metrics\n",
    "    mlflow.log_metric(\"avg_latency_ms\", validation_df[\"latency_ms\"].mean())\n",
    "    mlflow.log_metric(\"p95_latency_ms\", validation_df[\"latency_ms\"].quantile(0.95))\n",
    "    mlflow.log_metric(\"avg_total_tokens\", validation_df[\"total_tokens\"].mean())\n",
    "    mlflow.log_metric(\"total_tokens_consumed\", validation_df[\"total_tokens\"].sum())\n",
    "\n",
    "    # Add scores to dataframe\n",
    "    validation_df[\"groundedness_score\"] = val_groundedness_scores\n",
    "    validation_df[\"correctness_score\"] = val_correctness_scores\n",
    "\n",
    "    # Log artifacts\n",
    "    validation_df.to_csv(\"/tmp/validation_results.csv\", index=False)\n",
    "    mlflow.log_artifact(\"/tmp/validation_results.csv\")\n",
    "\n",
    "    print(f\"\\nâœ… Validation logged to MLflow\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")\n",
    "\n",
    "    # Compare with baseline\n",
    "    print(f\"\\nðŸ“Š Validation Metrics:\")\n",
    "    print(f\"   groundedness_mean: {avg_groundedness:.2f}\")\n",
    "    print(f\"   answer_correctness_mean: {avg_correctness:.2f}\")\n",
    "    print(f\"   avg_latency_ms: {validation_df['latency_ms'].mean():.2f}\")\n",
    "    print(f\"   avg_total_tokens: {validation_df['total_tokens'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b33e97cc-b872-4c87-addc-672b67b8deff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 8.1.5: Making the Deployment Decision\n",
    "\n",
    "**Learning Objective 2:** Compare multiple model or prompt configurations using MLflow evaluation metrics and **justify a deployment choice**.\n",
    "\n",
    "After running evaluations on both baseline and optimized configurations, we need a structured framework to make deployment decisions. This ensures decisions are data-driven, documented, and defensible.\n",
    "\n",
    "---\n",
    "\n",
    "#### Deployment Decision Framework\n",
    "\n",
    "| Criterion | Threshold | Weight | Rationale |\n",
    "|-----------|-----------|--------|-----------|\n",
    "| **Quality (Correctness)** | Max 5% degradation | 40% | User experience is paramount |\n",
    "| **Groundedness** | Max 10% degradation | 25% | RAG systems must stay grounded |\n",
    "| **Latency (P95)** | Max 20% increase | 20% | Performance affects UX |\n",
    "| **Cost (Tokens)** | Min 20% reduction | 15% | Must justify optimization effort |\n",
    "\n",
    "---\n",
    "\n",
    "#### Decision Matrix\n",
    "\n",
    "| Scenario | Quality Î” | Cost Î” | Decision | Justification |\n",
    "|----------|-----------|--------|----------|---------------|\n",
    "| Win-Win | â‰¤0% | â‰¥-30% | âœ… **DEPLOY** | Quality maintained, significant savings |\n",
    "| Acceptable Trade-off | -5% | â‰¥-40% | âœ… **DEPLOY** | Minor quality loss offset by major savings |\n",
    "| Marginal | -5% to -10% | -20% to -30% | âš ï¸ **REVIEW** | Needs stakeholder approval |\n",
    "| Unacceptable | >-10% | Any | âŒ **REJECT** | Quality degradation too high |\n",
    "| No Benefit | â‰¤0% | <-10% | âŒ **REJECT** | Insufficient cost savings |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc7037d-93d5-4a6a-b7f9-91fcd7135c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEPLOYMENT DECISION FRAMEWORK\n",
    "# =============================================================================\n",
    "# Compare baseline vs optimized and make data-driven deployment decision\n",
    "\n",
    "def make_deployment_decision(baseline_metrics: dict, optimized_metrics: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate whether to deploy the optimized configuration.\n",
    "\n",
    "    Returns decision with justification and confidence score.\n",
    "    \"\"\"\n",
    "    decision = {\n",
    "        \"recommendation\": None,\n",
    "        \"confidence\": 0.0,\n",
    "        \"justification\": [],\n",
    "        \"metrics_comparison\": {},\n",
    "        \"risk_assessment\": None\n",
    "    }\n",
    "\n",
    "    # Calculate deltas (negative = degradation for quality, improvement for cost)\n",
    "    quality_delta = (\n",
    "        (optimized_metrics.get('correctness_mean', 3) - baseline_metrics.get('correctness_mean', 3))\n",
    "        / max(baseline_metrics.get('correctness_mean', 3), 1) * 100\n",
    "    )\n",
    "\n",
    "    groundedness_delta = (\n",
    "        (optimized_metrics.get('groundedness_mean', 3) - baseline_metrics.get('groundedness_mean', 3))\n",
    "        / max(baseline_metrics.get('groundedness_mean', 3), 1) * 100\n",
    "    )\n",
    "\n",
    "    latency_delta = (\n",
    "        (optimized_metrics.get('latency_p95', 500) - baseline_metrics.get('latency_p95', 500))\n",
    "        / max(baseline_metrics.get('latency_p95', 500), 1) * 100\n",
    "    )\n",
    "\n",
    "    cost_delta = (\n",
    "        (optimized_metrics.get('avg_tokens', 200) - baseline_metrics.get('avg_tokens', 200))\n",
    "        / max(baseline_metrics.get('avg_tokens', 200), 1) * 100\n",
    "    )\n",
    "\n",
    "    decision[\"metrics_comparison\"] = {\n",
    "        \"quality_change_pct\": quality_delta,\n",
    "        \"groundedness_change_pct\": groundedness_delta,\n",
    "        \"latency_change_pct\": latency_delta,\n",
    "        \"cost_change_pct\": cost_delta\n",
    "    }\n",
    "\n",
    "    # Apply decision rules\n",
    "    quality_ok = quality_delta >= -5\n",
    "    groundedness_ok = groundedness_delta >= -10\n",
    "    latency_ok = latency_delta <= 20\n",
    "    cost_benefit = cost_delta <= -20  # At least 20% reduction\n",
    "\n",
    "    # Decision logic\n",
    "    if quality_delta > -1 and cost_delta <= -30:\n",
    "        decision[\"recommendation\"] = \"DEPLOY\"\n",
    "        decision[\"confidence\"] = 0.95\n",
    "        decision[\"justification\"].append(\"Win-win: Quality maintained with significant cost savings (>30%)\")\n",
    "        decision[\"risk_assessment\"] = \"LOW\"\n",
    "\n",
    "    elif quality_ok and groundedness_ok and cost_delta <= -40:\n",
    "        decision[\"recommendation\"] = \"DEPLOY\"\n",
    "        decision[\"confidence\"] = 0.85\n",
    "        decision[\"justification\"].append(\"Acceptable trade-off: Minor quality impact offset by major savings (>40%)\")\n",
    "        decision[\"risk_assessment\"] = \"LOW-MEDIUM\"\n",
    "\n",
    "    elif quality_ok and groundedness_ok and cost_benefit:\n",
    "        decision[\"recommendation\"] = \"DEPLOY\"\n",
    "        decision[\"confidence\"] = 0.75\n",
    "        decision[\"justification\"].append(\"Good trade-off: Quality within tolerance, meaningful cost reduction\")\n",
    "        decision[\"risk_assessment\"] = \"MEDIUM\"\n",
    "\n",
    "    elif quality_delta < -10 or groundedness_delta < -15:\n",
    "        decision[\"recommendation\"] = \"REJECT\"\n",
    "        decision[\"confidence\"] = 0.90\n",
    "        decision[\"justification\"].append(\"Quality degradation exceeds acceptable threshold\")\n",
    "        decision[\"risk_assessment\"] = \"HIGH\"\n",
    "\n",
    "    elif not cost_benefit:\n",
    "        decision[\"recommendation\"] = \"REJECT\"\n",
    "        decision[\"confidence\"] = 0.80\n",
    "        decision[\"justification\"].append(\"Insufficient cost savings to justify optimization effort\")\n",
    "        decision[\"risk_assessment\"] = \"N/A\"\n",
    "\n",
    "    else:\n",
    "        decision[\"recommendation\"] = \"REVIEW\"\n",
    "        decision[\"confidence\"] = 0.60\n",
    "        decision[\"justification\"].append(\"Marginal case - requires stakeholder review\")\n",
    "        decision[\"risk_assessment\"] = \"MEDIUM\"\n",
    "\n",
    "    # Add specific metric observations\n",
    "    if quality_delta < 0:\n",
    "        decision[\"justification\"].append(f\"Quality decreased by {abs(quality_delta):.1f}%\")\n",
    "    else:\n",
    "        decision[\"justification\"].append(f\"Quality improved by {quality_delta:.1f}%\")\n",
    "\n",
    "    if cost_delta < 0:\n",
    "        decision[\"justification\"].append(f\"Token usage reduced by {abs(cost_delta):.1f}%\")\n",
    "\n",
    "    return decision\n",
    "\n",
    "# Calculate metrics from our evaluation runs\n",
    "baseline_metrics = {\n",
    "    \"correctness_mean\": eval_results_df.get('correctness_score', pd.Series([3]*len(eval_results_df))).mean(),\n",
    "    \"groundedness_mean\": eval_results_df.get('groundedness_score', pd.Series([3]*len(eval_results_df))).mean(),\n",
    "    \"latency_p95\": eval_results_df['latency_ms'].quantile(0.95),\n",
    "    \"avg_tokens\": eval_results_df['total_tokens'].mean()\n",
    "}\n",
    "\n",
    "optimized_metrics = {\n",
    "    \"correctness_mean\": validation_df.get('correctness_score', pd.Series([3]*len(validation_df))).mean(),\n",
    "    \"groundedness_mean\": validation_df.get('groundedness_score', pd.Series([3]*len(validation_df))).mean(),\n",
    "    \"latency_p95\": validation_df['latency_ms'].quantile(0.95),\n",
    "    \"avg_tokens\": validation_df['total_tokens'].mean()\n",
    "}\n",
    "\n",
    "# Make deployment decision\n",
    "deployment_decision = make_deployment_decision(baseline_metrics, optimized_metrics)\n",
    "\n",
    "# Display decision report\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸš€ DEPLOYMENT DECISION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "recommendation_icon = {\n",
    "    \"DEPLOY\": \"âœ…\",\n",
    "    \"REJECT\": \"âŒ\",\n",
    "    \"REVIEW\": \"âš ï¸\"\n",
    "}.get(deployment_decision[\"recommendation\"], \"â“\")\n",
    "\n",
    "print(f\"\\n{recommendation_icon} RECOMMENDATION: {deployment_decision['recommendation']}\")\n",
    "print(f\"   Confidence: {deployment_decision['confidence']*100:.0f}%\")\n",
    "print(f\"   Risk Level: {deployment_decision['risk_assessment']}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š METRICS COMPARISON:\")\n",
    "for metric, value in deployment_decision[\"metrics_comparison\"].items():\n",
    "    direction = \"â†‘\" if value > 0 else \"â†“\" if value < 0 else \"â†’\"\n",
    "    print(f\"   {metric}: {value:+.1f}% {direction}\")\n",
    "\n",
    "print(f\"\\nðŸ“ JUSTIFICATION:\")\n",
    "for i, reason in enumerate(deployment_decision[\"justification\"], 1):\n",
    "    print(f\"   {i}. {reason}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Log decision to MLflow\n",
    "with mlflow.start_run(run_name=\"deployment_decision\"):\n",
    "    mlflow.log_param(\"recommendation\", deployment_decision[\"recommendation\"])\n",
    "    mlflow.log_param(\"confidence\", deployment_decision[\"confidence\"])\n",
    "    mlflow.log_param(\"risk_assessment\", deployment_decision[\"risk_assessment\"])\n",
    "    for metric, value in deployment_decision[\"metrics_comparison\"].items():\n",
    "        mlflow.log_metric(metric, value)\n",
    "    print(\"âœ… Deployment decision logged to MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdae0310-2967-46cd-b03c-3c72a2966a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 8.2: Establish Review Cycle Workflow\n",
    "\n",
    "We'll create a structured review workflow that can be run periodically to:\n",
    "- Compare current performance against baselines\n",
    "- Detect drift in quality or performance\n",
    "- Generate actionable reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6667d900-b4f1-4ee7-9e91-22a399f64b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PERIODIC REVIEW WORKFLOW\n",
    "# =============================================================================\n",
    "\n",
    "class PeriodicReviewWorkflow:\n",
    "    \"\"\"\n",
    "    Implements a structured review workflow for LLM monitoring.\n",
    "    Following Chapter 8 best practices for periodic evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, baseline_metrics, alert_manager):\n",
    "        self.baseline = baseline_metrics\n",
    "        self.alert_manager = alert_manager\n",
    "        self.review_history = []\n",
    "\n",
    "    def run_review(self, current_df, review_name=\"periodic_review\"):\n",
    "        \"\"\"Execute a complete review cycle.\"\"\"\n",
    "        review_timestamp = datetime.now()\n",
    "\n",
    "        # Calculate current metrics\n",
    "        current_metrics = calculate_monitoring_metrics(current_df)\n",
    "\n",
    "        # Compare with baseline\n",
    "        comparison = self._compare_metrics(current_metrics)\n",
    "\n",
    "        # Check for alerts\n",
    "        alerts = self.alert_manager.evaluate_rules(current_metrics)\n",
    "\n",
    "        # Generate recommendations\n",
    "        recommendations = self._generate_recommendations(comparison, alerts)\n",
    "\n",
    "        # Create review report\n",
    "        report = {\n",
    "            \"review_name\": review_name,\n",
    "            \"timestamp\": review_timestamp,\n",
    "            \"current_metrics\": current_metrics,\n",
    "            \"baseline_comparison\": comparison,\n",
    "            \"alerts\": alerts,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"status\": \"HEALTHY\" if not alerts else \"NEEDS_ATTENTION\"\n",
    "        }\n",
    "\n",
    "        self.review_history.append(report)\n",
    "\n",
    "        return report\n",
    "\n",
    "    def _compare_metrics(self, current):\n",
    "        \"\"\"Compare current metrics against baseline.\"\"\"\n",
    "        comparison = {}\n",
    "\n",
    "        for key in ['latency_p50', 'latency_p95', 'avg_total_tokens', 'error_rate']:\n",
    "            if key in self.baseline and key in current:\n",
    "                baseline_val = self.baseline[key]\n",
    "                current_val = current[key]\n",
    "\n",
    "                if baseline_val > 0:\n",
    "                    pct_change = (current_val - baseline_val) / baseline_val * 100\n",
    "                else:\n",
    "                    pct_change = 0\n",
    "\n",
    "                comparison[key] = {\n",
    "                    \"baseline\": baseline_val,\n",
    "                    \"current\": current_val,\n",
    "                    \"change_pct\": pct_change,\n",
    "                    \"status\": \"OK\" if abs(pct_change) < 20 else \"DEGRADED\" if pct_change > 0 else \"IMPROVED\"\n",
    "                }\n",
    "\n",
    "        return comparison\n",
    "\n",
    "    def _generate_recommendations(self, comparison, alerts):\n",
    "        \"\"\"Generate actionable recommendations.\"\"\"\n",
    "        recommendations = []\n",
    "\n",
    "        # Check latency\n",
    "        if comparison.get('latency_p95', {}).get('status') == 'DEGRADED':\n",
    "            recommendations.append({\n",
    "                \"priority\": \"HIGH\",\n",
    "                \"area\": \"Latency\",\n",
    "                \"action\": \"Investigate latency increase. Consider scaling up or optimizing prompts.\"\n",
    "            })\n",
    "\n",
    "        # Check token usage\n",
    "        if comparison.get('avg_total_tokens', {}).get('status') == 'DEGRADED':\n",
    "            recommendations.append({\n",
    "                \"priority\": \"MEDIUM\",\n",
    "                \"area\": \"Cost\",\n",
    "                \"action\": \"Token usage increased. Review prompt templates and context lengths.\"\n",
    "            })\n",
    "\n",
    "        # Check error rate\n",
    "        if comparison.get('error_rate', {}).get('status') == 'DEGRADED':\n",
    "            recommendations.append({\n",
    "                \"priority\": \"HIGH\",\n",
    "                \"area\": \"Reliability\",\n",
    "                \"action\": \"Error rate increased. Check endpoint health and input validation.\"\n",
    "            })\n",
    "\n",
    "        # Add alert-based recommendations\n",
    "        for alert in alerts:\n",
    "            if alert['severity'] == 'critical':\n",
    "                recommendations.append({\n",
    "                    \"priority\": \"CRITICAL\",\n",
    "                    \"area\": alert['rule'],\n",
    "                    \"action\": f\"Address immediately: {alert['message']}\"\n",
    "                })\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "# Initialize review workflow\n",
    "review_workflow = PeriodicReviewWorkflow(current_metrics, alert_manager)\n",
    "\n",
    "# Run a review\n",
    "review_report = review_workflow.run_review(traffic_analysis_df, \"lab_final_review\")\n",
    "\n",
    "print(\"ðŸ“‹ Periodic Review Report\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Review: {review_report['review_name']}\")\n",
    "print(f\"   Timestamp: {review_report['timestamp']}\")\n",
    "print(f\"   Status: {review_report['status']}\")\n",
    "\n",
    "print(f\"\\n   Baseline Comparison:\")\n",
    "for metric, data in review_report['baseline_comparison'].items():\n",
    "    print(f\"      {metric}: {data['current']:.2f} ({data['change_pct']:+.1f}%) - {data['status']}\")\n",
    "\n",
    "if review_report['recommendations']:\n",
    "    print(f\"\\n   Recommendations:\")\n",
    "    for rec in review_report['recommendations']:\n",
    "        print(f\"      [{rec['priority']}] {rec['area']}: {rec['action']}\")\n",
    "else:\n",
    "    print(f\"\\n   âœ… No recommendations - system performing within expected parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a198673-1589-4230-a362-49355e102cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Generating the Review Report\n",
    "\n",
    "**What we're doing:** Creating a human-readable report that summarizes the review findings for stakeholders who may not be technical.\n",
    "\n",
    "**Report structure:**\n",
    "\n",
    "1. **Executive Summary**: High-level health status\n",
    "   - Overall health: HEALTHY / NEEDS ATTENTION / CRITICAL\n",
    "   - Review period and sample size\n",
    "   - One-line summary of key findings\n",
    "\n",
    "2. **Key Metrics Table**: Current vs baseline comparison\n",
    "   ```\n",
    "   âœ… latency_p50: 245ms (baseline: 240ms, change: +2.1%)\n",
    "   âš ï¸ latency_p95: 890ms (baseline: 720ms, change: +23.6%)\n",
    "   âœ… error_rate: 1.2% (baseline: 1.5%, change: -20.0%)\n",
    "   ```\n",
    "\n",
    "3. **Active Alerts**: Any triggered alert rules\n",
    "   - Severity level (CRITICAL/WARNING/INFO)\n",
    "   - Rule that triggered\n",
    "   - Specific message\n",
    "\n",
    "4. **Recommendations**: Prioritized action items\n",
    "   - CRITICAL: Address immediately\n",
    "   - HIGH: Address within 24 hours\n",
    "   - MEDIUM: Address within 1 week\n",
    "\n",
    "**Why structured reports matter (Chapter 8 best practice):**\n",
    "- Executives need summaries, not raw data\n",
    "- Consistent format enables trend tracking over time\n",
    "- Recommendations drive action, not just awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f23fa6-f3fb-43e0-a476-93e696fe4883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE REVIEW REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸ“‹ Generating Review Report...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PERIODIC REVIEW REPORT\")\n",
    "print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ“Š EXECUTIVE SUMMARY\")\n",
    "print(f\"   Review Period: Last 7 days\")\n",
    "print(f\"   Total Requests Analyzed: {len(traffic_analysis_df)}\")\n",
    "print(f\"   Overall Health: {'âœ… HEALTHY' if len(review_report['alerts']) == 0 else 'âš ï¸ NEEDS ATTENTION'}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ KEY METRICS\")\n",
    "for metric, data in review_report['baseline_comparison'].items():\n",
    "    status_icon = \"âœ…\" if data['status'] == 'ok' else \"âš ï¸\" if data['status'] == 'warning' else \"ðŸ”´\"\n",
    "    print(f\"   {status_icon} {metric}: {data['current']:.2f} (baseline: {data['baseline']:.2f}, change: {data['change_pct']:+.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸš¨ ALERTS ({len(review_report['alerts'])})\")\n",
    "if review_report['alerts']:\n",
    "    for alert in review_report['alerts']:\n",
    "        print(f\"   [{alert['severity'].upper()}] {alert['rule_name']}: {alert['message']}\")\n",
    "else:\n",
    "    print(\"   No active alerts\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ RECOMMENDATIONS ({len(review_report['recommendations'])})\")\n",
    "if review_report['recommendations']:\n",
    "    for i, rec in enumerate(review_report['recommendations'], 1):\n",
    "        print(f\"   {i}. [{rec['priority'].upper()}] {rec['area']}: {rec['action']}\")\n",
    "else:\n",
    "    print(\"   No recommendations - system performing optimally\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"END OF REPORT\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a64378ea-7df8-4712-b07e-18a445ae48ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 8.3: Create Evaluation Dataset Versioning\n",
    "\n",
    "**What we're doing:** Saving a timestamped version of our evaluation dataset so we can reproduce results and track changes over time.\n",
    "\n",
    "**How versioning works:**\n",
    "\n",
    "1. **Generate version string** from timestamp:\n",
    "   ```python\n",
    "   version = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "   # Result: \"20241130_143022\"\n",
    "   ```\n",
    "\n",
    "2. **Save versioned Delta table**:\n",
    "   ```python\n",
    "   versioned_table_name = f\"{CATALOG}.{SCHEMA}.evaluation_dataset_v{version}\"\n",
    "   spark_df.write.saveAsTable(versioned_table_name)\n",
    "   # Creates: main.llm_monitoring_lab.evaluation_dataset_v20241130_143022\n",
    "   ```\n",
    "\n",
    "3. **Log to MLflow** for discoverability:\n",
    "   ```python\n",
    "   mlflow.log_param(\"dataset_version\", version)\n",
    "   mlflow.log_param(\"dataset_table\", versioned_table_name)\n",
    "   mlflow.log_artifact(\"eval_dataset.csv\")  # Downloadable copy\n",
    "   ```\n",
    "\n",
    "**Why version datasets?**\n",
    "- **Reproducibility**: \"What data did we use for the March evaluation?\"\n",
    "- **Debugging**: \"Did the dataset change, or did the model regress?\"\n",
    "- **Compliance**: Audit trails for regulated industries\n",
    "- **A/B testing**: Compare model performance on identical datasets\n",
    "\n",
    "**Best practice:** Always version both the dataset AND the evaluation results together. Link them via MLflow run IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24feacc7-726d-4f3d-aaa8-330b51b58876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EVALUATION DATASET VERSIONING\n",
    "# =============================================================================\n",
    "\n",
    "# Save versioned evaluation dataset\n",
    "version = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "versioned_table_name = f\"{CATALOG}.{SCHEMA}.evaluation_dataset_v{version}\"\n",
    "\n",
    "# Save to Delta table with version\n",
    "eval_spark_df = spark.createDataFrame(eval_df)\n",
    "eval_spark_df.write.mode(\"overwrite\").saveAsTable(versioned_table_name)\n",
    "\n",
    "print(f\"âœ… Evaluation dataset versioned: {versioned_table_name}\")\n",
    "\n",
    "# Log version to MLflow\n",
    "with mlflow.start_run(run_name=f\"dataset_version_{version}\"):\n",
    "    mlflow.log_param(\"dataset_version\", version)\n",
    "    mlflow.log_param(\"dataset_table\", versioned_table_name)\n",
    "    mlflow.log_param(\"num_samples\", len(eval_df))\n",
    "    mlflow.log_param(\"categories\", list(eval_df['category'].unique()))\n",
    "\n",
    "    # Log dataset as artifact\n",
    "    eval_df.to_csv(f\"/tmp/eval_dataset_{version}.csv\", index=False)\n",
    "    mlflow.log_artifact(f\"/tmp/eval_dataset_{version}.csv\")\n",
    "\n",
    "    print(f\"   Logged to MLflow experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ed2da16-abbd-432a-bb52-829b0421ee5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Lab Summary and Key Takeaways\n",
    "\n",
    "## What You Accomplished\n",
    "\n",
    "In this hands-on lab, you successfully achieved all 7 learning objectives:\n",
    "\n",
    "### 1. âœ… Designed an End-to-End Evaluation and Monitoring Workflow\n",
    "- Set up MLflow experiment tracking for a production LLM system\n",
    "- Created synthetic evaluation datasets with ground truth\n",
    "- Established baseline metrics for ongoing comparison\n",
    "- Implemented periodic review cycles\n",
    "\n",
    "### 2. âœ… Compared Multiple Configurations and Justified Deployment Choice\n",
    "- Ran evaluations on baseline and optimized configurations\n",
    "- Applied the deployment decision framework with weighted criteria\n",
    "- Made data-driven deployment recommendations with justification\n",
    "- Logged decisions to MLflow for audit trail\n",
    "\n",
    "### 3. âœ… Analyzed Inference Tables for Latency, Tokens, and RAG Grounding\n",
    "- Queried inference tables to analyze production traffic patterns\n",
    "- Identified slow queries and latency anomalies\n",
    "- Performed RAG grounding analysis to detect hallucination signals\n",
    "- Correlated context quality with answer quality\n",
    "\n",
    "### 4. âœ… Distinguished Retrieval Failures from Model/Prompt Failures\n",
    "- Implemented failure classification framework\n",
    "- Diagnosed root causes: retrieval, model, or prompt issues\n",
    "- Generated actionable recommendations for each failure type\n",
    "- Used inference logs to identify patterns\n",
    "\n",
    "### 5. âœ… Used Agent Monitoring for Multi-Step Workflows\n",
    "- Implemented tracing for Q&A pipeline steps (preprocess, retrieve, generate, postprocess)\n",
    "- Identified bottlenecks in the workflow\n",
    "- Measured step-by-step latency contributions\n",
    "- Analyzed trace data to optimize performance\n",
    "\n",
    "### 6. âœ… Applied Cost Controls and Validated Without Quality Degradation\n",
    "- Analyzed token usage patterns and cost drivers\n",
    "- Implemented prompt compression and context truncation\n",
    "- Validated that optimizations maintained quality thresholds\n",
    "- Calculated projected cost savings\n",
    "\n",
    "### 7. âœ… Selected Appropriate Evaluation Judges and Ground Truth Requirements\n",
    "- Learned when to use LLM-as-judge vs ground truth comparison\n",
    "- Understood trade-offs between evaluation approaches\n",
    "- Applied the right evaluation strategy for RAG Q&A systems\n",
    "- Implemented custom scorers using MLflow's new GenAI API\n",
    "\n",
    "---\n",
    "\n",
    "## Key Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3783ea79-3412-4dd1-9d15-97f7012f0b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"           LAB COMPLETION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ“Š Evaluation Metrics:\")\n",
    "print(f\"   Samples Evaluated: {len(eval_df)}\")\n",
    "print(f\"   Categories: {list(eval_df['category'].unique())}\")\n",
    "\n",
    "print(f\"\\nâš¡ Performance Baseline:\")\n",
    "print(f\"   P50 Latency: {current_metrics['latency_p50']:.2f} ms\")\n",
    "print(f\"   P95 Latency: {current_metrics['latency_p95']:.2f} ms\")\n",
    "print(f\"   Avg Tokens: {current_metrics['avg_total_tokens']:.1f}\")\n",
    "\n",
    "print(f\"\\nðŸ’° Cost Optimization:\")\n",
    "if len(comparison_df) > 0:\n",
    "    print(f\"   Token Reduction: {comparison_df['token_reduction'].mean():.1f} tokens/query\")\n",
    "    print(f\"   Projected Monthly Savings: ${savings/len(comparison_df)*10000*30:.2f}\")\n",
    "\n",
    "print(f\"\\nðŸ” Anomaly Detection:\")\n",
    "print(f\"   Latency Anomalies Detected: {len(anomalies['latency_anomalies'])}\")\n",
    "print(f\"   Token Anomalies Detected: {len(anomalies['token_anomalies'])}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Review Status: {review_report['status']}\")\n",
    "\n",
    "print(f\"\\nâœ… Lab completed successfully!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4b7412a-a1c2-4c49-9415-4721e4b212cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "After completing this lab, consider:\n",
    "\n",
    "1. **Deploy to Production**: Apply these monitoring patterns to your production LLM systems\n",
    "2. **Customize Metrics**: Add domain-specific evaluation metrics for your use case\n",
    "3. **Automate Reviews**: Schedule periodic review workflows using Databricks Jobs\n",
    "4. **Expand Alerting**: Integrate alerts with your team's notification systems (Slack, PagerDuty)\n",
    "5. **Continuous Improvement**: Use evaluation results to guide model fine-tuning or prompt engineering\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Databricks MLflow Documentation](https://docs.databricks.com/mlflow/index.html)\n",
    "- [Model Serving Best Practices](https://docs.databricks.com/machine-learning/model-serving/index.html)\n",
    "- [LLM Evaluation with MLflow](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html)\n",
    "- [Databricks Monitoring](https://docs.databricks.com/lakehouse-monitoring/index.html)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing the Hands-On Lab!** ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fce55d71-b996-4906-9d12-604d3d155a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Cleanup (Optional)\n",
    "\n",
    "Run the following cell to clean up resources created during this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f45caef8-909b-4ebf-85bf-944db57766f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLEANUP RESOURCES\n",
    "# =============================================================================\n",
    "# Uncomment and run to delete resources created during this lab\n",
    "\n",
    "# Delete Model Serving endpoint\n",
    "# w.serving_endpoints.delete(name=ENDPOINT_NAME)\n",
    "# print(f\"âœ… Deleted endpoint: {ENDPOINT_NAME}\")\n",
    "\n",
    "# Delete Delta tables\n",
    "# spark.sql(f\"DROP SCHEMA IF EXISTS {CATALOG}.{SCHEMA} CASCADE\")\n",
    "# print(f\"âœ… Deleted schema: {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "# Delete MLflow experiment\n",
    "# Note: MLflow experiments cannot be permanently deleted, only archived\n",
    "# mlflow.delete_experiment(experiment_id)\n",
    "# print(f\"âœ… Archived experiment: {EXPERIMENT_NAME}\")\n",
    "\n",
    "print(\"âš ï¸ Cleanup code is commented out. Uncomment to delete resources.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Orielly -Chapter 8-LLM_Evaluation_Monitoring",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}